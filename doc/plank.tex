\documentclass[letterpaper,11pt]{article}

%% Style.
%\usepackage{charter}
\renewcommand{\rmdefault}{pplx}
\usepackage{eulervm}
\bibliographystyle{plain}
\usepackage[margin=1in]{geometry}

%% Format.
\input{setup}
\usepackage{cite}
\usepackage{stmaryrd}

%% Topmatter.
\title{ \hax: A Plank for Higher-order Attribute Contraction Schemes }
\author{%
  Cynthia Kop \\
 University of Copenhagen
 \and
 Kristoffer H. Rose \\
 Two Sigma Investments
 \and
 Maria Schett \\
 University of Innsbruck
 \and
 Lionel Villard \\
 IBM Research
}

%% Discussion
\newcommand{\CK}[1]{\textcolor{blue}{CK: #1}}
\newcommand{\KR}[1]{\textcolor{red}{KR: #1}}
\newcommand{\LV}[1]{\textcolor{green}{LV: #1}}
\newcommand{\MS}[1]{\textcolor{violet}{MS: #1}}


\begin{document}
\maketitle

\begin{abstract}\noindent
  %%
  We present \hax, a core (or ``plank'') calculus that can serve as the foundation for the \CRSX
  (Combinatory Reductions Systems with eXtensions) and \HAX (Higher-order Attribute Contraction
  Schemes) compiler specification languages.
  %%
  Formally, \hax is a general higher order rewriting formalism extended with first class
  \emph{associations}, and equipped with a parametric polymorphic sort system.
  %%
  In this paper we give the formal definition of the \hax calculus and its sort system, and we show
  how the central constructs of the much richer \HAX and \CRSX formalisms can be represented in
  \hax. We also outline how \hax can be implemented, and summarize central properties of the system.

  \CK{Cynthia's comments.} \KR{Kris' comments.} \LV{Lionel's comments.} \MS{Maria's comments.}
\end{abstract}

\compacttableofcontents

%------------------------------------------------------------------------

\section{Introduction}\label{sec:intro}

Several systems that manipulate programs, so-called \emph{meta-programming} systems, have emerged
over the years, ranging from generic specification languages, where the goal is not to define how
but only declare the semantics of the program manipulation, all the way to tools that support
specific aspects of program execution or compiler generation.

One direction has been to use a combination of \emph{higher order
  rewriting}~\cite{Jouannaud:klop2005} combined with \emph{higher order abstract syntax} (HOAS)
\cite{PfenningElliot:pldi1988}. This approach is used by \CRSX (Combinatory Reduction Systems with
eXtensions)~\cite{Rose:1996}, developed for writing industrial compilers at IBM
Research~\cite{Rose:hor2010,Rose:rta2011,dp60:ibm2013}, and the derived system \HAX (Higher-order
Attribute Contration Schemes)~\cite{Rose:ts2015}, developed to teach compiler construction at
NYU~\cite{RoseRose:cims2015}.

However, the direct implementation of the full \CRSX language~\cite{crsx} turned out to be quite
complex, and over time we have developed notions of what the ``core'' elements of the language
are. At the same time, \HAX has highlighted what additional programming paradigms one should add to
get a more useful programming language for writing compilers.

\TBD{Example of complex feature that can be simplified.}

The \hax calculus, presented here, represents the synthesis or what we have found to be the
essential features of a system for implementing most features of compilers.

\TBD{Plan}%
and finally Section~\ref{sec:conc} concludes and compares to related work.

%------------------------------------------------------------------------

\section{Overview}
\label{sec:overview}

In this section we outline the \hax calculus, with a minimal syntax that supports the \HAX source
language. \KR{Changed to Lionel's proposed syntax, except attributes are called ``associations''
  (they are different) and can only occur as construction ``pieces.''} \MS{pieces == subterms?}

\begin{figure*}[ht]
  \begin{align}
    \tag{\hax{}Script}
    H &::= D^* 
    \\
    \tag{Declaration}
    D &::= \kw{sort}~S~F
    \\[\jot]
    \tag{Form}
    F &::= \kw{data}~d\,\kw(\,\set{PF}^{*\kw,}\,\kw)\,\kw;
    \bigm| \kw{scheme}~f\,\kw(\,\set{PF}^{*\kw,}\,\kw)\,\kw;
    \bigm| \kw{variable}\,\kw;
    \bigm| \kw{rule}~T~\kw{$→$}~T\,\kw;
    \\
    \tag{PieceForm}
    \set{PF} &::= \kw[\,S^{*\kw,}\,\kw]\,S
    \bigm| \kwm\{\, S:S \,\kwm\}
    \\
    \tag{Sort}
    S &::= s\,\kw[\,S^{*\kw,}\,\kw]
    \bigm| α
    \\[\jot]
    \tag{Term}
    T &::= c\,\kw(\,P^{*\kw,}\,\kw)
    \bigm| v
    \bigm| M\,\kw[\,T^{*\kw,}\,\kw]
    \\
    \tag{Piece}
    P &::= \kw[\,v^{*\kw,}\,\kw]\,T
    \bigm| \kwm\{\, \set{A}^{*\kw,} \,\kwm\}
    \\
    \tag{Association}
    \set{A} &::= T\,\kw:\,T
    \bigm| \kw{$¬$}\,T
    \bigm| \kw:\,M
  \end{align}
  \vspace*{-2em}
  \caption{\hax syntax.}
  \label{fig:syntax}
\end{figure*}

\MS{About Figure~\ref{fig:syntax}
  \begin{itemize}
  \item $ D ::= \kw{sort}~S~a^*~\kwm\{\, F^*\,\kwm\}$. What does the $a$ stand for?
  \item The keyword $\kw{rule}$ is redundant? Maybe also $\kw{data}$ and $\kw{scheme}$?
  \item $O ::= ~\mid \kw{default}$ \\
    Is this really ``core''? I understand $\kw{default}$ as the ``fallback'' rule of a set of rules
    with the same function symbol, e.g., $ Plus([]\#1(), []\#2[]) → \#2[] ;$ in
    Figure~\ref{fig:peano}. But it would be easily computable whether a rule is a fallback rule,
    i.e., the most general instance of a set of rules. Am I missing something?
  \item There are three different ways to ``brace'' 
     \begin{itemize}
     \item $\kwm\{\ \kwm\} $: for associations
     \item $\kw{[ ]}$: for variables and meta variable arguments
     \item $\kw{()}$: for term, data and scheme arguments
     \end{itemize}
     I find this a bit confusing. Why not only one? Do the braces \emph{add} any info?
  \end{itemize}
}

\begin{definition}[\hax syntax]
  %%
  The \hax syntax is shown in Figure~\ref{fig:syntax}. The top level of a \hax script is a sequence
  of declarations $D^*$ and the grammar assumes that we have three categories of tokens defined:
  %%
  \MS{Wouldn't e.g., $¬$, $\kw{scheme}$, or $→$ also be a token?}
  %%
  %%
  \begin{itemize}

  \item $c,s,d,f ∈ \mathcal{C}$ stand for \emph{constructor} tokens, which are capitalized words
    like "Integer" or "A" or "CamelCaseWord". They are used for sort, function symbol, and data
    symbol names. By convention, we use $s$/$d$/$f$ for sort/data/function names, respectively, and
    $c$ when either a data or function constructor makes sense.  
  \item $v,α ∈ \mathcal{V}$ stand for \emph{variable} tokens, which are lower case words like "x" or
    "foo" or even "lowWord", used for term variables and sort variables (respectively). 
    %%
    \MS{Minor detail: Why don't we separate the two variable tokens in term and sort variables,
      i.e., introduce them separately?}
    %% 
  \item $M ∈ \mathcal{M}$ stands for \emph{meta-variable} tokens, which are tokens that start with
    "#" like "#arg" or "#BigFoot" or even just "#1" or merely~"#".
  \end{itemize}
  %%
  The ``$A^{*\kw,}$'' 
  %%
  \MS{We mix a bit notation here, sometimes ``$A^{*\kw,}$'', but also e.g., $c(P_1,…,P_n)$. I
    personally prefer the latter, but I would stick to one. Or am I missing something?}
  %%
  notation means zero or more $A$s separated by commas. We shall use the
  following terminology:
  %%
  \begin{itemize}

  \item A term of the form $c(P_1,…,P_n)$ ($n≥0$) is called a \emph{construction}; if furthermore
    the top symbol $c$ is declared with a "scheme c {…}" declaration then it is called a
    \emph{function construction}, and if it is declared with "data d {…}" then it is called a
    \emph{data construction}.
    %%
    \MS{Here comes a really general question: How do data and function constructors/constructions
      relate to constructor and defined symbols in term rewriting (To clarify: A symbol is
      \emph{defined} wrt to a set of rules, if it occurs at the root of a left-hand side of a
      rule. The other symbols are constructors.) I have a feeling, that this is actually similar? 
      If so, I may argue, that the distinction is easily computable and is maybe not core?
      }
    %% 

  \item Arguments $P$ of constructions take one of two forms: \emph{scopes} $[v_1,…,v_n]T$, which
    introduce new local variables $v_1,…,v_n$ along with a term $T$ wherein they can occur, and
    \emph{associations} $\{A_1,…,A_n\}$. Each $A$ is either an \emph{entry} $T:T$, or an
    \emph{exclusion} $¬T$, or a \emph{catcher} ${:}M$.

  \item A term of the form $v$ is a \emph{variable occurrence}; if the variable $v$ occurs contained
    in a scope $[…v…]T$ then the variable occurrence is \emph{bound}.

  \item A term of the form $M[T_1,…,T_n]$ ($n≥0$) is a \emph{meta-application}

  \end{itemize}
  %%
  
  %%
  \begin{itemize}
  \item ; if the terms $T_i$ are distinct bound variable occurrences then this is a \emph{pattern
      meta-application}. \KR{Notice that this means one \emph{cannot} substitute a variable by an
      association!}  

    A term is called a \emph{pattern fragment} if all contained meta-applications are pattern
    meta-applications, and a \emph{pattern} if it is also a function construction.
  \end{itemize}

  \MS{I next write down a couple of Terms wrt.\ to the grammar in Figure~\ref{fig:syntax}.
      I'll start without Associations, just to get an idea.}
   \begin{hacs}[numbers=right,texcl]
     Zero                  
     Plus(#X, #Y) 
     S(Zero)         
     x                    
     #X                    
    [x]#X[x]               // This pattern meta-application is not a Term. Is this intended?
    [x,y]#F[y,x]           // ditto
    S([x,y]#X[y,x,z])
    #X[x]
    #F[Zero]           
    #F[#X]
   \end{hacs}
   \MS{So currently, a meta-application is not a term (except for $\#X$). Do we want this?}
   
   \MS{E.g., in KR thesis, TeReSe, a distinction between meta term and term (i.e., a meta term without
     meta variables) is made. Do we also want to do this?}
   
   \MS{There are not any examples making use of Associations yet. So I am not sure I understand the
     semantics here. I'll continue by writing down some Terms with Associations of the form $T \kw{:} T$:}
   \begin{hacs}[numbers=right,texcl]
     Plus(x, { x : Zero }) 
     Plus(#X, { x : Zero }) 
     S({ #X : #Y })                             
     #Z[S({ #X : #Y })]                    
     S({[x]#X[x] : Zero })  
     Plus(Zero, {Zero : S(Zero)})
     Plus( {x : y}, { z : #X })
   \end{hacs}
   \MS{I expect, most of them don't make sense. Could someone indicate which and why? They are all
     derivable from the syntax.}
   
  \MS{I'll take the above examples and continue with Associations of the form  $¬T$:}
   \begin{hacs}[numbers=right,texcl]
     Plus(x, { ¬x }) 
     Plus(x, { ¬Zero }) 
     Plus(#X, { ¬Zero }) 
     S({ ¬#X })                             
     #Z[S({ ¬#Z[x] })]                    
     S({ ¬S([x]#X[x]) })  
     Plus(Zero, { ¬S(Zero)})
     Plus( { ¬x}, { ¬z})
   \end{hacs}
   \MS{Again I expect that most of them don't make sense. Could someone indicate which and why? They
     are all derivable from the syntax.}
 
     \MS{Finally I'll take the above examples again and continue with Associations of the form  $: M$:}
   \begin{hacs}[numbers=right,texcl]
     Plus(x, { : #X }) 
     Plus(#X, { : #X }) 
     S({ : #X })                             
     Plus(#X, { : #Y })
     Plus([x].#X[x], { : #X })
     Plus( { : #X}, { : #X})
     Plus( { : #X}, { : #Y})
   \end{hacs}
   \MS{Again I expect that most of them don't make sense. Could someone indicate which and why? They
     are all derivable from the syntax.}    

\end{definition}


\begin{figure*}[p]
  \vspace*{-1em}
  %%
  \begin{hacs}[numbers=right,texcl]
sort N[] {                                                        // natural number sort
 data Z() ;                                                       // zero
 data S([]N[]) ;                                                 // successor
 scheme Plus([]N[], []N[]) ;                                    // $+$ operator
 rule Plus([]Z(), []#2[]) →  #2[] ;                             // $0+x = x$
 rule Plus([]S([]#1[]), []#2[]) →  S([]Plus([]#1[], []#2[])) ;  // $(1+x)+y=1+(x+y)$
}
  \end{hacs}
  \vspace*{-1em}
  \caption{Peano numerals in \hax.}
  \label{fig:peano}
  %%
  \vspace{1em}
  \begin{hacs}[numbers=right,texcl]
sort Λ[] {                                                       // sort of λ terms
 variable ;                                                      // permit variables
 data Lm([Λ[]]Λ[]) ;                                            // abstraction
 scheme Ap([]Λ[], []Λ[]) ;                                      // application
 rule Ap([]Lm([x]#body[x]), []#arg[]) → #body[#arg[]] ;   // β-reduction
}
  \end{hacs}
  \vspace*{-1em}
  \caption{Untyped λ calculus in \hax.}
  \label{fig:lambda}
  %%
  \vspace{1em}
  \begin{hacs}[numbers=right,texcl]
sort List[α] { data Nil(); data Cons([]α, []List[α]);
 scheme Append([]List[α], []List[α]);
 rule Append([]Nil(), []#2[]) →  #2[];
 rule Append([]Cons([]#11[], []#12[]), []#2[]) →  Cons([]#11[], Append([]#12[], []#2[]));
 scheme Map([β]α, []List[β]);
 rule Map([x]#[x], []Nil()) →  Nil();
 rule Map([x]#[x], []Cons([]#1[], []#2[])) →  Cons([]#[#1[]], Map([x]#[x], []#2[]));
}
  \end{hacs}
  \vspace*{-1em}
  \caption{Polymorphic lists in \hax.}
  \label{fig:list}
  %%
%%%   \vspace{1em}
%%%   \small
%%%   \begin{hacs}[numbers=right,texcl]
%%% sort Dummy[] { variable; }
%%% sort Bool[] val { data T({Dummy[]:Bool[]}); data F({Dummy[]:Bool[]});
%%%  data Not([]Bool[], {Dummy[]:Bool[]}); data Or([]Bool[], []Bool[], {Dummy[]:Bool[]});
%%% 
%%%  scheme Eval1([]Dummy[], []Bool[]); scheme Eval2([]Dummy[], []Bool[]);
%%%  rule Eval1([]d, []T({})) → T({d:T({})});
%%%  rule Eval1([]d, []F({})) → F({d:F({})});
%%% 
%%%  rule Eval1([]d, []Not([]#[], {¬d}) → Eval2([]d, []Not([]Eval1([]d, []#[])));
%%%  rule Eval2([]d, []Not([]#[]val{d:T()})) → Not([]#[]) val{d:F()};
%%%  rule Eval2([]d, []Not([]#[]val{d:F()})) → Not([]#[]) val{d:T()};
%%% 
%%%  rule Eval1([]d, []Or([]#1[], []#2[])val{¬d})
%%%    → Eval2([]d, []Or([]Eval1([]d, []#1[]), Eval1([]d, []#2[])));
%%%  rule Eval2([]d, []Or([]#1[]val{d:F()}, []#2[]val{d:#v2[]})) → Or([]#1[], []#2[])val{d:#v2[]};
%%%  rule Eval2([]d, []Or([]#1[]val{d:T()}, []#2[])) → Or([]#1[], []#2[]) val{d:T()};
%%% 
%%%  scheme Eval([]Bool[]); rule Eval([]#[]) →  Eval1([]dummy, []#[]);
%%% }
%%%   \end{hacs}
%%%   \vspace*{-1em}
%%%   \caption{Boolean \emph{value} attribute synthesis in \hax.}
%%%   \label{fig:bool}
%%%   %%
\end{figure*}



\begin{example}[Peano]\label{ex:peano}
  The classical first order Peano arithmetic rules are a simple \hax system, shown in
  Figure~\ref{fig:peano}.  The example illustrates how a sort is defined with data constructors and
  a scheme for rewriting, essentially following standard notations for many-sorted term rewriting,
  except for the slightly unusual (meta)variable notation with "#"s. The example illustrates the
  following points:
  %%
  \begin{itemize}

  \item The defined sort is "N", written "N[]", as it has no sort parameters. All the other
    declarations define artifacts of the "N" sort, so are inside the \kw{sort} declaration's
    \kw{\{\}}s.

  \item There are two \kw{data} constructors: "Z" with no parameters, and "S" with a single numeric
    argument. The single numeric argument is specified as "[]N[]" because it has no locally scoped
    binders ("[]"$_-$) and the "N" sort has no sort parameters ($_-$"[]").

  \item There is a single function (\kw{scheme}) constructor, "Plus", with two numeric arguments.

  \item There are two \kw{rule}s for the function symbol, implementing the usual Peano addition rules.

  \end{itemize}
  %%
  A sample rewrite sequence using this system, corresponding to the computation $1+1=2$, is
  %%
  \begin{displaymath}
    "Plus([]S([]Z()), []S([]Z()))" →
    "S([]Plus([]Z(), []S([]Z())))" →
    "S([]S([]Z()))"
  \end{displaymath}
\end{example}
%%
\MS{May I drop the empty parenthesis in Example~\ref{ex:peano}, Figure~\ref{fig:peano},
  Figure~\ref{fig:lambda}, and Figure~\ref{fig:list}? They create a lot of noise.}
%%

\MS{Could I get an intution, why it is preferable to group data, scheme and rule declarations by the
  return type?}

\begin{remark}
  One difference between the CRSX family, including \hax, and other higher order rewriting
  formalisms, is that the binder mechanism is part of the parent construction, \eg, the sort of the
  "S" constructor defines that instances must have the shape "S([]…)" with "…" being itself a Peano
  number.  Otherwise, binding and substitution are in the style of CRS higher order rewrite
  systems~\cite{Klop+:tcs1993}.\footnote{The notation does differ from the original CRS notation in
    that we use ``\#'' as a marker for meta-variables instead of the original reserved use of $Z$,
    and we use square brackets instead of round for meta-application arguments.}
\end{remark}

\begin{example}[untyped λ calculus]\label{ex:lambda}
  The untyped λ calculus is shown in Figure~\ref{fig:lambda} in \hax.  The declarations can be
  explained as follows:
  %%
  \begin{itemize}

  \item The "Λ" sort includes a special "variable" case to indicate that variables can occur in
    terms. \CK{Should there be a qualifier like ``unbound'' in this sentence?}
    \KR{Perhaps---suggestions?} \MS{Wouldn't Figure~\ref{fig:list} then also require \kw{variable}?
      It uses variables ($x, \beta, \alpha$) as well. However, neither Figure~\ref{fig:list} nor
      Figure~\ref{fig:lambda} use unbound variables. What exactly do we want to express with the
      \kw{variable}?}

  \item "Λ" also includes a usual case for application construction, which is a "scheme" because it
    can (sometimes) be rewritten.

  \item "Λ" includes an abstraction construction, which is a "data" case, and which includes a
    subterm with a single binder scoped over that subterm. The scoped subterm is written as
    "[Λ[]]Λ[]", which should be read as ``a subterm with a locally bound variable of sort "Λ" and a
    body of sort "Λ" in which it can occur.''

  \item We specify one rewrite "rule" for "Λ": β-reduction. As usual, the rule specifies how an
    application of an abstraction is simplified. The interesting aspect of the pattern is how the
    abstraction is matched: the part of the pattern "[x]#body[x]" means ``the scoped subterm with
    binder "x" and subterm "#bind" wherein we keep track of where "x" occurs.'' Note the similarity
    to the declaration of the subterm of the "Lm" constructor.

  \item Once an application of an abstraction is matched, the "rule" gives the result of
    simplification as "#body[#arg[]]", which means that we construct a copy of "#body" except all
    occurrences of the variable we matched (and kept track of) in the pattern "#body[x]" are
    substituted with what "#arg[]" matched.

  \end{itemize}
  %%
  A usual λ term like $(λx.x x)(λy.y)$ is represented as
  \begin{displaymath}
    "Ap([]Lm([x]Ap([]x, []x)), []Lm([y]y))"    
  \end{displaymath}
  and simplifies like this:
  %%
  \begin{displaymath}
    "Ap([]Lm([x]Ap([]x, []x)), []Lm([y]y))" →
    "Ap([]Lm([y]y), []Lm([y]y))" →
    "Lm([y]y))"
  \end{displaymath}
  %%
\end{example}

\begin{example}[lists]\label{ex:list}
  Figure~\ref{fig:list} shows a standard definition of polymorphic lists over an arbitrary element
  sort.  The target sort is "List[α]" which is a usual polymorphic way to express a list of members
  of an unspecified parameter sort~"α".  Inside the scope of the "sort" declaration, "α" denotes the
  member sort of the result list of all constructs. So the declaration of "data Cons([]α,[]List[α])"
  means that the "Cons" constructor takes one argument of the same sort as the member of the result
  list, as well as an argument of the same sort as the result, "List[α]".

  \TBD{Example evaluation.}
  %%
\end{example}

%%% \begin{example}[Boolean formulae]\label{ex:bool}
%%%   Figure~\ref{fig:bool} shows how a Boolean expression can be annotated with an attribute "val",
%%%   which has the computed logical value of the expression. Because \hax has no automatic propagation
%%%   of attributes, the annotations are generated by helper "Eval1" and "Eval2" schemes.
%%%   %%
%%%   Line~1 sets up a "Dummy" sort for a dummy key variable for maps. %
%%%   Line~2 sets up an attribute "val", which maps ``dummy variables'' to "Bool" values. %
%%%   Lines 3~and 4 set up the "Bool" sort with data constructors for the Boolean expressions true
%%%   ("T"), false ("F"), negation ("Not"), and disjunction ("Or"). %
%%%   Line~6 declares the "Eval1" and "Eval2" schemes, which take a dummy variable and a Boolean term
%%%   (and form a Boolean term by being defined inside the "sort Bool").  %
%%%   Lines 7~and 8 define the "Eval1" scheme on the two trivial cases: literal thruth values are
%%%   annotated with a "val" that maps the dummy variable to the truth value. %
%%%   Lines 10~through 12 define the "Eval1" scheme for negation. %
%%%   The first rule in line~10 expresses that "Eval1" can rewrite a "Not"-term where the "val"
%%%   attribute does \emph{not} have a binding for the dummy variable: in that case, the result is
%%%   obtained by applying the "Eval1" scheme recursively to the negated subterm and allowing the full
%%%   negation to be ``postprocessed'' by the "Eval2" scheme. %
%%%   Lines 11~and 12 give the details for how this works: once a negation term is obtained that has an
%%%   actual "val" attribute, then the "Eval2" scheme can rewrite the term to one where the negated
%%%   value is assigned to the negation term. %
%%%   Similarly, Lines 14~through 17 give rules for evaluating a disjunction. %
%%%   Finally, line~19 defines a root scheme that annotates an entire Boolean term by creating a "dummy"
%%%   variable and then passing control to "Eval1".
%%% 
%%%   Here is a possible simplification using this scheme to annotate a term corresponding to the
%%%   logical formula $(¬F)∨T$:
%%%   %%
%%%   \begin{hacs}
%%%     Eval([]Or([]Not([]F()), []T()))
%%%     → Eval1([]d, []Or([]Not([]F()), []T()))
%%%     → Eval2([]d, []Or(Eval1([]d, []Not([]F())), Eval1([]d, []T())))
%%%     → Eval2([]d, []Or(Eval1([]d, []Not([]F())), []T()val{d:T()}))
%%%     → Eval2([]d, []Or(Eval2([]d, []Not(Eval1([]d, []F()))), []T()val{d:T()}))
%%%     → Eval2([]d, []Or(Eval2([]d, []Not([]F()val{d:F()})), []T()val{d:T()}))
%%%     → Eval2([]d, []Or([]Not([]F()val{d:F()})val{d:T()}, []T()val{d:T()}))
%%%     → Or([]Not([]F()val{d:F()})val{d:T()}, []T()val{d:T()})val{d:T()}
%%%   \end{hacs}%
%%%   %%
%%%   The final term has every subterm annotated with its logical value.
%%%   %%
%%% 
%%%   \CK{I don't think this example works, though: in the end, you have all those formulas annotated,
%%%     but you don't have any reference to the variable d in the terms!  How can you check what the
%%%     value of the dummy variable is in the attribute set?  If you create a new variable to match
%%%     against, it won't be equal to $d$.  For what I remember, the ``dummy'' thing we originally had
%%%     going was just a sort with only one nullary constructor, which \emph{would} be easily
%%%     queryable.} \KR{That would depend on the context capturing $d$. Suggestions for improvements
%%%     welcome.}
%%% \end{example}

\TBD{Summarize unexplained features, like default rules.}

%------------------------------------------------------------------------

\section{Sorting}
\label{sec:sorting}

The \hax calculus in practice restricts the terms of the grammar in Figure~\ref{fig:syntax} further
by only allowing sortable scripts. Informally, sorting ensures that
%%
\begin{itemize}
\item the pattern and contraction restrictions are obeyed;
\item binders are used in the shape declared for constructors;
\item subterms (including variable and meta-variable occurrences) have the right sort;
\item attributes are used with properly sorted values;
\item attributes are used only on constructors for which they have been declared;
\item the attributes are the same on all data constructors of a given sort;
\item \emph{all} data terms of the same sort have the same attributes.
\end{itemize}
%%
If you check with the examples, then you can see that the examples all satisfy these restrictions
except Example~\ref{ex:bool}, which breaks the last of the above constraints. \TBD{Fix but how:
  patch example or rules?}

\CK{About the TBD: the example breaks the rule only because some data terms are
missing the $\mathit{val}$ attribute, right?  This ties in to my earlier
comment below Figure 1, although only a little: the attribute here should be a
catch-all meta-variable on the left, and the empty set on the right.  You can
actually do that with the syntax as given, but it would be quite ugly. \\
The most natural solution seems to be calling it syntactic sugar: we could
point out that Example~\ref{ex:bool} \emph{seems} to violate the rule, but
only due to syntactic sugar, as explained later?  Alternatively, we could
change the example.  To make it a bit less ugly, a possibility is to add a
sort $\mathtt{DecidedBool}$ with only true/false values, to be used as the
values for the attribute; typically, you'd probably want to know that those
values will in fact be true/false rather than a Not or Or.  This sort would
not need to have an attribute, so would save you from having to write an
addition attribute set inside an attribute set.}
\KR{Is it improved by the previous comments?}

\begin{notation}[vectors]
  We will use \emph{vector notation} with $\ov{X}$ denoting $X_1,…,X_n$ for some $n≥0$; $ε$ denotes
  all empty vectors. Vectors are combined by simple concatenation, and will freely abuse this
  notation and, for example, write $\ov{[\ov{x}]x}$ as an abbreviation of
  $[x_{11}…x_{1m_1}]x_1…[x_{n1}…x_{nm_n}]x_n$ for suitable $n,m_1,…,m_n ≥ 0$.
\end{notation}

\begin{definition}[sort environments]
  %%
  A \emph{global sort environment} $Γ$ is a structure that combines
  %% 
  \begin{itemize}

  \item $Γ_{\op{hasvar}}\colon 2^{\mathcal{C}}$ is a set of sorts (the sorts that allow variables).

  \item $Γ_{\op{cons}}\colon \mathcal{C} → S×F$ from constructor name to sort-form pairs.

  \item $Γ_{\op{attr}}\colon \mathcal{V} → \set{AF}$ from attribute name to attribute forms.

  \item $Γ_{\op{cons-attr}}\colon \mathcal{C} → 2^{\mathcal{V}}$ from constructor name to set of
    attribute names.

  \item $Γ_{\op{data-atts}}\colon \mathcal{C} → 2^{\mathcal{V}}$ from sort name to sets of data
    attribute names.

  \end{itemize}
  %%
  A \emph{local sort environment} $Δ$ is a structure that combines
  %% 
  \begin{itemize}

  \item $Δ_{\op{free}}\colon \mathcal{V} → \set{S}$ from variable names to sorts.

  \item $Δ_{\op{meta}}\colon \mathcal{M} → \set{PF}$ from meta-variable names to binder forms.

  \item $Δ_{\op{attr-meta}}\colon \mathcal{M} → \set{BAV}$ from attribute ``catch-all'' meta-variable
    names to attribute forms.

  \item $Δ_{\op{bound}}\colon \mathcal{V} → \set{S}$ from bound variable names to sorts.

  \end{itemize}
  %%
  with $BAV ::= [\,v^*\,]\,AV$.

  \TBD{Extension of maps?}

  We will confuse $Γ$ and $Δ$ with their components and omit the subscript when it is apparent from
  the context.
  %%
\end{definition}

We separate the mappings for free and bound variables for semantic reasons, as it will be useful
when showing properties of the type system.

\begin{figure*}[p]%\small
  \vspace*{-3em}
  \begin{gather*}
    %
    \intertext{\shoveright{Sorting of Declaration\hfil\fbox{$ Γ ⊢ D $}}}
    %
    \dfrac
    { Γ,s[\ov{S}] ⊢ (\ov{F}) }
    { Γ ⊢ \kw{sort}~s[\ov{S}]\,\kwm\{\, \ov{F} \,\kwm\} }
    % 
    \\[1ex]
    % 
    \dfrac
    { }
    { Γ ⊢ \kw{attribute}~a~\kwm\{\,s[\ov{S}]\,\kw:\,S_2\,\kwm\}\,\kw; }
    \quad Γ(a) = \{s[\ov{S}]:S_2\}, ~ s∈Γ_{\op{hasvar}} 
    % 
    \\
    %
    \intertext{\shoveright{Sorting of Form\hfil\smash{\fbox{$ Γ,s[\ov{S}] ⊢ F $}}}}
    %
    \dfrac
    { }
    { Γ,s[\ov{S}] ⊢ \kw{data}~c\,\kw(\,\ov{\set{PF}}\,\kw)~\ov{a}\,\kw; }
    \quad Γ(c) = (s[\ov{S}], c(\ov{\set{PF}})\ov{a}), ~ Γ_{\op{data-atts}}(s) = \{\ov{a}\}
    % 
    \\[1ex]
    %
    \dfrac
    { }
    { Γ,s[\ov{S}] ⊢ \kw{scheme}~f\,\kw(\,\ov{\set{PF}}\,\kw)~\ov{a}\,\kw; }
    \quad Γ(f) = (s[\ov{S}], f(\ov{\set{PF}})\ov{a})
    % 
    \\[1ex]
    %
    \dfrac
    { }
    { Γ,s[\ov{S}] ⊢ \kw{variable}\,\kw; }
    \quad s∈Γ_{\op{hasvar}}
    % 
    \\[1ex]
    %
    \dfrac
    { Γ,Δ,\op{Pattern} ⊢ T_1 : s[\ov{S}]  \qquad Γ,Δ,\op{Contract} ⊢ T_2 : s[\ov{S}] }
    { Γ,s[\ov{S}]  ⊢ O~\kw{rule}~T_1~\kw{$→$}~T_2\,\kw; }
    % 
    \\
    %
    \intertext{\shoveright{Sorting of Terms\hfil\smash{\fbox{$ Γ,Δ,\set{CP} ⊢ T : S $}}}}
    %
    \dfrac
    { Γ,Δ,CP ⊢ \ov{B} : \ov{\set{PF}} \qquad Γ,Δ,CP,\{\ov{a}\} ⊢ \ov{A} }
    { Γ,Δ,CP ⊢ c\,\kw(\,\ov{B}\,\kw)\,\ov{A} : S }
    \quad Γ(c) = (S, c(\ov{\set{PF}})\ov{a})
    % 
    \\[1ex]
    %
    \dfrac
    { Γ,Δ,CP,\{\ov{a}\} ⊢ \ov{A} }
    { Γ,Δ,CP ⊢ v \, \ov{A} : s[\ov{S}] }
    \quad Δ(v) = s[\ov{S}],~ Γ_{\op{data-attr}}(s) = \{\ov{a}\},~ s∈Γ_{\op{hasvar}}
    % 
    \\[1ex]
    %
    \dfrac
    { Γ,Δ,CP ⊢ \ov{T} : \ov{S} \qquad Γ,Δ,CP,\{\ov{a}\} ⊢ \ov{A} }
    { Γ,Δ,CP ⊢ M\,\kw[\,\ov{T}\,\kw] \, \ov{A} : S }
    \quad Δ(M) = [\ov{S}⇒S]~; \op{DataAtts}\{\ov{a}\} ∈ Γ(S)
    % 
    \\
    %
    \intertext{\shoveright{Sorting of Binder\hfil\smash{\fbox{$ Γ,Δ ⊢ B : \set{PF} $}}}}
    %
    \dfrac
    { Γ,Δ[\ov{v}↦\ov{S}],CP ⊢ T : S }
    { Γ,Δ,CP ⊢ \kw[\,\ov{v}\,\kw]\,T : \kw[\,\ov{S}\,\kw]\,S }
    %
    \\[\jot]
    %
    \intertext{\shoveright{Sorting of Association\hfil\smash{\fbox{$ Γ,Δ,\{\ov{a}\} ⊢ A $}}}}
    %
    \dfrac
    { Γ,Δ,CP ⊢ AV : \{S_1:S_2\} }
    { Γ,Δ,CP,\{\ov{a}\} ⊢ a\,AV }
    \quad a∈\{\ov{a}\};~ Γ(a) = \{S_1:S_2\}
    %
    \\[\jot]
    %
    \intertext{\shoveright{Sorting of Association Value\hfil\smash{\fbox{$ Γ,Δ ⊢ AV : \{S:S\} $}}}}
    %
    \dfrac
    { Γ,Δ,CP ⊢ T_1 : S_1 \qquad Γ,Δ ⊢ T_2 : S_2 }
    { Γ,Δ,CP ⊢ \kwm\{\,T_1\,\kw:\,T_2\,\kwm\} : \{S_1:S_2\} }
    %
    \qquad
    %
    \dfrac
    { }
    { Γ,Δ,CP ⊢ \kwm\{\kw:\,M\,\kwm\} : \{S_1:S_2\} }
    ~ Δ({:}M) = \{S_1:S_2\}
    %
    \\[\jot]
    %
    \dfrac
    { Γ,Δ,CP ⊢ T : S_1 }
    { Γ,Δ,CP,\op{Pattern} ⊢ \kwm\{\,\kw{$¬$}\,T\,\kwm\} : \{S_1:S_2\} }
    % 
    \qquad
    %
    \dfrac
    { }
    { Γ,Δ,CP,\op{Contract} ⊢ \kwm\{\kwm\} : \{S_1:S_2\} }
    %
  \end{gather*}
  \caption{\hax sort rules.}
  \label{fig:sortrules}
\end{figure*}

\begin{definition}
  Given a \hax script $H$, a sort $\op{Void}∈S$ which does not occur in $H$, and a sort environment
  $Γ$, and define the additional syntax rule
  %%
  \begin{displaymath}
    CP ::= \op{Contract} \mid \op{Pattern}
  \end{displaymath}
  %% 
  The \hax script is \emph{well-sorted for $Γ$} if $Γ$ is the smallest sort environment for which we
  can prove
  %%
  \begin{displaymath}
    Γ,\op{Void} ⊢ H
  \end{displaymath}
  %%
  using the rules in Figure~\ref{fig:sortrules} and the added conventions that
  \begin{itemize}

  \item $A⊢(\ov{B})$ means $A⊢B_1~\cdots~A⊢B_n$.

  \item $\op{KeySort}("s["\ov{S}"]")$ is true for sorts $s$ with a declaration
    $"sort "s"[…]{…variable;…}"$.

  \item 

  \end{itemize}

  Given $H,Γ$ as above, a term $T$ is said to be \emph{$S$-sorted in $H$} if we can prove
  %%
  \begin{displaymath}
    Γ,\op{Contract} ⊢ T : S
  \end{displaymath}
  %%
  using the rules.
\end{definition}

\TBD{Example sort derivation.}

%------------------------------------------------------------------------

\section{Rewriting}
\label{sec:rewriting}

In this section we formally define rewriting in the \hax calculus. We follow [\TBD{Cynthia?}] in
defining rewriting in the context of a sort assignment.

\begin{definition}[substitution]
  
\end{definition}

\TBD{mv, fv, substitution, rewriting.}

\begin{theorem}[subject reduction]
  Given a well-sorted \hax script $H$ with the rewrite 


A well-sorted term can only rewrite to a well-sorted term.
\end{theorem}

%------------------------------------------------------------------------

\section{Properties}
\label{sec:properties}

In this section we provide proofs for several standard rewrite properties of~\hax.

\TBD{What do we know of properties of such a system…Cynthi or Maria or Julian?}


%------------------------------------------------------------------------

\section{Implementing \hax}
\label{sec:implement}

In this section we give notes on how we see \hax implemented.

\TBD{Struct for nodes, reference counting, lazy free variable lists…Lionel?}

\TBD{What about dispatchification…Maria?}

%------------------------------------------------------------------------

\section{Supporting Compiler Paradigms}
\label{sec:compiling}

In this section we outline how the standard compiler construction idioms of the full \HAX language
translate into~\hax.

\begin{definition}[higher order inference rule]\label{def:infer}%
  %%
  A (higher order) \emph{inference rule} has the form
  %%
  \begin{displaymath}
    \dfrac
    { ∀\,\ov{x} : (\, C_1⇒P_1 ~\cdots~ C_n⇒P_n \,) }
    { P_0 ⇒ C_{n+1}}
    ~(L)
  \end{displaymath}
  %%
  where
  %%
  \begin{itemize}
  \item $L$ is the unique name of the rule.
  \item $P_0$ and all of $C_1,…,C_n$ are \emph{function constructions}.
  \item $P_0$ is a \emph{pattern} and all of $P_1,…,P_n$ are \emph{pattern fragments}.
  \item $∀i : \op{mv}(C_i) ⊆ \op{mv}(P_0…P_{i-1})$.
  \end{itemize}
  %%
  The \emph{inference simplification system} for the above rule is the system
  %%
  \begin{align}
    P_0 &→ L_1(P_0, [\ov{x}]C_1) \tag{$L_0$}\\
    L_1(P_0, [\ov{x}]P_1) &→ L_2(P_0, [\ov{x}]P_1, [\ov{x}]C_2) \tag{$L_1$}\\[-\jot]
    &~~\vdots\notag\\
    L_{n-1}(P_0, [\ov{x}]P_1, …, [\ov{x}]P_{n-1}) &→ L_n(P_0, [\ov{x}]P_1, …, [\ov{x}]P_{n-1}, [\ov{x}]C_n) \tag{$L_{n-1}$}\\
    L_n(P_0, [\ov{x}]P_1, …, [\ov{x}]P_n) &→ C_{n+1} \tag{$L_n$}
  \end{align}
  %%
  (where all symbols, including the variables $\ov{x}$, are the same as in the initial rule). All
  these rules need to be part of the "sort" declaration for the sort of $P_0$ and $C_{n+1}$, along
  with "scheme" declarations for all the introduced $L_i$ symbols.
  %%
\end{definition}

\begin{example}
  \TBD{Example inference simplification.}
\end{example}

We can prove that the inference simplification system for an inference rule implements the same
semantics for non-overlapping $P_0$s. Even with an identical prefix $P_0,\ov{x},C_1$ for two rules,
the system can be generated: in that case the two $L_1$ symbols of the rules must be aliased to a
common function symbol; in general $L_1…L_k$ must be collapsed for identical prefixes
$P_0,\ov{x},C_1,P_1, C_2,…,P_{k-1}, C_k$. \TBD{Explain?}

\begin{definition}[synthesized attributes]\label{def:synth}%
  %%
  A \emph{singleton synthesis rule} has the shape
  %%
  \begin{displaymath}
    c\biggl(\cdots \vcenter{\txt{
          $c_i({\cdots})\,{{↑}a_i\{{\cdots}\}}$\\
          $M_i[{\cdots}]\,{{↑}a_i\{{\cdots}\}}$
        }}\cdots\biggr)
    ~{{↑}a\{{\cdots}\}}
  \end{displaymath}
  %%
  (with the embedded subterms denoting all such subterms; the ``singleton'' restriction comes from
  the single synthesized attribute on all subterms).

  The \emph{synthesis simplification system} for the synthesis rule is
  %%
  \begin{align*}
    \op{Needs}_a\Biggl(
    c\biggl(\cdots \vcenter{\txt{
        $c_i({\cdots})$\\
        $M_i[{\cdots}]$
      }}\cdots\biggr)
    \Biggr)
    &→
    \op{Collect}\Biggl(
    c\biggl(\cdots \vcenter{\txt{
        $\op{Needs}_{{a_i}}(c_i({\cdots}))$\\
        $\op{Needs}_{{a_i}}(M_i[{\cdots}])$
      }}\cdots\biggr)    
    \Biggr)
    \\[1em]
    \op{Collect}\Biggl(
    c\biggl(\cdots \vcenter{\txt{
          $c_i({\cdots})\,{↑}a_i\{{\cdots}\}$\\
          $M_i[{\cdots}]\,{↑}a_i\{{\cdots}\}$
      }}\cdots\biggr)
    \Biggr)
    &→
    c\biggl(\cdots \vcenter{\txt{
          $c_i({\cdots})\,{↑}a_i\{{\cdots}\}$\\
          $M_i[{\cdots}]\,{↑}a_i\{{\cdots}\}$
        }}\cdots\biggr)
    ~{↑}a\{{\cdots}\}
  \end{align*}
  %%
\end{definition}

\begin{example}
  \TBD{Example synthesis simplification.}
\end{example}

\TBD{Discuss multiple dependent and independent synthesized attributes, and proof sketch that the
  simplification is correct; also the erasure of not-patterns, and check details from .}

\begin{definition}[inherited attributes]\label{def:synth}%
  %%
  \begin{align*}
    F\bigl(\cdots \vcenter{\txt{
          $A({·})\,{↑}a({·})$\\
          $X[{·}]\,{↑}x({·})$
        }}\cdots\bigr)
    {↓}c({·})
    &→
    \cdots\vcenter{\txt{
        $B({·})\,{↑}b({·})$\\
        $Y[{·}]\,{↑}y({·})$}}
    \cdots\vcenter{\txt{
        $G({·})\,{↓}g({·})$\\
        $Z[{·}]\,{↓}z({·})$}}
    \cdots
    \\
    \intertext{becomes}
    %%
    F\bigl(\cdots \vcenter{\txt{
          $A({·})$\\
          $X[{·}]$
        }}\cdots\bigr)
    {↓}c({·})
    &→
    F'\bigl(\cdots \vcenter{\txt{
          $\op{Needs}_a(A({·}))$)\\
          $\op{Needs}_x(X[{·}])$
        }}\cdots\bigr)
    {↓}c({·})
    \\[1em]
    F'\bigl(\cdots \vcenter{\txt{
          $A({·})\,{↑}a({·})$\\
          $X[{·}]\,{↑}x({·})$
        }}\cdots\bigr)
    {↓}c({·})
    &→
    \cdots\vcenter{\txt{
        $B({·})\,{↑}b({·})$\\
        $Y[{·}]\,{↑}y({·})$}}
    \cdots\vcenter{\txt{
        $G({·})\,{↓}g({·})$\\
        $Z[{·}]\,{↓}z({·})$}}
    \cdots
  \end{align*}
  %% 
\end{definition}

%------------------------------------------------------------------------

\section{A Constraint Model}
\label{sec:constraints}

In this section, we introduce a notion of \emph{constraint associations} and primitives for
\emph{constraint solving}.

\TBD{Develop this…Eva?}

%------------------------------------------------------------------------

\section{Conclusion}
\label{sec:conc}

With \hax, we have presented a rather small calculus that can serve as the underlying formalism for
reasoning about as well as implementing the \CRSX and \HAX languages.

\TBD{What is covered.}

~\cite{Knuth:mst1968} 
~\cite{Aho+:2006}


\paragraph*{Related work.}

We would like to give credit to SIS~\cite{Mosses:daimi1979}, which shares with \hax the use of
\emph{simplification} using a λ-calculus based formalism.

The most prominent system that supports implementation of compilers in formal (rewriting and
attribute grammar) form is ASF+SDF~\cite{Brand+:toplas2002}, which is based on first order
rewriting. While modules have been added for symbol table management, these lack the full
integration and easy way to handle scoped intermediate languages. The successor,
Rascal~\cite{Bos+:eptcs2011} adds a module for HOAS, but Rascal specifications operate in a world of
side effects, which we find hard to reconcile with higher-order term structures (with scopes).

The notion of ``higher-order'' used by \hax is similar to but not quite the same as in higher-order
attribute grammars (HAG)~\cite{VogtSwierstraKuiper:pldi1989}. Like HAGs, \hax specifications permit
constructing and passing of abstract syntax fragments in attributes but the ``higher order'' aspect
of \hax also covers the rewriting side, where we can build parameterized abstractions over any part
of a specification, including with attributes. Indeed, one can use substitution inside attributes,
and have absence of attributes and substitution block rewriting.

\paragraph*{Future work.} \TBD{Speculate!}


\paragraph*{Acknowledgements.} \TBD{Thank everyone.}

EU-funded Marie Skłodowska-Curie ``HORIP'' action collaboration with Cynthia Kop.


%------------------------------------------------------------------------

\bibliography{crs}


\TBD{End of sane part.}
\hrule
\vspace*{1pc}

%------------------------------------------------------------------------
\appendix

\section{Sugar}

\begin{notation}
  %% 
  \HAX permits some additional syntactic sugar:
  %% 
  \begin{itemize}

  \item Empty parenthesis and brackets can be omitted.

  \item The \kw{data} and \kw{rule} keywords can be omitted.

  \item \TBD{Update to simplified syntax.} Sort cases can be written with \kw{\texttt{|}} and without
    \kw{\texttt{\{\}}}…

  \item "attribute" declarations can be combined: $"attribute"~a_1~\set{AF}_1,…, a_n~\set{AF}_n ;$
    is an abbreviation of $"attribute"~a_1~\set{AF}_1;…; "attribute"~a_n~\set{AF}_n;$.

  \item An additional SortCase contruct is allowed:
    \begin{displaymath}
      \kw{template}~P~\kw{$→$}~T\,\kw;  \quad⇒\quad
      \kw{scheme}~P'\,\kw; ~ \kw{\texttt{|}} ~ \kw{data} ~ T'\,\kw; ~ \kw{rule} ~ P~\kw{$→$}~T\,\kw;
    \end{displaymath}
    where $P'$ and $T'$ are variants of $P$ and $P$ that have been cleared of "#"s.

  \item Finally, we rewrite special \emph{synthesis rules}:
    \begin{displaymath}
      c(\,\ov{\set{PB}}\,)~→~\ov{↑a~\set{V}}\,\kw;
      \quad⇒\quad
      \kw{rule}~c(\,\ov{\set{PB}}\,)~→~c(\,\ov{\set{PB}'}\,)~\ov{↑a~\set{V}}\,\kw;
    \end{displaymath}
    where $\set{PB}_i'$ is $\set{PB}_i$ with negative attribute patterns $\{¬{…}\}$ removed.

  \item We rewrite special \emph{inheritance rules}:
    \begin{displaymath}
      c(\,\ov{\set{PB}}\,)~→~\ov{↓v~\set{V}}\,\kw;
      \quad⇒\quad
      \kw{rule}~c(\,\ov{\set{PB'}}\,)~\ov{↓v~\set{V}}~→~c(\,\ov{\set{PB}'}\,)\,\kw;
    \end{displaymath}
    where 

  \item As defined below, every term $f(…)\ov{A}$ for a symbol $f$ defined with
    \begin{displaymath}
      \kw{\texttt{|} scheme}~f\,\kw(…\kw)~\ov{\set{AI}}
    \end{displaymath}
    must include an $A_i$ instance for every attribute defined by some $\set{AI}_i$.  The system
    automatically includes missing inherited attributes as follows:
    \begin{itemize}
    \item In patterns, a missing attribute is inserted as "↓a(#a)" or "↓a{:#a}" as appropriate.
    \item Missing attributes that were defined in the pattern are inserted the same.
    \item Remaining missing attributes must be of a map sort and are inserted as~"↓a{}".
    \end{itemize}

  \item We allow two more forms of \set{PA}:
    \begin{displaymath}
      \set{PA} ::= {…} \bigm| \kw{$↑$}\,m \bigm| \kw{$↓$}\,m
    \end{displaymath}
    They expand to all relevant synthesized and inherited attributes, respectively.

  \end{itemize}
\end{notation}

\begin{example}
  With syntactic sugar, we can simplify the Peano example to
  %%
  \begin{hacs}[numbers=right]
    sort N | Z | S(N) | scheme Plus(N, N) ;
    Plus(Z, S(#2)) → #2 ; 
    Plus(S(#1), #2) → S(Plus(#1, #2)) ;
  \end{hacs}
\end{example}

\begin{example}[untyped λ calculus]
  The untyped λ calculus is specified as follows in \hax with syntactic sugar:
  %%
  \begin{hacs}
    sort L | variable | Lm([x]Lam[x as L]) | scheme Ap(L, L) | x ;
    rule Ap(Lm([x]#body[x]), #arg) →  #body[#arg] ;
  \end{hacs}
  %%
  The example shows how binding is declared and used for substitution in the style of CRS
  systems~\cite{Klop+:tcs1993}:\footnote{The notation differs from the original CRS notation in that
    we use ``\#'' as a marker for meta-variables instead of the original reserved use of $Z$, and we
    use square brackets for substitution variables instead of round.}
  %%
  (Note that the same example in the (non-raw) full \HAX notation can include a parser for the
  native custom λ calculus syntax:
  %%
  \begin{hacs}
    token ID | [a-z] [a-z0-9_]* ;
    sort V | symbol ⟦⟨ID⟩⟧ ;
    sort L | ⟦λ⟨V binds x⟩.⟨L[x as L]⟩⟧ | scheme ⟦⟨L@1⟩⟨L@2⟩⟧@1
           | ⟦⟨V⟩⟧@2 | sugar ⟦(⟨L#⟩)⟧@2 →  #;
    ⟦ (λx.⟨L#body[x]⟩) ⟨L#arg⟩ ⟧ →  #body[#arg] ;
  \end{hacs}
  %%
  including precedence markers, syntactic sugar, \etc; for details see the full \HAX
  manual. Essentially the syntax-rich example translates to a parser and the former.)
\end{example}

\section{Inference Systems}
\label{sec:infer}

A common notation for specifying program analysis and …

A set of of inference rules is ...

\begin{definition}
  %%
  A \HAX \emph{ground sequential inference rule} has the form
  %%
  \begin{equation}
    \dfrac{ T_1 ⇒ P_1 \quad\cdots\quad T_n ⇒ P_n }{ P_0 ⇒ T_{n+1} }
    \label{eq:infer}
  \end{equation}
  %%
  where $n≥0$ and
  %%
  \begin{enumerate}
  \item $P_0$ (the \emph{initial} pattern) can be any \HAX pattern.
  \item $T_i$ (the \emph{tests}, $1≤i≤n$) are \HAX terms, and are allowed occurrences of meta-variables from $P_0…P_{i-1}$.
  \item $P_i$ (the \emph{constraints}, $i>0$) must be \HAX subpatterns.
  \item $T_{n+1}$ (the \emph{conclusion}) is allowed occurrences of all meta-variables.
  \end{enumerate}
  %%
  An inference rule with $n=0$ is called an \emph{axiom}, with $n>0$ a \emph{proper inference}.
  %%
  The $T_i⇒P_i$ over the line are called \emph{premise judgments}. The $P_0⇒T_{n+1}$ under the line
  is called the \emph{conclusion judgment}.
  %%
  Below we will vary the indexing scheme to capture various enumerations of sets of inference rules.
\end{definition}

\begin{definition}[pattern family]\label{def:patfam}
  Consider a set of ground sequential inference rules.  Such a set can obviously be indexed by the
  distinct initial patterns into families of rules
  \begin{displaymath}
    \dfrac{ T_{ij1} ⇒ P_{ij1} \cdots T_{ijn_{ij}} ⇒ P_{ijn_{ij}} }{ P_i ⇒ T_{ij} } ~(L_{ij})
  \end{displaymath}
  where all the $P_i$ used to index the family are pairwise distinct.  The groups obtained in this
  way are a \emph{pattern family}.
\end{definition}

\begin{definition}[leftmost matching]
  Consider a pattern family as in Definition~\ref{def:patfam}.  The family is said to be
  \emph{leftmost matching} if the group of rules for each $P_i$ satisfies one of the following
  conditions:
  \begin{itemize}
  \item either the family contains a single axiom, \ie, $1≤j≤1$ with $n_{ij}=0$,
  \item or the family contains only proper inferences, \ie, $1≤j≤m_i$ with $n_{ij}>0$.
  \end{itemize}
\end{definition}

%% Sub-index each non-axiom $P_i$-group by leftmost premise construction into sub-families of
%%    the form
%%    \begin{displaymath}
%%      \dfrac{ T_{ij} ⇒ P_{ijk1} \quad T_{ijk2} ⇒ P_{ijk2} \quad\cdots\quad T_{ijkn_{ijk}} ⇒ P_{ijkn_{ijk}} }{ P_i ⇒ T_{ijk} }
%%    \end{displaymath}
%%    where all the $T_{ij}$ used to index the sub-family within a $P_i$-group are pairwise disjoint.

\begin{definition}[left-operations]
  Given a leftmost-matching pattern family. Each $P$-indexed group of proper inference rules will
  have the following form:
  \begin{displaymath}
    \dfrac{ T_{j1} ⇒ P_{j1} \quad T_{j2} ⇒ P_{j2} \quad\cdots\quad T_{jn_j} ⇒ P_{jn_j} }{ P ⇒ T_j } ~ (L_j)
  \end{displaymath}
  with one $j$ per rule in the family, which is by definition non-empty ($1≤j≤n$) and contains only
  proper inferences ($n_j≥1$).
  %%
  \begin{enumerate}

  \item Given a label $L$, the \emph{left-flattening} rewrite rules of the group are
    %% 
    \begin{align*}
      L(P) &→ L'(T_{11}, …,T_{n1}, P) \\[\jot]
      L'(P_{11}, m_2, …, m_n, P) &→ L_1(P, P_{11}) \\[-\jot]
      &~\vdots\\
      L'(m_1, …, m_{n-1}, P_{n1}, P) &→ L_n(P, P_{n1})
    \end{align*}
    %% 
    with $L'$ a fresh symbol associated with the group and the $m_j$ fresh meta-variables.

  \item The \emph{left-eliminated} rules for the group are the rules
    \begin{displaymath}
      \dfrac{ T_{j2} ⇒ P_{j2} \quad\cdots\quad T_{jn_j} ⇒ P_{jn_j} }{ L_j(P, P_{j1}) ⇒ T_j } ~ (L'_j)
    \end{displaymath}
    with the $L'_j$ fresh labels derived from the $L_j$ labels.

  \end{enumerate}
\end{definition}

\begin{definition}[left-unfolded]
  Given a leftmost-matching pattern family and consider the proper inference rule group indexed by
  $P$. The following system is the \emph{left-unfolded} inference system for the $P$-indexed group:
  \begin{enumerate}

  \item The left-flattening rewrite rules of the group for a fresh~$L$.

  \item The new inference rule (which refers to the rewrite rules)
    \begin{displaymath}
      \dfrac{ L(P) → m \quad m ⇒ m' }{ P ⇒ m } ~ (L)
    \end{displaymath}

  \item The left-eliminated inference rules (which may be axioms or proper inferences).

  \end{enumerate}
\end{definition}

\begin{proposition}
  Given a set of ground sequential inference rules, which is a leftmost-matching pattern
  family. Pick one group of proper inference rules, indexed by the initial pattern $P$. The original
  system and the system where the group has been replaced with the left-unfolded group have the same
  normal forms.
\end{proposition}
\begin{proof}
  Easy: full proof tree before corresponds to proof tree after with simple conversion of the
  eliminated eliminated premise to an application of the new rule and a single use of the rewrite
  rule.
\end{proof}

Note that we can only prove that full ``big-step'' evaluations are equivalent: the new rules may
``get stuck'' in interesting ways (\TBD{example with overlapping patterns}).

\begin{lemma}
  Start with any leftmost matching ground sequential inference rule system and apply left-unfolding
  repeatedly except on axioms and the introduced inference rules
  \begin{displaymath}
    \dfrac{ L(P) → m \quad m ⇒ m' }{ P ⇒ m }
  \end{displaymath}
  The resulting system has the same normal forms as the original system.
\end{lemma}

To finish the transform from inference rules to \HAX rules we need two additional notions.

\begin{definition}
  An inference system is \emph{rooted} if it has one rule that occurs as the root rule of every
  proof tree.
\end{definition}

\begin{definition}
  A leftmost-matching ground sequential inference rule system is \emph{left deterministic} if the
  left-flattening 
\end{definition}

\begin{theorem}
  A rooted and leftmost matching ground sequential inference system can be implemented by a 
\end{theorem}

\end{document}


%------------------------------------------------------------------------
% Tell Emacs that this is a LaTeX document and how it is formatted:
% Local Variables:
% mode:latex
% fill-column:100
% TeX-master: t
% TeX-auto-untabify: nil
% End:
