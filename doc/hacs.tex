%% Introduction to HACS.
%%
%% Copyright (c) 2011-2015 Kristoffer Rose <krisrose@crsx.org>
%% License: Eclipse Public License 1.0 <https://www.eclipse.org/legal/epl-v10.html>
%%
\documentclass[11pt]{article} %style: font size.
\usepackage[utf8]{inputenc}

%% Special configurable markup.
\newcommand{\reportnumber}[1]{}
\newcommand{\authorname}[2]{#2}
\newcommand{\authorinstitution}[2]{\\#2}
\newcommand{\also}{/\xspace}
\newenvironment{figureunit}[1][]{\def\figureunitcaption{#1}}{\figureunitcaption}

\usepackage[type={CC},modifier={by},version={4.0}]{doclicense}
\newcommand{\basecopyright}{\noindent
  \HAX is © 2011, 2015 Kristoffer Rose and released under the
  \href{https://www.eclipse.org/legal/epl-v10.html}{Eclipse Public License 1.0}.\\
  \noindent\doclicenseImage[imagewidth=3em] Documentation is © 2011,2015 Kristoffer Rose and additionally available under \doclicenseNameRef.}
\newcommand{\documentcopyright}{\basecopyright}

%% Style.
\usepackage[margin=.7in]{geometry}
\usepackage[T1]{fontenc}
\bibliographystyle{plainurl}
% Figure wrapper!
\makeatletter
\newsavebox{\savedfigureunit}
\renewenvironment{figureunit}[1][]{\def\thefigureunitcaption{#1}%
  \begin{lrbox}{\savedfigureunit}\begin{minipage}{\linewidth}}%
{\thefigureunitcaption\vspace{1ex}\end{minipage}\end{lrbox}%
  \colorbox{blue!10}{\usebox{\savedfigureunit}}}
\makeatother
\renewcommand{\rmdefault}{pplx}
\usepackage{eulervm}
\AtBeginDocument{\SelectTips{eu}{11}}
%\usepackage{ts}

%% Base format.
\input{setup}

%% Special version information.
\usepackage{catchfile}
\CatchFileDef{\VERSION}{../VERSION}{}
\def\setversion VERSION=#1 #2\relax{\def\version{#1\xspace}}\expandafter\setversion\VERSION\relax

%% Topmatter.
\title{
  \emph{Introduction to} Compiler Generation Using \HAX
}
\author{
  \authorname1{Kristoffer H. Rose}
  \authorinstitution1{Two Sigma Investments \also New York University}
}
\reportnumber{2015-1}

\begin{document}
\maketitle

\begin{abstract}\noindent
  Higher-order Attribute Contraction Schemes---or \HAX---is a language for programming compilers.
  With \HAX it is possible to create a fully functional compiler from a single source file.  This
  document explains how to get \HAX up and running, and walks through the code of a simple example
  with each of the main stages of a compiler as specified in \HAX: lexical analysis, syntax
  analysis, semantic analysis, and code generation.

  \compacttableofcontents

  \vspace*{2em}\small\color{gray}\noindent%
  This document describes \HAX version \version available from
  \href{http://github.com/crsx/hacs}{github.com/crsx/hacs}.\\
  \documentcopyright
\end{abstract}


\section{Introduction}\label{sec:intro}

\emph{Higher-order Attribute Contraction Schemes}, or \HAX, is a formal system for symbolic
rewriting extended with programming idioms commonly used when coding compilers. \HAX is developed as
a front-end to the \CRSX higher-order rewriting engine~\cite{crsx}, although \HAX users need not be
concerned with the details of higher-order rewriting (even if those are, in fact, most interesting).

A compiler written in \HAX consists of a single \emph{specification file} with a series of formal
sections, each corresponding to a stage of the compiler.  Each section is written in a formal style
suitable for that stage of the compiler. Specifically, \HAX supports the following notations:
%%
\begin{description}

\item[Regular Expressions.] Used to describe how an input text is partitioned into
  \emph{tokens}. The regular expressions of \HAX follow common
  conventions~\cite{Aho+:2006}. Section~\ref{sec:tokens} gives details of this notation.

\item[Context-Free Grammars.] \HAX uses a form of BNF~\cite{NaurEtal:cacm1960} \emph{context-free
    grammars} but tweaks the notation to look more like \emph{templates}, to allow for reuse of the
  notation in the subsequent rewrite rules. \HAX includes simple mechanics to allow for the
  formalization of the transformation from token stream to abstract syntax. Details in
  Section~\ref{sec:syntax}.

\item[Recursive Translation Schemes.] Simple translation in general, and code generation in
  particular, is traditionally achieved by \emph{recursive translation} from one abstract form to
  another.  \HAX includes special notations for defining such translations, described in
  Section~\ref{sec:schemes}, as well as a full programming language for defining auxiliary so-called
  ``semantic'' sorts and operators, detailed in Section~\ref{sec:eval}.

\item[Attribute Grammars.] Analyses can often be conveniently described in the style of
  \emph{syntax-directed definitions}~\cite{Aho+:2006}, originally introduced as \emph{attribute
    grammars}~\cite{Knuth:mst1968}, which describe how properties propagate through the abstract
  syntax tree.  Section~\ref{sec:collect} details how the basic propagation rules work for
  synthesized attributes. Section~\ref{sec:sdd} explains how inherited attributes are integrated to
  enable the encoding of full syntax-directed definitions.

\item[Higher-order Abstract Syntax.] Most programming languages use \emph{lexical scoping}, and
  compiler internals as well as target representations increasingly manipulate lexical scopes as part
  of their normal operation~\cite{MarlowPeyton-Jones:2010,Morrisett+:popl1998}. \HAX supports this
  by providing explicit support for \emph{higher-order abstract syntax}, and integrates this with
  support for attributes with mappings from variables to values for modeling \emph{symbol tables}.

\end{description}
%%
A typical compiler written with \HAX involves multiple instances of each of the above, as each used
language, including intermediate and target languages, is specified using grammars and has
language-specific analyses, with the transformations between them specified as translation schemes.

\paragraph*{History.}

Many systems for writing programs that manipulate other programs, so-called \emph{meta-programming},
have emerged over the years. These range from generic specification languages, where the goal is not to
define how but only to declare the semantics of the program manipulation, all the way to tools that
support specific aspects of program execution or compiler generation (a survey is beyond the scope
of this document).

One such system was CRSX~\cite{Rose:1996} developed for industrial use at IBM Research by a team led
by the author~\cite{Rose:hor2007,Rose:hor2010,Rose:rta2011,crsx}.  CRSX is a language based on
\emph{higher-order rewriting}~\cite{Jouannaud:klop2005} combined with \emph{higher-order abstract
  syntax} (HOAS) \cite{PfenningElliot:pldi1988}, further extended for handling environments natively
and using pluggable parsers. The programming of the IBM Data\-Power XQuery
compiler~\cite{dp60:ibm2013} using CRSX proved that the approach can drastically reduce the
development time of a compiler (the cited XQuery compiler was estimated to have been developed in a
quarter of the traditional development time) as well as resulting in a rather more compact and
high-level source program.

However, the CRSX notation, based on combinatory reduction systems~\cite{Klop:1980,Klop+:tcs1993},
which combines λ calculus~\cite{Church:1941,Barendregt:1984} and term rewriting
systems~\cite{Klop:1992:Handbook}, has proven to be unwieldy for several reasons, first of all by
being quite different from standard notations used in compiler construction reference
works~\cite{Aho+:2006}. Adding a polymorphic sort system to CRSX following \emph{contraction
  systems}~\cite{Aczel:1978} changed the system to be similar to Inductive Type Systems
\cite{BlanquiJouannaudOkada:tcs2002}), which helped, but did not make the system easy enough to, for
example, teach compiler construction.

\HAX is an attempt to remedy this situation by providing a front-end for CRSX that allows the use of
standard notations and concepts of (formal) programming language descriptions to directly program
compilers and other systems for manipulating code, as discussed above. \HAX has been successfully
used to teach the graduate computer science compiler construction class at New York
University~\cite{RoseRose:cims2015}.

\paragraph*{Acknowledgements.}

The author would like to thank his coteacher at NYU, Eva Rose, our grader, José Pablo Cambronero, as
well as our students in the compiler construction class, for constructive comments to and insightful
questions on \HAX.\footnote{A special shout-out goes to John Downs for starting the Emacs
  \texttt{M-x hacs-mode} syntax highlighting mode~\cite{git:hacsel} and Tyler Palsulich for a \HAX
  mode for Sublime Text~\cite{git:hacs-sublime}.}
%%
The implementation of \HAX would not have been remotely possible without the CRSX team at IBM:
Morris Matsa, Scott Boag, and especially Lionel Villard, who suffered through understanding and
programming core fragments of the CRSX system that is used underneath \HAX.
%%
\HAX owes its sanity to a collaboration with Cynthia Kop, both as an intern with the Watson team in
2011, which created the polymorphic sort checker, in her thesis work~\cite{Kop:2012}, and in our
continuing collaboration on keeping the formal basis for the system up to date.
%%
Finally, the author would like to thank Two Sigma for supporting this work, and in particular Eliot
Walsh for corrections and advice on the use of English.

\paragraph*{Outline of this report.}

The remainder of this document introduces the most important features of the \HAX language by
explaining the relevant parts of the included \emph{First.hx} example (inspired by
\cite[Figure~1.7]{Aho+:2006}) as well as several other minor examples.
%%
Section~\ref{sec:run} shows how to install \HAX and run the example, before proceeding to
the writing of specifications. %
Section~\ref{sec:tokens} explains lexical analysis; %
Section~\ref{sec:syntax} syntax analysis; %
Section~\ref{sec:schemes} basic recursive translation schemes; %
Section~\ref{sec:eval} semantic sorts, operations, and data; %
Section~\ref{sec:collect} bottom-up semantic analysis; %
Section~\ref{sec:sdd} general syntax-directed definitions; %
and Section~\ref{sec:hoas} higher order abstract syntax. %
Section~\ref{sec:comp} addresses the manipulation of primitive values, %
and Section~\ref{sec:examples} provides several examples of how everything can be combined.
%%
Finally, %
Appendix~\ref{app:manual} has a reference manual, %
Appendix~\ref{app:errors} explains some of the (still) cryptic error messages, %
and Appendix~\ref{app:limits} lists some current limitations.
Bibliographic references are collected last.


\section{Getting Started}
\label{sec:run}

This section walks through the steps for getting a functional \HAX installation on your
computer.\footnote{Please report any problems with this procedure to \emph{hacs-bugs@crsx.org}.}

\begin{requirements}
  To run the \HAX examples presented here you need a *nix system (including a shell and the usual
  utilities) with these common programs: a Java development environment (at least Java~1.6~SE SDK,
  with "java" and "javac" commands); and a standard *nix development setup including GNU Make, flex,
  and C99 and C++ compilers. In addition, the setup process needs internet access to retrieve the
  \CRSX base system~\cite{crsx}, JavaCC parser generator~\cite{JavaCC}, and \emph{icu} Unicode C
  libraries~\cite{ICU}. Finally, these instructions are written for and tested on Ubuntu and Debian
  GNU/Linux systems using the "bash" shell; if your system is different, some adaptation may be
  needed.
\end{requirements}

\begin{commands}[install \HAX]\label{com:all}
  %%
  If you have an old version of \HAX installed, then it is best to remove it first with a command like
  %%
  \begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{rm -fr ~/.hacs hacs}}
  \end{code}
  %%$
  Now retrieve the \emph{hacs-\version.zip} archive, extract it to a new directory, and install it, for
  example with the following commands:\footnote{User input is \textcolor{blue}{blue}.}
  %%
  \begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{wget http://crsx.org/hacs-^version.zip}}
energon1[~]$ ^textcolor{blue}{^texttt{unzip hacs-^version.zip}}
energon1[~]$ ^textcolor{blue}{^texttt{cd hacs}}
energon1[hacs]$ ^textcolor{blue}{^texttt{make FROMZIP=1 install install-support}}
  \end{code}
  %%
  These commands will need Internet access to use the \texttt{wget} command to retrieve support
  libraries.\footnote{Specifically, \HAX needs the \CRSX system, JavaCC parser generator, and ICU4C
    Unicode library. The setup is documented in \emph{src/Env.mk}.} The above command will install
  \HAX in a \emph{.hacs} subdirectory of your home directory. You can change this with the option
  \verb|prefix=|… to (all uses of) \emph{make}; if so, then make sure to replace occurrences of
\verb|$HOME/.hacs| everywhere below with your chosen directory.

  The main \emph{make} command may take several minutes the first time and should end without error.

  Please check that your new installation works with these commands:
  \begin{code}[commandchars=\^\{\}]
energon1[hacs]$ ^textcolor{blue}{^texttt{cd}}
energon1[~]$ ^textcolor{blue}{^texttt{mkdir myfirst}}
energon1[~]$ ^textcolor{blue}{^texttt{cd myfirst}}
energon1[~]$ ^textcolor{blue}{^texttt{cp $HOME/.hacs/share/doc/hacs/examples/First.hx .}}
energon1[~]$ ^textcolor{blue}{^texttt{$HOME/.hacs/bin/hacs First.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./First.run --scheme=Compile \}}
               ^textcolor{blue}{^texttt{--term="^{initial := 1; rate := 1.0; position := initial + rate * 60;^}"}}
  LDF T,  #1 
    STF initial, T
    LDF T_77,  #1.0 
    STF rate, T_77
    LDF T_1,  initial 
    LDF T_1_60,  rate 
    LDF T_2,  #60 
    MULF  T_2_21 ,  T_1_60 ,  T_2 
    ADDF  T_96 ,  T_1 ,  T_2_21 
    STF position, T_96
  \end{code}
  %%
  Congratulations---you just built your first compiler!\footnote{Please do not mind the spacing --
    that is how \HAX prints in its present state.}
  %%
  The following assumes that you have issued the following command to make the main \emph{hacs}
  command available for use:
  %%
  \begin{code}[commandchars=\^\{\}]
energon1[hacs]$ ^textcolor{blue}{^texttt{alias hacs=$HOME/.hacs/bin/hacs}}
  \end{code}
  %%
  (It may be worth including this command in your setup, or including the
  \verb|$HOME/.hacs/bin| directory in your \verb|$PATH|.)
  %%
\end{commands}

\begin{example}[module wrapper]\label{ex:wrapper}%
  The source file for the \emph{First.hx} file used in the example above has the structure
  %%
  \begin{hacs}[mathescape,xleftmargin=\parindent]
/* Our first compiler. */
module org.crsx.hacs.samples.First
{
  // Sections.
  $\text{\it Lexical Analysis (Section~\ref{sec:tokens})}$
  $\text{\it Syntax Analysis (Section~\ref{sec:syntax})}$
  $\text{\it Semantic Analysis (Sections \ref{sec:schemes}, \ref{sec:collect} and \ref{sec:sdd})}$
  $\text{\it Code Generator (Section~\ref{sec:examples})}$
  $\text{\it Main (Section~\ref{sec:schemes})}$
}
  \end{hacs}
  Notice that \HAX permits C/Java-style comments and that the final component of the module name is
  the same as the base of the filename.
\end{example}

\begin{notation}
  The structure shown in Example~\ref{ex:wrapper} is formally explained in the appendix
  (Manual~\ref{man:structure}), as is how to run \HAX (Manual~\ref{man:run}).
\end{notation}

\begin{notation}[special Unicode characters]\label{man:unicode}
  \HAX uses a number of special symbols from the standard Unicode repertoire of characters, shown in
  Table~\ref{tab:unicode}.
\end{notation}

\begin{table}[ht]
  \begin{figureunit}[
      \caption{Unicode special characters used by \HAX.}
      \label{tab:unicode}
    ]
    \begin{displaymath}
      \begin{tabular}{ccc}
        \Xhline{2\arrayrulewidth}
        \emph{Glyph} & \emph{Code Point} & \emph{Character} \Bigstrut \\
        \Xhline{2\arrayrulewidth}
        {¬} & U+00AC & logical negation sign \\
        {¶} & U+00B6 & paragraph sign \\
        \hline
        ↑ & U+2191 & upwards arrow \\
        → & U+2192 & rightwards arrow \\
        ↓ & U+2193 & downwards arrow \\
        \hline
        ⟦ & U+27E6 & mathematical left white square bracket \\
        ⟧ & U+27E7 & mathematical right white square bracket \\
        ⟨ & U+27E8 & mathematical left angle bracket \\
        ⟩ & U+27E9 & mathematical right angle bracket \\
        \Xhline{2\arrayrulewidth}
      \end{tabular}
    \end{displaymath}
  \end{figureunit}
\end{table}


\section{Lexical Analysis}
\label{sec:tokens}

Lexical analysis is the process of splitting the input text into tokens. \HAX uses a rather standard
variation of \emph{regular expressions} for this. This section explains the most common rules; the
appendix (Manual~\ref{man:token}) gives the formal rules.

\begin{example}[tokens and white space]\label{ex:lexical}
  %%
  Here is a \HAX fragment for setting up the concrete syntax of integers, basic floating point
  numbers, identifiers, and white space, for use by this simple language:
  %%
  \begin{hacs}[xleftmargin=\parindent,numbers=right,texcl]
// White space convention.
space [ \t\n] ;

// Basic terminals.
token INT  | ⟨Digit⟩+ ;
token FLOAT  | ⟨Digit⟩* "." ⟨Digit⟩+ ;
token ID  | ⟨Lower⟩+ ('_'? ⟨INT⟩)* ;

// Special categories of letters.
token fragment Digit  | [0-9] ;
token fragment Lower  | [a-z] ;
  \end{hacs}
  %% 
\end{example}

The example illustrates the following particulars of \HAX lexical expressions.

\begin{notation}[lexical syntax]\leavevmode
  %%
  \begin{enumerate}

  \item Declarations generally start with a keyword or two and are terminated by a ";" (semicolon).

  \item "token" declarations in particular have the "token" keyword followed by the name of the
    token and a regular expression between a "|" (vertical bar) and a ";" (semicolon). Each defines
    the token as a \emph{terminal symbol} that can be used in other token declarations as well as
    the syntax productions described in the next section.

  \item Token names must be words that start with an uppercase letter.

  \item A regular expression is a sequence of units, corresponding to the concatenation of a
    sequence of characters that match each one.  Each unit can be a \emph{character class} such as
    "[a-z]", which matches a single character in the indicated range (or, more generally, in one of
    a sequence of individual characters and ranges), a \emph{string} such as \hacsc|"."| or
    \hacsc|'foo'| (either kind of quotes is allowed), or a \emph{reference} to a token or fragment
    such as "⟨Lower⟩", enclosed in the special Unicode mathematical angle brackets (see
    Table~\ref{tab:unicode}).

  \item The declaration of a "token fragment" specifies that the token can only be used in other
    token declarations, not in syntax productions.

  \item Every regular expression component can be followed by a repetition marker "?", "+", or~"*",
    and regular expressions can be grouped with parentheses.

  \item The regular expression for white space is setup by "space" followed by the regular
    expression of what to skip -- here spaces, tabs, and newlines, where \HAX uses backslash to
    escape in character classes with usual C-style language escapes.

  \end{enumerate}
  %%
  In addition, this manual follows the convention of naming proper grammar terminals with ALL-CAPS
  names, like "INT", to make them easy to distinguish from nonterminals below. (Token declarations
  are not grammar productions: tokens cannot be recursively defined, and a token referenced from
  another token is merely an inlining of the character sequences allowed by what is referenced.)
\end{notation}

Notice that while it is possible to make every keyword of your language into a named token in this
way, this is not necessary, as keywords can be given as literals in syntax productions, covered in
the next section.

\begin{commands}[lexical analysis]
  The fragment above is part of \emph{First.run} from Section~\ref{sec:intro}, which can thus be
  used as a lexical analyzer.  This is achieved by passing two arguments to the \emph{First.run}
  command: a \emph{token sort} and a \emph{token term}.\footnote{The command has more options that
    will be introduced as needed.}  Execution proceeds by parsing the string following the syntax of
  the token. For example, the following checks the lexical analysis of a number:
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --sort=FLOAT --term=34.56}
34.56
  \end{code}
  Note that the term \verb|34.56| could also have been provided enclosed in quotes, such as
  \verb|"34.56"| or \verb|'34.56'|.  If there is an error, the lexical analyzer will inform us of
  this:
  \begin{code}[commandchars=^\{\}]
$ ^textcolor{blue}{./First.run --sort=INT --term=34.56}
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
  Encountered " <T_FLOAT> "34.56 "" at line 1, column 1.
Was expecting:
    <T_INT> ...
  \end{code}
  where the trail of Java exceptions has been truncated.  The important information is in the first
  few lines.
\end{commands}

As the example illustrates, all the token declarations together define how the processed stream of
characters is partitioned into terminal symbols with no need for consulting a grammar.
%%
The reference manual for tokens in the appendix (Manual~\ref{man:token}) gives further details,
including some additional constructs.


\section{Syntax Analysis}
\label{sec:syntax}

Once tokens have been defined, it is possible to use \HAX to program a syntax analysis with a grammar
that specifies how to decompose the input text according to a \emph{concrete syntax} and how to
construct the desired \emph{abstract syntax tree} (AST) from that. Notice that \HAX does not provide
a ``parse tree'' in the traditional sense, \ie, a tree that represents the full concrete syntax
parse: only an AST is built.  Grammars are structured following the \emph{sorts} of AST nodes, with
concrete syntax details managed through precedence annotations and ``syntactic sugar''
declarations. The complete specification for grammars is in the appendix (Manual~\ref{man:syntax}).

\begin{example}\label{ex:syntax}
  %%
  Here is the syntax analysis grammar from the \emph{First.hx} example. This small example source
  language merely has blocks, assignment statements, and a few expression forms, like so:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right]
main sort Stat  | ⟦ ⟨ID⟩ := ⟨Exp⟩ ; ⟨Stat⟩ ⟧
                | ⟦ { ⟨Stat⟩ } ⟨Stat⟩ ⟧
                | ⟦⟧ ;

sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
           | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
           | ⟦ ⟨INT⟩ ⟧@2
           | ⟦ ⟨FLOAT⟩ ⟧@2
           | ⟦ ⟨ID⟩ ⟧@2
           | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;
  \end{hacs}
  %%
  The grammar structures the generated AST with two sorts: "Stat" for statements and "Exp" for
  expressions.
  %%
\end{example}

The example grammar above captures the \HAX version of several standard parsing notions:
%%
\begin{description}

\item[Literal syntax] is indicated by the double ``syntax brackets,'' $⟦…⟧$. Text inside $⟦…⟧$
  consists of three things only: spaces, literal character ``words,'' and references to
  nonterminals and predefined tokens inside $⟨…⟩$.  In this way, literal syntax is similar to macro
  notation or ``quasi-quotation'' of other programming notations.

\item[Syntactic sugar] is represented by the "sugar" part of the "Exp" sort declaration, which
  states that the parser should accept an "Exp" in parentheses, identified as "#", and replace it
  with just that same "Exp", indicated by "→ Exp#".  This avoids any need to think of parentheses in
  the generated AST as well as in the rules below.

\item[Precedence rules] are represented by the "@"-annotations, which assign precedence and
  associativity to each operator. This example marks all references to the "Exp" nonterminal inside
  the productions for "Exp" with the lowest permitted precedence in each case. The first rule in
  line 5 says that the "+" operator is restricted on the right to only expressions with at least
  precedence 1 (but not restricted on the left, causing it to be left-recursive). The outer "@1" in
  line 6 states that all "*" expressions have precedence 1, and the inner "@"-annotations allow left
  subexpressions of "*" with at least precedence 1 ("*" is also left recursive), whereas right
  subexpressions must have at least precedence~2. Notice that left (right) recursion is identified
  by the \emph{leftmost (rightmost) unit in the production having the outer precedence}.

\end{description}
%%
The precedence notation allows the definition of one sort per ``sort of abstract syntax tree node.''
This enables the use of a single kind of AST node to represent all the ``levels'' of expression,
which helps the subsequent steps.

\begin{remark} Precedence is traditionally done with additional ``helper'' productions. It is
    possible, for example, to recognize the same "Exp" language as in Example~\ref{ex:syntax} by
    something akin to the following concrete \HAX specification (in which the sugar is also
    concrete):
  %% 
  \begin{hacs}[xleftmargin=\parindent]
sort Exp0 | ⟦ ⟨Exp0⟩ + ⟨Exp1⟩ ⟧ | ⟦ ⟨Exp1⟩ ⟧ ;
sort Exp1 | ⟦ ⟨Exp1⟩ * ⟨Exp2⟩ ⟧ | ⟦ ⟨Exp2⟩ ⟧ ;
sort Exp2 | ⟦ ⟨Int⟩ ⟧ | ⟦ ⟨Float⟩ ⟧ | ⟦ ⟨ID⟩ ⟧ | ⟦ ( ⟨Exp⟩ ) ⟧ ;
  \end{hacs}%|
  %%
  However, this grammar generates a different result tree, where the nodes have the three different
  sorts used instead of all being of the single "Exp" sort that the precedence annotations make
  possible.  The transformed system also illustrates how \HAX deals with left recursion with
  "@"-annotations: each becomes an instance of \emph{immediate left recursion}, which is eliminated
  automatically using standard techniques.
\end{remark}

Also note that the notation is admittedly dense. This is intentional, as the notation can be
generalized to serve all the formalisms of the following sections.  Here are the formal rules.

\begin{notation}[syntax analysis]\leavevmode
  \begin{enumerate}

  \item Each sort is defined by a "sort" declaration followed by a number of \emph{productions},
    each introduced by a "|" (bar). (The first "|" corresponds to what is usually written ::= or →
    in BNF grammars.)  All productions for a sort define cases for that sort as a nonterminal,
    called the ``target'' nonterminal.

  \item Sort names must be words that start with an uppercase letter.

  \item Concrete syntax is enclosed in "⟦…⟧" (``double'' or ``white'' brackets). Everything inside
    double brackets should be seen as \emph{literal syntax}, even "\" (backslash), \emph{except} for
    \HAX white space (corresponding to "[\ \t\n\r]"), which is ignored, and references in "⟨…⟩" (angle
    brackets), which are special.

  \item References to \emph{terminals} (tokens) and \emph{nonterminals} (other productions) are
    wrapped in "⟨…⟩" (angle brackets).

  \item \emph{Precedence} is indicated with "@"$n$, where higher numbers $n$ designate higher
    (tighter) precedence. After every top-level "⟦⟧" and placed last inside every "⟨⟩"-reference to
    a target nontermina,l there is a precedence, which defaults to "@0".  The precedence of
    self-references at the beginning and end of a production must be \emph{greater than or equal to}
    the outer precedence; at most one of the ends can have precedence equal to the outer one.

  \item The special "sugar" declaration expresses that the specified concrete syntax with a single
    disambiguated self-reference is \emph{replaced} by what is written after the~→. A reference is
    ``disambiguated'' by trailing it with a meta-variable starting with~"#".

  \end{enumerate}
  %% 
  Of all these rules, the one thing that is unique to parsing is the precedence notation with "@".
  When specifying a grammar then \emph{every} target nonterminal reference has a precedence, which
  determines how to parse ambiguous terms. So imagine that every "⟨⟩" contains a "@" marker at the
  end, defaulting to "@0", and that every "⟦⟧" is terminated with a "@" marker, again defaulting to
  "@0".
\end{notation}

Notice that \HAX will do three things automatically:
%%
\begin{enumerate}
\item Split the productions into subproductions according to the precedence assignments.
\item Eliminate immediate left recursion, as in the example.
\item Left factor the grammar, which means that productions within a sort may start with a common
  prefix.
\end{enumerate}
%%
However, this is \emph{not} reflected in the generated trees. They will follow the grammar as
specified, eliminating the need to be aware that this conversion happens.

\begin{commands}
  %%
  It is possible to parse an expression from the command line:
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --sort=Exp --term="(2+(3*(4+5)))"}
2 + 3 * ( 4 + 5 )
  \end{code}
  %%$
  Notice that the printout differs slightly from the input term as it has been ``resugared'' from
  the AST with minimal reinsertion of the sugared syntax.
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --sort=Exp --term="2 ** 3"}
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException: 
  Encountered " "*" "* "" at line 1, column 4.
Was expecting one of:
    "(" ...
    <T_INT> ...
    <T_FLOAT> ...
    <T_ID> ...
  \end{code}
  %%
  Similarly to the lexical analysis, the syntax analysis can return an error in the case that a term
  provided does not satisfy the grammar.
  %%
\end{commands}

\section{Abstract Syntax and Recursive Translation Schemes}
\label{sec:schemes}

This section explains how to express recursive translation schemes over the abstract syntax implied
by a grammar. The formal specification for this section is in the appendix (Manual~\ref{man:parsed}
and \ref{man:raw}).

\begin{example}\label{ex:ast}%
  Consider the following subset of the example expression grammar of Example~\ref{ex:syntax}:
  %%
  \begin{hacs}[xleftmargin=\parindent]
sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
           | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
           | ⟦ ⟨INT⟩ ⟧@2
           | ⟦ ⟨FLOAT⟩ ⟧@2
           | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;
  \end{hacs}
  %%
  This grammar serves two purposes: to describe all token sequences that can be parsed and to
  describe the structures that are generated from them. Erasing all the information that is purely
  there for parsing leaves just
  %%
  \begin{hacs}[xleftmargin=\parindent]
sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp  ⟩ ⟧
           | ⟦ ⟨Exp  ⟩ * ⟨Exp  ⟩ ⟧
           | ⟦ ⟨INT⟩ ⟧
           | ⟦ ⟨FLOAT⟩ ⟧ ;
  \end{hacs}
  %%
  This is dubbed the \emph{abstract syntax} for the "Exp" sort, because all the helper information
  for the parser has been removed, leaving only the essential, structural information.
\end{example}

The abstract syntax, illustrated by the example, is relevant because the output of a \HAX-generated
parser is an \emph{abstract syntax tree}, or \emph{AST}, thus all subsequent processing with \HAX is
based on this simplified structure.

Formally, the abstract syntax is obtained as follows:
%%
\begin{itemize}
\item Erase all "@"$n$ precedence markers.
\item Remove "sugar" productions.
\end{itemize}
%%
What remains are minimal productions with the essential information describing the AST.

\begin{remark}%
  The dragon book~\cite{Aho+:2006} achieves a similar but even bigger leap by associating the
  grammar with explicit precedence, written
  \begin{align*}
    E &→ E+T \mid T \\
    T &→ T*F \mid F \\
    F &→ (~E~) \mid \textbf{int} \mid \textbf{float}
  \end{align*}
  with the abstract syntax
  \begin{displaymath}
    E → E+E \mid E*E \mid \textbf{int} \mid \textbf{float}
  \end{displaymath}
  which additionally ``folds'' the $T$ and $F$ productions into $E$, as was effectively done in
  the previous section.
\end{remark}

With such a description of the AST, it is possible to write code that operates on the AST,
implementing the notion of \emph{syntax-directed translation} (although it would perhaps be even
better called ``abstract syntax-directed translation'').

\begin{definition}
  A scheme is \emph{syntax-directed} if it has one case per abstract syntax production.
\end{definition}

In practice, syntax-directed translation schemes defined in \HAX have one \emph{rule} per abstract
syntax production.

\begin{example}[scheme over syntax]\label{ex:leftmost}
  Consider the abstract expression grammar from Example~\ref{ex:ast}, and define a new scheme called
  "Leftmost", which for an expression returns the leftmost number of the expression. To achieve
  this, first declare the scheme as follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort Exp | scheme Leftmost(Exp) ;
  \end{hacs}
  %%
  The declaration states that the scheme takes one parameter of sort "Exp" in ()s and delivers a
  result that is also of sort "Exp". (Translations that convert from one sort to another will come
  into play as needed going forward.)

  To have "Leftmost" actually do something requires a set of rules of the form
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(…) →    …;
  \end{hacs}
  %%
  where the two "…" in each case are replaced with a \emph{pattern} and a \emph{result}, in this
  case both of sort "Exp". The patterns are obtained directly from the abstract syntax productions
  simply by marking every "⟨⟩"-embedded token or nonterminal with a ``disambiguation'' mark, "#"$n$,
  giving the following:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)         →   …;
Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)         →   …;
Leftmost(⟦⟨INT#⟩⟧)                     →   …;
Leftmost(⟦⟨FLOAT#⟩⟧)                  →   …;
  \end{hacs}
  %%
  It is now possible to express the right side of each translation rule using the now named
  fragments of the pattern, keeping in mind that the result should always be of "Exp" sort:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)         →   Leftmost(Exp#1) ;
Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)         →   Leftmost(Exp#1) ;
Leftmost(⟦⟨INT#⟩⟧)                     →   ⟦⟨INT#⟩⟧ ;
Leftmost(⟦⟨FLOAT#⟩⟧)                  →   ⟦⟨FLOAT#⟩⟧ ;
  \end{hacs}
  %%
  The first two rules pass the "#1" fragment, which is of "Exp" sort, into "Leftmost", which is
  guaranteed (by the declaration) to return something of "Exp" sort. The last two rules explicitly
  return the argument form of sort "Exp". Notice that in the last two rules, the pattern sets up "#"
  to be of sort "INT" and "FLOAT", respectively, disallowing their use directly as the result, as
  they have the wrong sort. Rather, we use syntax construction to obtain the correct sort by making
  use of an appropriate production.
\end{example}

A syntax-directed scheme like "Leftmost" is called a \emph{semantic} translation scheme because it
is declared without any new syntax, \ie, without any use of "⟦⟧".

\begin{commands}[invoke scheme]
  The "Leftmost" scheme above is also included in \emph{First.hx}.  Since it operates on a single
  syntactic expression, it is possible to invoke the "Leftmost" scheme from the command line as
  follows:
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --scheme=Leftmost --sort=Exp --term="2*3+4"}
 2
  \end{code}
  %%$
  It is necessary to specify the sort of the input expression here because "Leftmost" takes an "Exp"
  argument, which is different from the main "Stat" sort.
\end{commands}

The notation for defining a syntax-directed scheme is as follows.

\begin{notation}[syntax-directed schemes]\leavevmode
  \begin{enumerate}

  \item Set the result "sort" and add a declaration for the scheme. A scheme "S" of result sort "R"
    with argument sorts "A" and "B" is declared by
    \begin{hacs}
  sort R |  scheme S(A, B);
  \end{hacs}
  A scheme is named with a capitalized word (the same as sort names), optionally followed
  by some ``arguments'' in "()"s, where the declaration gives the sort for each argument.

\item To make the scheme "S" syntax-directed in, say, "A", create a separate rule for each ``pattern
  case'' of "A", which is just each abstract syntax production for "A" with a "#"$n$ marker after
  the token or nonterminal name to identify the subexpression of that token or nonterminal for use
  on the right side of the~"→". Thus,
  \begin{hacs}
  sort A  |  ⟦ a ⟨E⟩ b ⟧  |  ⟦ ⟨G⟩ c ⟨H⟩ ⟧ ;
  \end{hacs}
  should have
  \begin{hacs}
  sort R;
  S(⟦ a ⟨E#1⟩ b ⟧, #2)        →   …;
  S(⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, #3)    →   …;
  \end{hacs}
  The "#"-markers are called \emph{meta-variables} and are tied to one specific sort in a rule,
  whether by its marker inside syntax or position as an argument. In the example this means that in
  the first rule, "#1" is of sort "E" and "#2" of sort "B", whereas in the second rule, "#1" is of
  sort "G", "#2" of sort "H", and here "#3" is of sort "B". (Rules are very like "sugar" productions
  except a given construct can have more than one rule; "sugar", however, is limited to a single
  rule that cannot depend on the inner structure of the construct.)

\item Each rule should be equipped with a result (right of the~"→") of the result sort, which can
  use the ``meta-variable'' "#"-named fragments using any (syntactic or semantic) mechanism for
  building something of the result sort. For example, adding
  \begin{hacs}
  sort R  |  ⟦ x ⟨E⟩ ⟧  |  scheme Other(B);
  \end{hacs}
  allows writing
  \begin{hacs}
  sort R;
  S(⟦ a ⟨E#1⟩ b ⟧, #2)         →  ⟦ x ⟨E#1⟩ ⟧ ;
  S(⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, #3)    →  Other(#3) ;
  \end{hacs}

  %% \item Finally notice that If it helps readability then non-syntax meta-variables as well as syntax
  %%   units can be explicitly sorted, so the rules of the previous item can be written
  %%   \begin{hacs}
  %%   S(A⟦ a ⟨E#1⟩ b ⟧, B#2)        →   R⟦ x ⟨E#1⟩ ⟧ ;
  %%   S(A⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, B#3)    →   Other(B#3) ;
  %%   \end{hacs}

\end{enumerate}
\end{notation}

\begin{example}[default rules]
  Since the last two cases of the "Leftmost" system in Example~\ref{ex:leftmost} above really just
  return the entire term again, the system can instead be written as follows:
  %%
  \begin{hacs}
  sort Exp | scheme Leftmost(Exp) ;
  Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)    →  Leftmost(#1) ;
  Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)    →  Leftmost(#1) ;
  default Leftmost(#)  →  # ;
  \end{hacs}
  %%
  which explicitly calls out that if neither of the two regular cases apply, then the scheme just
  returns the contained expression.
  %%
\end{example}

Thus far, the syntactic brackets "⟦…⟧" have been consistently used for ``stuff in the input,''
essentially \emph{input data}, and semantic constructors (like "Leftmost" above) for ``things that
can be computed,'' or \emph{functions}. \HAX does not, in fact, insist on this separation. In
particular, it is permissible to define ``syntactic schemes,'' which introduce new syntax that has
simplification rules associated with it.

Specifically, syntactic schemes are schemes that are defined using "⟦…⟧" notation. They are very
similar to "sugar" declarations, except that they can have multiple rules with pattern matching to
select which to apply, in contrast to sugar declarations, which can only have one generic case with
a single unconstrained meta-variable.

\begin{example}[syntactic scheme]\label{ex:flatten}
  Consider the syntactic list (data) sort
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort List | ⟦ ⟨Elem⟩ ⟨List⟩ ⟧ | ⟦⟧ ;
  \end{hacs}
  %%
  There is sometimes a need to \emph{flatten} nested lists when working with complex expressions.
  This can, of course, be done with a usual syntax-directed semantic scheme and a lot of nested
  "⟦⟨⟩⟧" constructions. An alternative is to define a \emph{syntactic scheme}, which is a scheme
  expressed in syntactic form but is in fact defined by rewrite rules. Flattening of lists can, for
  example, be defined as follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort List | scheme ⟦ { ⟨List⟩ } ⟨List⟩ ⟧ ;
⟦ { ⟨Elem#1⟩ ⟨List#2⟩ } ⟨List#3⟩ ⟧   →  ⟦ ⟨Elem#1⟩ { ⟨List#2⟩ } ⟨List#3⟩ ⟧ ;
⟦ { } ⟨List#⟩ ⟧ → # ;
  \end{hacs}
  %%
  This creates a scheme that can be informally written as "⟦{_}_⟧" with the understanding that the
  two occurrences of ``"_"'' should be filled with lists as prescribed by the syntax specification
  in the "scheme" declaration. Notice that the two rules differ on the \emph{content} of the braces
  and are clearly designed to fully eliminate all possible contents of the braces. This is
  essential; the scheme should be \emph{complete}. To be precise: the first "_" position in the
  "⟦{_}_⟧" function definition is filled differently in the two rules, namely once with each of the
  data shapes of lists -- indeed the rules are syntax-directed in the first list argument.
\end{example}

Syntactic schemes are very useful for working with output structures; for example, the flattening
scheme of Example~\ref{ex:flatten} makes it much easier to manipulate sequences of assembler
instructions.

\begin{figure}[p]
  \begin{figureunit}[
      \caption{\emph{examples/Stack.hx}.}
      \label{fig:stack}
    ]
    \inputhacs[texcl,mathescape,xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Stack.hx}
  \end{figureunit}
\end{figure}

\begin{example}[stack code compiler]\label{ex:stack}
  Figure~\ref{fig:stack} shows the \HAX script \emph{Stack.hx}, which contains a compiler from
  simple expressions to stack code. Lines \ref{code:stack:gram1}--\ref{code:stack:gram2} contain a
  simple expression grammar, as already discussed. Lines
  \ref{code:stack:code1}--\ref{code:stack:code2} contain a separate grammar, this time for the
  output stack "Code" (the special "¶" marks indicate where to insert newlines when printing code,
  and are not part of the syntax). Lines \ref{code:stack:flat1}--\ref{code:stack:flat2} contain a
  flattening syntax scheme (as in Example~\ref{ex:flatten}) for sequences of instructions.  Finally,
  lines \ref{code:stack:comp1}--\ref{code:stack:comp2} contain the syntax-directed translation from
  expressions to stack code. It is used it in the usual way,
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs Stack.hx}
HACS \version
…
$ \textcolor{blue}{./Stack.run --scheme=Compile --term="(1+2)*(3+4)"}
  PUSH 1 
  PUSH 2 
  ADD 
  PUSH 3 
  PUSH 4 
  ADD 
  MULT 
  \end{code}
  %%
  Simple code generation tasks can be handled with recursive schemes, such as this one. However,
  serious compilation tasks will require proper attributes and semantic helper structures, detailed
  in the following sections.
  %%
\end{example}


\section{Semantic Data, Operators, and Evaluation}
\label{sec:eval}

For most compilers, simple recursive translations, such as presented above, do not suffice. More
complex programming tools are necessary to support more sophisticated analyses and transformations.

Specifically, note that rules for syntax-directed schemes are similar to definitions by case in
functional programming. However, there are important differences to be detailed here that sometimes
make \HAX pick rules differently than functional programming would.

As discussed in the previous section, it is possible to have functions written with syntactic
brackets, and similarly it is possible to have data written with semantic constructors, called
``semantic data.''  Semantic data forms are introduced as non-syntax (so-called ``raw'') notations
that can be used in patterns.

\begin{example}[semantic data constants]\label{ex:unif}
  %%
  Another fragment of the \emph{First.hx} example has the semantic sorts and operations that are
  used. For the toy language that just means the notion of a \emph{type} with the way that types are
  ``unified'' to construct new types.
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent,numbers=right]
// Types to associate to AST nodes.
sort Type | Int | Float ;

// The Type sort includes a scheme for unifying two types.
| scheme Unif(Type,Type) ;
Unif(Int, Int) → Int;
Unif(#1, Float) → Float;
Unif(Float, #2) → Float;
  \end{hacs}
  %% 
  The code declares a new sort, "Type", which is a \emph{semantic sort} because it does not include
  any syntactic cases: all the possible values (as usual listed after leading "|"s) are simple
  \emph{term structures} written without any ⟦⟧s. Term structures are written with a leading
  ``constructor,'' which should be a capitalized word (the same as sort and scheme names),
  optionally followed by some arguments in "()"s, where the declaration gives the sort for each
  argument (here there are none).

  The rules for the "Unif" scheme, above, can be used, for example, to simplify a composite term as
  follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Unif(Unif(Int, Float), Int) → Unif(Float, Int) → Float
  \end{hacs}
  %%
  Note that overlaps are permissible, but it is important to verify determinacy, \ie, if a
  particular combination of arguments can be subjected to two rules, then they should give the same
  result! This example satisfies this requirement because the term "Unif(Float,Float)" can be
  rewritten by both the rule in line 7 and the rule in line~8, but it does not matter, because the
  result is the same. Also note that \emph{the order of the rules does not matter}: the three rules
  in lines 6--8 above can be given in any order.
  %% 
\end{example}

\begin{figure}[p]
  \begin{figureunit}[
      \caption{Peano numbers with addition and lists (\emph{examples/SZ.hx}).}
      \label{fig:sz}
    ]\small
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/SZ.hx}
  \end{figureunit}
\end{figure}

\begin{example}
  Consider the complete \HAX script in Figure~\ref{fig:sz}. Lines 3--12 define a syntax of
  expressions with lists, sums, external references, and the Peano convention that "0" stands for
  itself and "s "$n$ stands for $n+1$. Line 15 defines a new semantic data sort, "Value", which
  represents the same information but outside of syntax brackets. Line 18 defines a scheme as
  before, except now it is defined in lines 19--23 also over the data forms with arguments. Lines
  25--31 provide a traditional syntax directed translation from the syntactic "Exp" format to the
  semantic "Value" form. However, an attempt to run the "Load" scheme of the script on a single
  example like this:
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs SZ.hx}
…
$ \textcolor{blue}{./SZ.run --scheme=Load --term="s 0+s 0"}
« $Print2-SZ$Value[SZ$Value_Succ[SZ$Value_Succ[SZ$Value_Zero]], 0] »
  \end{code}
  delivers the right result, which is the Peano numeral for $2$,
  however, \emph{it is not printed properly}: the "Succ" constructors show up in internal form. This
  is because \emph{semantic data structures have no syntax and therefore cannot be printed}. The
  proper way is to use the "Calc" script, which translates back to external form, like this:
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs SZ.hx}
…
$ \textcolor{blue}{./SZ.run --scheme=Calc --term="s 0+s 0"}
 s  s  0   
  \end{code}
  which generates the correct result. This uses the "Calc" scheme defined in lines 34--35 and the
  "Unload" scheme from line~37 of Figure~\ref{fig:sz}.
\end{example}

However, while \HAX definitions for schemes and data sorts look like function definitions and
algebraic data types, they are evaluated differently than those would be in functional programming.
Specifically, \HAX allows rules to match and be applied \emph{before} the matched arguments have
been evaluated, which can give results that are surprising to the uninitiated. Specifically, \HAX is
neither ``eager'' nor ``lazy,'' although it is closer to the latter.

Consider this context:
\begin{hacs}[xleftmargin=\parindent]
sort Bool | True | False | scheme Or(Bool, Bool);
\end{hacs}
Defining the "Or" scheme as follows
\begin{hacs}[texcl,xleftmargin=\parindent]
Or(False, False) → False ;        //1
default Or(#1, #2) → True ;       //2 \textcolor{red}{problematic}
\end{hacs}
is only correct under one condition: that the arguments to "Or" are never computed by
schemes. Consider, for example, the term
\begin{hacs}[xleftmargin=\parindent]
Or(Or(False, False), False)
\end{hacs}
For this term, \HAX is allowed to decide that "//1" cannot be immediately applied, because \emph{at
  this time} the first parameter is different from "False", and \HAX may then decide to instead use
"//2". This is probably not what is intended.

The best way to avoid this is to fully expand the observed parameters for "Or", which leads to the
classic definition,
\begin{hacs}[xleftmargin=\parindent]
Or(False, #2) →  #2 ;
Or(True, #2) →  True ;
\end{hacs}
With rules that check for equality between several parameters, this requires further care. Consider,
for example, the following additions that search a list for a specific boolean value:
\begin{hacs}[texcl,xleftmargin=\parindent]
sort Bools | Cons(Bool, Bools) | Nil ;
sort Bool | scheme Find(Bools, Bool) ;
Find(Cons(#b, #bs), #b) → True ;                      //3
Find(Cons(#b, #bs), #b2) → Find(#bs, #b2) ;          //4 \textcolor{red}{problematic}
default Find(#bs, #b) → False ;                        //5 \textcolor{red}{problematic}
\end{hacs}
These rules are problematic for two reasons. First, \HAX can \emph{always} pick rule "//4" over rule
"//3", and in this way give a false negative, as will be discussed below. Second, if the list is
computed, then the "default" rule can be accidentally used. Consider an expression like
\begin{hacs}[xleftmargin=\parindent]
Find(Append(...), True)
\end{hacs}
(with some suitable definition of "Append"). Because this does not immediately match "//3+4", \HAX
can decide \emph{at that time} to apply rule "//5", which of course is wrong.

A fix for the second issue is to instead use
\begin{hacs}[texcl,xleftmargin=\parindent]
Find(Nil, #b) → False ;                                         //6
Find(Cons(#b, #bs), #b) →   True ;                            //7 \textcolor{red}{careful}
default Find(Cons(#b, #bs), #b2) →   Find(#bs, #b2) ;        //8
\end{hacs}
This avoids issues where the list parameter is unevaluated: they will not match any rule, so no
wrong simplification is done.

However, it still requires care to ensure that \emph{both} of the compared pieces in "//7" are \emph{always}
in data form. To see why, consider an expression like
\begin{hacs}[xleftmargin=\parindent]
Find(Cons(Or(...), ...), True)
\end{hacs}
Again, this does not immediately fit "//7" without evaluating the "Or(...)"  subterm, so \HAX may
instead use "//8", leading to a false negative result.

The fix to this is to guarantee that elements of a list are always data, and not unevaluated. One
way to achieve this is to use a special constructor when building such lists. If the expression is
\begin{hacs}[xleftmargin=\parindent]
Find(EvalCons(Or(...), ...), True)
\end{hacs}
with
\begin{hacs}[xleftmargin=\parindent]
sort Bools | scheme EvalCons(Bool, Bools) ;
EvalCons(T, #bs) →   Cons(T, #bs) ;
EvalCons(F, #bs) →   Cons(F, #bs) ;
\end{hacs}
then the problem does not occur, because the "Or" is forced to be evaluated before the value is
stored in the list. With  the use of "EvalCons" instead of "Cons" everywhere, "//6-8" are safe
to use. Note that "//3-5" are still not safe because they may observe an unevaluated "EvalCons",
which would still allow picking the wrong rule. The "EvalCons" approach has the advantage of stating
explictly what is forced.

Finally, the above is so common that it is supported with a special declaration that can be added to
achieve the same effect: if the rules are written as
\begin{hacs}[xleftmargin=\parindent]
Find(Nil, #b) → False ;
[data #b]  Find(Cons(#b, #bs), #b) →   True ;
default Find(Cons(#b, #bs), #b2) →   Find(#bs, #b2) ;
\end{hacs}
with the \emph{option prefix} "[data #b]", then the rule will force complete evaluation of the "#b"
component before it is determined that the second rule does not apply and thus that the "default"
rule may be used. Indeed it is seen as good practice to use the "data" option for all instances
where two subterms are compared for equality.


\section{Synthesizing Information}
\label{sec:collect}

\HAX has special support for assembling information in a ``bottom-up'' manner, corresponding to the
use of \emph{synthesized attributes} in compiler specifications written as syntax-directed
definitions (SDD), also known as attribute grammars. This section explains \emph{how} to convert any
SDD synthetic attribute definition into one appropriate for \HAX, introducing the necessary \HAX
formalisms along the way.  You can find further details in the appendix Manual~\ref{man:attributes}.

\begin{example}
  Consider the following single definition of the synthesized attribute $t$ for expressions~$E$:
  %% 
  \begin{equation}
    \begin{array}{l|l}
      \Xhline{2\arrayrulewidth}
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut\\
      \hline
      E → E_1 + E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \Bigstrut\\
      \Xhline{2\arrayrulewidth}
    \end{array}
    \tag{E1}
  \end{equation}
  %% 
  The rule is what the Dragon book calls ``S-attributed'' because it exclusively relies on
  synthesized attributes. This allows expression of the rule directly in \HAX as follows.
  %% 
  \begin{enumerate}

  \item The first thing to do is declare the attribute and associate it with the $E$ sort.
    \begin{hacs}
   attribute ↑t(Type);
   sort E | ↑t ;
    \end{hacs}
    the "↑" indicates ``synthesized'' because the attribute moves ``up'' in the tree. The
    declaration of the attribute indicates with "(Type)" that the \emph{value} of the synthesized
    attribute is of sort "Type".  Attributes are always named with lowercase names.
  
  \item Second, create patterns from the abstract syntax production, as in the previous section: the
    pattern for the single production is
    \begin{hacs}
   ⟦ ⟨E#1⟩ + ⟨E#2⟩ ⟧
    \end{hacs}
    using the subscripts from \thetag{E1} as "#"-disambiguation marks.

  \item Next add in \emph{synthesis patterns} for the attributes to be read.  Each attribute
    reference like $E_1.t$ becomes a pattern like "⟨E#1 ↑t(#t1)⟩", where the meta-variables like
    "#t1" should each be unique.  For this example,
    \begin{hacs}
   ⟦ ⟨E#1 ↑t(#t1)⟩ + ⟨E#2 ↑t(#t2)⟩ ⟧
    \end{hacs}
    which sets up "#t1" and "#t2" as synonyms for $E_1.t$ and $E_2.t$, respectively.

  \item Finally, add in the actual synthesized attribute, using the same kind of pattern at the
    \emph{end} of the rule (and add a ";"), to give
    \begin{hacs}
   ⟦ ⟨E#1 ↑t(#t1)⟩ + ⟨E#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2)) ;
    \end{hacs}
    %% 
    This is read, ``When considering an "E" (the current sort), which has the (abstract syntax) shape
    $⟦⟨E⟩+⟨E⟩⟧$ where furthermore the first expression has a value matching "#t1" for the
    synthesized attribute "t", and the second expression has a value matching "#t2" for the
    synthesized attribute "t", then the entire expression should be assigned the value
    "Unif(#t1,#t2)" for the synthesized attribute~"t".''

  \end{enumerate}
  %% 
  Assuming that "Unif" refers to the semantic scheme defined in Example~\ref{ex:unif}, the process
  is complete.
\end{example}

\begin{notation}[value synthesis rules]\leavevmode
  \begin{enumerate}

  \item Synthesized simple attributes are declared with declarations like $"attribute"↑a(S);$ with
    $a$ a lowercase attribute name and $S$ the sort name.

  \item The synthesized attribute $a$ is associated with a sort by adding the pseudo-production
    ${|}↑a$ to the sort declaration.

 \item Synthesis rules as discussed here have the form $p↑a(r)$, where $p$ is like a pattern in a
    rule, but with the requirement that $p$ be a \emph{data} instance (not a scheme); $a$ is an
    attribute name; and $r$ should be a replacement of the value sort of~$a$.

  \end{enumerate}
\end{notation}

\begin{figure}[p]
  \begin{figureunit}[
      \caption{Synthesizing the value of a Boolean expression (\emph{examples/Bool.hx}).}
      \label{fig:bool}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Bool.hx}
  \end{figureunit}
\end{figure}
%%% Note: fig:bool moved UP, example is further below.

\begin{example}\label{ex:collect}
  %%
  Example~\ref{ex:syntax} presented the abstract syntax of the small language processed by
  \emph{First.hx}. A type analysis of the expressions of the language (for now excluding variables)
  might look as follows as a standard SDD (syntax-directed definition), where $E$ is the "Exp"
  nonterminal and is associated with one attribute: $E.t$ is the synthesized "Type" of the
  expression $E$.  In the notations of \cite{Aho+:2006}, the SDD can be specified something like
  this:
  %% 
  \begin{equation*}
    \begin{array}{l|l}
      \Xhline{2\arrayrulewidth}
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut\\
      \hline
      E → E_1 + E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \Bigstrut\\[\jot]
      \quad\mid E_1 \ast E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \\[\jot]
      \quad\mid \textbf{int} & E.t = \op{Int} \\[\jot]
      \quad\mid \textbf{float} & E.t = \op{Float} \\[\jot]
      \Xhline{2\arrayrulewidth}
    \end{array}
  \end{equation*}
  %%
  where it is again assumed that $\op{Unif}$ is defined as discussed in Example~\ref{ex:unif}.
  %%
  Convert this SDD to the following \HAX (using the proper names for the sorts as actually found in
  \emph{First.hx}):
  %%
\begin{hacs}[xleftmargin=\parindent,numbers=right,texcl]
attribute ↑t(Type);       // synthesized type

sort Exp | ↑t ;           // expressions have an associated synthesized type, $E.t$

// Synthesis rules for $E.t$.
⟦ ⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨Exp#1 ↑t(#t1)⟩ * ⟨Exp#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨INT#⟩ ⟧ ↑t(Int);
⟦ ⟨FLOAT#⟩ ⟧ ↑t(Float);
\end{hacs}
  %%
  Line 1 declares the value of the synthesized "t" attribute to be a "Type".
  %%
  Line 3 associates the synthetic attribute "t" to the "Exp" sort: all synthetic attributes are
  associated with one or more abstract syntax sorts.
  %%
  The remaining lines 5--9 are \emph{synthesis rules} that show for each form of "Exp" what the
  value should be, based on the values passed ``up'' from the subexpressions; these are generated
  from the abstract syntax patterns and synthesis semantic rules, as discussed above.
\end{example}

%%% Note: fig:bool moved UP
\begin{example}\label{ex:bool}
  Figure~\ref{fig:bool} shows an implementation of Boolean algebra implemented with synthesized
  attributes. Notice how the main "Eval" scheme is defined to ``request'' the value of the
  synthesized attribute, which then triggers the evaluation of the Boolean expression. An example
  run would be
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs Bool.hx}
…
$ \textcolor{blue}{./Bool.run --scheme=Eval --term='t|(!f)'}
 t
  \end{code}
\end{example}

Finally, note that in the case of multiple synthetic attributes, a synthesis rule \emph{only} adds
one new attribute to the program construct in question; it does not remove any other attributes
already set for it (see below for examples of such systems). %TODO: see where?


\section{Full Syntax-Directed Definitions with Environments}
\label{sec:sdd}

Descriptions to this point have used ``top-down'' recursive schemes with positional parameters and
``bottom-up'' synthesis of named attributes with simple values. The last component used in
traditional compiler specification is the use of \emph{inherited} named attributes, which are
distributed top-down, like parameters. \HAX supports this with a hybrid combination of implicit
named parameters for recursive schemes. The section introduces features of \HAX covered in the
appendix Manual~\ref{man:attributes}.

One of the main uses of inherited attributes in formal compiler specifications is \emph{symbol
  management}. The focus here is on the \HAX notion of \emph{environment}, which fills this niche
and cannot be easily achieved with usual positional arguments to recursive schemes.

\begin{example}
  Compiler construction formalization expresses the use of a symbol table using an SDD with an
  inherited attribute that for each node in the AST associates the appropriate symbol table for that
  node. Consider the following three simple semantic rules, which demonstrate this approach:
  %% 
  \begin{equation*}
    \begin{array}{l|ll}
      \Xhline{2\arrayrulewidth}
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut&\\
      \hline
      S → T_1 ~ \textbf{id}_2 = E_3; S_4
      &
      E_3.e = S.e; 
      S_4.e = \op{Extend}(S.e, \textbf{id}_2.sym, T_1) \Bigstrut\quad
      &\thetag{1}
      \\[\jot]
      E → E_1 + E_2
      &
      E_1.e=E.e; E_2.e=E.e
      &\thetag{2}
      \\[1pt]
      \quad~\mid \textbf{id}_1
      &
      E.t = \op{Lookup}(E.e, \textbf{id}_1.sym)
      &\thetag{3}
      \\[\jot]
      \Xhline{2\arrayrulewidth}
    \end{array}
  \end{equation*}
  %% 
  Rule \thetag{1} handles declarations in the toy language: it creates a new environment (or symbol
  table) that \emph{extends} the environment from the context, the \emph{inherited}
  environment~$S.e$, with a new coupling from the symbol of $\textbf{id}_2$ to the type~$T_1$: the
  notation means that the new environment $S_4.e$ contains the new mapping as well as all the
  bindings from $S.e$ (\emph{except} any previous mapping of the symbol $\textbf{id}_2$).
  %% 
  Rule \thetag{2} merely expresses that the $e$ attribute is inherited from the context to both
  subexpressions of addition expresions.
  %% 
  Finally, rule \thetag{3} uses the environment to attach a synthesized attribute $t$ onto an $E$
  that contains an \textbf{id} token (but is not otherwise used in these rules). Specifically, the
  notation is meant to suggest that the type value is obtained by a \emph{lookup} of the mapping for
  the symbol of $\textbf{id}_1$.

  Here are the steps to follow to translate the above SDD fragment to \HAX.
  %% 
  \begin{enumerate}

  \item First, encode the grammar (assuming an existing $T$ sort of types and
    "ID" tokens as in the previous section):
    \begin{hacs}
   sort S  | ⟦ ⟨T⟩ ⟨ID⟩ = ⟨E⟩ ; ⟨S⟩ ⟧ ;
   sort E  | ⟦ ⟨E⟩ + ⟨E@1⟩ ⟧   | ⟦ ⟨ID⟩ ⟧@1 ;
  \end{hacs}
  %% 
  (Assume that the "T" sort is defined elsewhere.)

\item Having defined the grammar, declare the new attribute:
  \begin{hacs}
   attribute ↓e{ID : T} ;
  \end{hacs}
  Like synthesized attributes, inherited attributes are always given lowercase names.  The "{ID:T}"
  part declares the value of the attribute to be a \emph{mapping} from values (token strings) of
  "ID" sort to values of "T" sort. Such mappings take the role of symbol tables from traditional
  compilers.

\item Second, associate the inherited attribute to one or more \emph{recursive schemes}, which will
  be responsible for propagating that inherited attribute over values of a certain sort.  This has
  to be done by inventing a separate scheme for each combination of a sort and an inherited
  attribute:
  \begin{hacs}
   sort S | scheme Se(S) ↓e ;
   sort E | scheme Ee(E) ↓e ;
  \end{hacs}
  As can be seen, the scheme generates results of the "S" and "E" sorts, in each case taking a
  single argument of the same sort, and for each indicating with a "↓" that the scheme
  \emph{carries} the associated inherited attribute. Notice that, unlike for synthesized
  declarations, there is no "|" in front of the attribute, because the attribute itself is specific
  to the scheme, not a declaration for the entire sort.

\item Next, encode the simplest rule, \thetag{2}. As in the previous section, observe that
  \thetag{2} operates on sums, which leads to a pattern like the following, using the subscripts
  from~\thetag{2}:
  \begin{hacs}
   ⟦ ⟨E#1⟩ + ⟨E#2⟩ ⟧
  \end{hacs}
  Now insert the pattern into the scheme, as was done for recursive syntax-directed schemes:
  \begin{hacs}
   Ee(⟦⟨E#1⟩ + ⟨E#2⟩⟧)
  \end{hacs}
  This clearly respects the sort constraints defined above, with "Ee" being applied to an "E"
  expression. Since there are no complicated dependencies in \thetag{2}, the process is almost
  complete. All that is left to do is create a rule that, on the right side of the "→" applies the
  "Ee" scheme recursively to the subexpressions that should inherit the "e" attribute, which are the
  two marked subexpression, \emph{inside the syntax markers}:
  \begin{hacs}
   Ee(⟦⟨E#1⟩ + ⟨E#2⟩⟧)  →  ⟦⟨E Ee(#1)⟩ + ⟨E Ee(#2)⟩⟧ ;
  \end{hacs}
  Notice that there is no explicit mention of the "e" attribute, only the \emph{implicit} copying
  that follows from the use of the "Ee" scheme. The recursive arrangement of the "Ee" wrappers
  implies the two attribute equations $E_1.e=E.e$ and $E_2.e=E.e$ from \thetag{2}.

%The following sentence appears to be missing a predicate
\item Next, rule \thetag{3}, which defines the synthesized attribute $t$ on \textbf{id}-expressions
  using a ``Lookup'' meta-function that is meant to suggest extracting the mapping from the $E.e$
  environment inherited from the context. Begin with a template like this:
  \begin{hacs}
   Ee(⟦⟨ID#1⟩⟧)  →  ⟦⟨ID#1⟩⟧ ;
  \end{hacs}
  which just states that the $e$ attribute is inherited ``into'' a name subterm (through "Ee"),
  essentially making $E.e$ available, but note that the rule does not use the inherited attribute. In
  particular, this rule fails to actually set the $t$ attribute. Fixing this entails first capturing
  the ``Lookup'' in the inherited environment. \HAX has a special notation for this. Write the
  pattern as follows:
  \begin{hacs}
   Ee(⟦⟨ID#1⟩⟧) ↓e{#1 : #t}  →     …
  \end{hacs}
  The pattern of this rule is equipped with a \emph{mapping constraint} on the "e" inherited
  attribute, which corresponds to the "Lookup" notation of \thetag{3} above. It will match when "Ee"
  is applied to a name, called "#1", which is \emph{also} mapped by the associated "e" attribute to
  a type, denoted~"#t". After capturing the mapped type this way, complete the rule by explicitly
  associating it to the "t" attribute associated with "ID" expressions.
  \begin{hacs}
   Ee(⟦⟨ID#1⟩⟧) ↓e{#1 : #t}  →   ⟦⟨ID#1⟩⟧ ↑t(#t) ;
  \end{hacs}
  Notice how mappings like "e" use "{}" notation for both declarations and constraints, and simply
  valued attributes like "t" use~"()".

\item Finally, encode \thetag{1}. Begin with
  \begin{hacs}
   Se(⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E#3⟩; ⟨S#4⟩⟧)  →    ⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E Ee(#3)⟩; ⟨S Se(#4)⟩⟧ ;
  \end{hacs}
  which merely establishes that the basic rules that the $S.e$ attribute inherited from the context
  (through the left hand "Se") are passed to both $E_3$ and $S_4$ through the right "Ee" and "Se",
  respectively. This captures everything except the ``Extend'' notation. \HAX supports this by
  allowing mapping constraints also on the right side of the "→" acting as \emph{extensions} of the
  map.
  \begin{hacs}
   Se(⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E#3⟩; ⟨S#4⟩⟧)
   →   ⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E Ee(#3)⟩; ⟨S Se(#4) ↓e{#2 : #1}⟩⟧ ;
  \end{hacs}
  Consider carefully how the "⟦…⟧" and "⟨…⟩" nest: the first set wraps syntax fragments, and the
  second wraps raw (non-syntax) fragments inside syntax. Attribute constraints are always in the raw
  fragments.

\end{enumerate}
\end{example}

With the addition of inherited attrbutes, we can now list all the possible attribute forms.

\begin{table}[t]
  \begin{figureunit}[\caption{Attribute constraints.}\label{tab:attributes}]
    \begin{displaymath}\small
      \begin{array}{c|p{.4\textwidth}|p{.4\textwidth}}
        \Xhline{2\arrayrulewidth}
        \textsc{Notation} & \textsc{In Pattern} & \textsc{In Result} \\
        \Xhline{2\arrayrulewidth}
        D"↑a(#)"
        & $D$ must synthesize "a"-value in "#"
        & $D$ will synthesize "a"-value in "#"
        \\
        D"↑a{:#}"
        & --- "a"-environment in ":#"
        & --- "a"-environment in ":#"
        \\
        D"↑a{#k : #v}"
        & --- --- which includes binding "#k" to "#v"
        & --- --- which includes binding "#k" to "#v"
        \\
        D"↑a{¬#k}"
        & --- --- which does not have binding for "#k"
        & n/a
        \\
        D"↑a{}"
        & n/a
        & $D$ synthesizes empty environment in "a"
        \\
        D"↑#"
        & \emph{All} $D$-synthesized attributes in "#"
        & \emph{All} attributes in "#" $D$-synthesized
        \\
        \hline
        F"↓a(#)"
        & $F$ must inherit "a"-value in "#"
        & $F$ will inherit "a"-value in "#"
        \\
        F"↓a{:#}"
        & --- "a"-environment in ":#"
        & --- "a"-environment in ":#"
        \\
        F"↓a{#k : #v}"
        & --- --- which includes binding "#k" to "#v"
        & --- --- which includes binding "#k" to "#v"
        \\
        F"↓a{¬#k}"
        & --- --- which does not have binding for "#k"
        & n/a
        \\
        F"↓a{}"
        & n/a
        & $F$ inherits empty environment in "a"
        \\
        \Xhline{2\arrayrulewidth}
      \end{array}
    \end{displaymath}
  \end{figureunit}
\end{table}

\begin{notation}[attributes]\leavevmode
  \begin{enumerate}

  \item Attributes are defined with attribute declarations:
    %%
    \begin{displaymath}
      "attribute"
      ~
      \left\{
      \begin{gathered}
        {↑} \\
        {↓}
      \end{gathered}
      \right\}
      ~
      a
      ~
      \left\{
      \begin{gathered}
        (\,S\,) \\
        \{\,S'\,\} \\
        \{\,S' : S\,\}
      \end{gathered}
      \right\}
      \,;
    \end{displaymath}
    %%
    where each braced unit represents a choice:
    %%
    \begin{itemize}
    \item The arrow determines whether to define a synthetic ("↑") or inherited ("↓") attribute.
    \item $a$ is the attribute name, a lowercase identifier.
    \item The last unit gives the category and sort(s) of the attribute: $(S)$ is a simply valued
      attribute of sort $S$, $\{S'\}$ is an attribute with a set of $S'$-members, and $\{S':S\}$ is an
      attribute with a map from $S'$-values to $S$-values. For sets and maps, the $S'$ sort must be
      a token sort (or a sort with a single "symbol" case, as explained in the next section).
    \end{itemize}

  \item With $D$ denoting data terms and $F$ ``function'' terms, \ie, applied schemes, yields the
    conventions of Table~\ref{tab:attributes} in \HAX rules.  Note that one can have several
    constraints on a single (data or function) term, as long as they are all properly declared for
    the appropriate sort or scheme, respectively.

  \end{enumerate}
\end{notation}

\begin{figure}[p]

  \begin{figureunit}[
      \caption{Synthesizing and then inheriting an environment (\emph{examples/LetrecMap.hx}).}
      \label{fig:letrec}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/LetrecMap.hx}
  \end{figureunit}

  \vspace*{2em}

  \begin{figureunit}[
      \caption{SDD for type checking.}
      \label{fig:sdd}
    ]
    \begin{equation*}
      \begin{array}{r@{\;}l|lr}
        \Xhline{2\arrayrulewidth}
        \multicolumn{2}{c|}{\textsc{Production}}  & \textsc{Semantic Rules} &\Bigstrut\\
        \hline\Bigstrut
        S &→ \textbf{id} := E_1; S_2
        & E_1.e = S.e; S_2.e = \op{Extend}(S.e, \textbf{id}.sym, E_1.t) &\thetag{S1}
        \\[\jot]
        &\mid \{~S_1~\}~S_2 & S_1.e = S.e; S_2.e = S.e &\thetag{S2}
        \\[\jot]
        &\mid ε & &\thetag{S3}
        \\[\jot]
        \hline\Bigstrut
        E &→ E_1 + E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E1}\\[\jot]
        &\mid E_1 \ast E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E2}\\[\jot]
        &\mid \textbf{int} & E.t = \op{Int}&\thetag{E3}\\[\jot]
        &\mid \textbf{float} & E.t = \op{Float}&\thetag{E4}\\[\jot]
        &\mid \textbf{id} & E.t = \text{if}~\op{Defined}(E.e,\textbf{id}.sym)&\thetag{E5}\\
        && \qquad\quad\text{then}~\op{Lookup}(E.e,\textbf{id}.sym)&\\
        && \qquad\quad\text{else}~\op{TypeError}&
        \\[\jot]
        \Xhline{2\arrayrulewidth}
      \end{array}
    \end{equation*}
  \end{figureunit}

\end{figure}

\begin{example}
  Figure~\ref{fig:letrec} illustrates how an environment can be synthesized and then inherited. The
  \HAX script implements variable substitutions of mutually recursive bindings, which is essentially achieved
  by two ``passes,'' one for synthesizing an environment that contains the collected bindings, and a
  second pass that actually distributes and applies the collected (now inherited) environment to the
  target variable. The key rule is in line 21, where the environment that has been synthesized in
  "b" is then copied over to the inherited attribute "e" to the "Apply" scheme.
\end{example}

The examples presented thus far have very simple attribute dependencies. A final example
demonstrates more complicated attribute dependencies.

\begin{figure}[p]
  \begin{figureunit}[
      \caption{\HAX code for type analysis.}
      \label{fig:sdd-hacs}
    ]
\begin{hacs}[texcl,xleftmargin=\parindent,xrightmargin=2em,numbers=right]
sort Type  | Int | Float | TypeError
            | scheme Unif(Type,Type) ;

Unif(Int, Int) → Int;
Unif(#t1, Float) → Float;
Unif(Float, #t2) → Float;
default Unif(#1,#2) → TypeError; // fall-back

attribute ↑t(Type);  // synthesized expression type
sort Exp | ↑t;

⟦ (⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ (⟨Exp#1 ↑t(#t1)⟩ * ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨INT#⟩ ⟧ ↑t(Int);
⟦ ⟨FLOAT#⟩ ⟧ ↑t(Float);
// Missing case: variables -- handled by Ee below.

attribute ↓e{Exp:Type};  // inherited type environment
sort Exp | scheme Ee(Exp) ↓e ;  // propagates $e$ over Exp

// These rules associate $t$ attribute with variables (missing case above).
Ee(⟦⟨ID#v⟩⟧) ↓e{#v : #t} →  ⟦⟨ID#v⟩⟧ ↑t(#t);
Ee(⟦⟨ID#v⟩⟧) ↓e{¬#v} →  error⟦Undefined identifier⟧;

Ee(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧ ↑#syn) → ⟦ ⟨Exp Ee(#1)⟩ + ⟨Exp Ee(#2)⟩ ⟧ ↑#syn ;
Ee(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧ ↑#syn) → ⟦ ⟨Exp Ee(#1)⟩ * ⟨Exp Ee(#2)⟩ ⟧ ↑#syn ;
Ee(⟦⟨INT#⟩⟧ ↑#syn) → ⟦⟨INT#⟩⟧ ↑#syn ;
Ee(⟦⟨FLOAT#⟩⟧ ↑#syn) → ⟦⟨FLOAT#⟩⟧ ↑#syn ;

sort Stat | scheme Se(Stat) ↓e ;  // propagates $e$ over Stat

Se(⟦⟨ID#v⟩ := ⟨Exp#1⟩; ⟨Stat#2⟩⟧ ↑#syn)
  → SeB(⟦⟨ID#v⟩ := ⟨Exp Ee(#1)⟩; ⟨Stat#2⟩⟧ ↑#syn);
{
  | scheme SeB(Stat) ↓e;  // helper scheme for assignment after expression type analysis
  SeB(⟦⟨ID#v⟩ := ⟨Exp#1 ↑t(#t1)⟩; ⟨Stat#2⟩ ⟧ ↑#syn)
   → ⟦⟨ID#v⟩ := ⟨Exp#1⟩; ⟨Stat Se(#2) ↓e{#v : #t1}⟩⟧ ↑#syn ;
}

Se (⟦ { ⟨Stat#1⟩ } ⟨Stat#2⟩ ⟧ ↑#syn) → ⟦ { ⟨Stat Se(#1)⟩ } ⟨Stat Se(#2)⟩ ⟧ ↑#syn ;

Se (⟦ ⟧ ↑#syn) → ⟦ ⟧ ↑#syn ;
\end{hacs}
  \end{figureunit}
\end{figure}

\begin{example}
  Figure~\ref{fig:sdd} shows a more realistic SDD for type checking, and Figure~\ref{fig:sdd-hacs}
  shows the corresponding \HAX. Here are the steps followed to obtain this script:
  %% 
  \begin{enumerate}

  \item Lines 1--7 define the "Type" semantic data sort along with the helper "Unif" semantic
    function as previously, except here extended with a "TypeError" case.

  \item Lines 9 and 10 define the synthesized type $t$ on expression data, and lines 12--15 give the
    type synthesis rules except for typing variables, to be described later. This makes it clear
    that \emph{type synthesis cannot happen until variable occurrences are typed}. In the SDD, this
    corresponds to all the $t$-assignments except the one in \thetag{E5}, which depends on~$E.e$.

  \item Line 18 declares the inherited environment attribute, and line 19 associates it with the
    recursive scheme "Ee" on expressions.

  \item Lines 21--28 give the environment propagation rules for expressions. Specifically notice how
    there are two cases for identifier, line 22 and 23, corresponding to whether the identifier is
    defined in the environment or not, with the latter resulting in an "error" message in special
    \HAX form. Also notice how the recursive rules in lines 25--28 take care to preserve all
    synthesized attributes on the terms that are traversed by using a catch-all "↑#syn" constraint.

  \item In total, lines 9--28 fully capture rules \thetag{E1--E5}. The \HAX specification adds a
    condition on evaluation: first apply the environment, and then synthesize the type.

  \item Line 30 declares a recursive scheme carrying the "e" attribute over statements, and lines
    40~and 42 are simple recursive rules for the propagation of "e" corresponding to
    \thetag{S2--S3}.

  \item Rule \thetag{S1} is slightly more complicated, because the inherited attribute has
    non-trivial dependencies. It is essential to know the dependency relationship of the attributes
    to devise a \emph{recursive strategy} for the attribute evaluation. Recall the following
    (realistic) dependency for \thetag{1}: ``The $E_2.t$ attribute cannot be computed until
    \emph{after} $E_2.e$ has been instantiated (and recursively propagated).'' In that case, one has
    to evaluate \thetag{S1} in two steps:
    \begin{enumerate}
    \item Do $E_2.e = S.e$, establishing the precondition for allowing the system to compute $E_2.t$.
    \item When the system has computed $E_2.t$, then do
      $S_3.e=\op{Extend}(S.e,\textbf{id}_1.sym,E_2.t)$.
    \end{enumerate}
    These two steps are achieved by having an extra carrier scheme, "SeB", which is locally
    declared, so that "Se" and "SeB" can handle the two steps above: first "Se", in lines 32--33, copies
    "e" \emph{just} to $E_1$ (from \thetag{S1}), and then \emph{chains} to "SeB".

  \item The helper scheme, "SeB" declared in line 35, is set up to \emph{wait} for the synthesized
    $t$ attribute (line 36) to be computed for $E_1$ and only \emph{then} replaces the term with a
    new one to compute the $S_2$ part with an extended environment (line 37).
    
  \end{enumerate}
  %%
  The environment helpers become translated into the native \HAX environment patterns from
  Table~\ref{tab:attributes} as follows:
  %% 
  \begin{itemize}

  \item A ``$\op{Defined}(N.e, x)$'' test is encoded by having two rules: one for the ``true''
    branch with the constraint "↓e{#v}" in a pattern, and one for the ``false'' case with the
    constraint "↓e{¬#v}" in the pattern.

  \item ``$\op{Lookup}(N.e, x)$'' is encoded by adding a constraint "↓e{#v:#}" in a pattern, which
    then binds the meta-variable "#" to the result of the lookup. (This will imply the ``defined''
    pattern discussed above.)

  \item ``$\op{Extend}(N.e, x, V)$'' is encoded by adding a constraint "↓e{#v:V}" in the
    replacement.

  \end{itemize}
  %% 
\end{example}


\section{Higher Order Abstract Syntax}
\label{sec:hoas}

The examples thus far have analyzed and translated abstract syntax trees with structure and
symbols. The next order of business is to construct \emph{new} structures with new scopes, or copied
fragments with scopes inside. These are the features where the ``H'' of \HAX matter.

\begin{example}[untyped λ calculus]
  A simple example of a system that does rewriting of ``higher order'' terms is the classic λ
  calculus. The λ calculus is specified in \HAX as shown in Figure~\ref{fig:lambda}.
  %%
  \begin{itemize}

  \item The specification has properties like others we have seen: it is a \emph{module} (lines 1
    and 12); it has a lexical specification of \emph{spacing} (line 2) and \emph{identifiers} (line
    3); it has a \emph{main sort} (line 5) of terms "T"; the term grammar uses "@" precedence
    markers (lines 7--9) and syntactic sugar for grouping (line 9); the term has one \emph{scheme},
    which means that we may have rules for application (line 7), which is so basic in λ calculus
    that it is denoted by simple concatenation.

  \item The first syntax production (line 6) is different:
    %%
    ``"| ⟦λ ⟨ID binds x⟩ . ⟨T[x as T]⟩⟧"''
    %%
    has several new constructs inside the syntax brackets:
    %%
    \begin{itemize}

    \item The first, "⟨ID binds x⟩", is just like "⟨ID⟩" by itself but marks this particular
      identifier as being a \emph{binder}, which is used in a special way, and for the scope of the
      production is named "x" (in case there is more than one).

    \item The second, "⟨T[x as T]⟩", is just like "⟨T⟩" by itself but marks this particular instance
      of "T" as the \emph{scope} for the binder named "x", with the added information that all
      occurrences of the bound variable should (also) be considered to be of sort~"T".

    \end{itemize}

  \item The third production (line 8) is also special: it declares that if a term is a simple "ID",
    then in fact it is a \emph{symbol}, which is required for identifiers to occur bound as
    sort~"T".

  \item Finally, the defined scoping in the classic $β$ rewrite rule is used for our one scheme in
    line~11: it expresses that a λ term which is an application (of the scheme declared in line~7)
    where the first subterm is a λ-abstraction (declared in line~6) can be rewritten.

  \end{itemize}
  %%
  The sample can be run the usual way:
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs Lambda.hx}
…
$ \textcolor{blue}{./Lambda.run --sort=T --term='(λx.x)y'}
 y
  \end{code}
  %%
\end{example}

\begin{figure}[t]
  \begin{figureunit}[
      \caption{λ calculus (\emph{examples/Lambda.hx}).}
      \label{fig:lambda}
    ]
    \inputhacs[texcl,mathescape,xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Lambda.hx}
  \end{figureunit}
\end{figure}

More formally, \HAX provides support for manipulating scoped terms using \emph{higher-order abstract
  syntax} (HOAS) \cite{PfenningElliot:pldi1988} through some new constructs:
%%
\begin{itemize}

\item Outside syntax, \HAX recognizes identifiers starting with a lowercase letter as
  \emph{variables}.

\item In concrete syntax grammar productions, for a token $T$ and a \HAX variable $x$, one can use
  the special reference $⟨T\,"binds"\,x⟩$ to define that this particular instance of $T$ is a
  \emph{binder}, with the variable~$x$ as the label to indicate the scoping.

\item In a grammar production with a $⟨T\,"binds"\,x⟩$ reference, there can be one reference of the
  form $⟨S[x\,"as"\,S']⟩$. This is like a reference $⟨S⟩$ but with the added information that all
  occurrences of the binder labeled $x$ \emph{must} occur inside the $S$ subterm, and furthermore that
  the occurrences will have the sort~$S'$. This implies that there must in addition be a grammar
  production like $"sort"\,S'\,|\,"symbol"\,⟦⟨T⟩⟧$, to allow the occurrences to be properly parsed.

\item Rules \emph{must} use the native variable form (no $⟨⟩$s) for binders.

\item Patterns (left of $→$) should include references to scoped subterms using a special notation
  where scoped meta-variables are ``applied'' to the bound variables in $[\,]$s, looking something
  like this: $⟦…⟨S\#[S'⟦x⟧]⟩…⟧$.

\item Replacements (right of $→$) should always have scoped metavariables applied to the same number
  of arguments in $[\,]$s as the corresponding pattern sets up. The result of such an application is
  to replace all bound occurrences of the variable that was indicated in the pattern with whatever
  is used in the replacement.

\end{itemize}
%%
The formal rules are rather dense, however, as will now become apparent with some examples, their use is
really just a compact way of encoding the usual notions of binders and parameter substitution that
are familiar from ordinary programming.

\begin{example}\label{ex:hoas}
  %%
  Consider the following variation of the grammar in Example~\ref{ex:syntax}, which makes the
  scoping rules of this little assignment language explicit.
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right]
main sort Stat  | ⟦ ⟨ID binds v⟩ := ⟨Exp⟩ ; ⟨Stat[x as Exp]⟩ ⟧
                | ⟦⟧ ;

sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
           | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
           | ⟦ ⟨INT⟩ ⟧@2
           | ⟦ ⟨FLOAT⟩ ⟧@2
           | symbol ⟦ ⟨ID⟩ ⟧@2
           | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;
  \end{hacs}
  %% 
  The HOAS constructs are only present in lines 1~and 8 of the grammar, and the difference may seem
  irrelevant. However, consider these simple rules that \emph{duplicate} and \emph{append}
  statements (such a rule may be useful in loop hoisting code, for example):
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right,firstnumber=last]
sort Stat | scheme Duplicate(Stat) | scheme Append(Stat,Stat) ;
Duplicate(#S) →  Append(#S, #S) ;
Append(⟦⟧, #S2) → #S2 ;
Append(⟦ old := ⟨Exp#2⟩; ⟨Stat#3[old]⟩ ⟧, #S2)
  → ⟦ new := ⟨Exp#2⟩; ⟨Stat Append(#3[new], #S2⟩ ⟧ ;
  \end{hacs}
  %%
  Notice how the pattern of the last rule in lines 13--14 explicitly calls out the used binding: the
  actual variable that is used in the program is referenced in the pattern as "old", and the pattern
  matching should also keep track of all the occurrences inside the scope, which is~"#3", by writing
  it as "#3[old]". (This exploits the aforementioned \HAX hack, where \HAX variables that are also
  ``raw'' variables can be written in this short form instead of the full "#3[Exp⟦old⟧]".)  This
  setup makes it possible to now \emph{systematically replace the symbol} with a new and fresh one,
  referred to as "new" in the replacement in line 16, but which the system will in fact replace with
  a new and unique name, both the binder \emph{and all the in-scope occurrences} inside~"#3". This
  ensures that there is no so-called ``variable capture,'' \ie, that occurrences of the "old" name
  inside "#S2" accidentally become double-bound.
  %%
\end{example}

Interestingly, HOAS also makes it impossible to have variables ``escape'' from their scope. A rule
in the old grammar from Example~\ref{ex:syntax} might look like
\begin{hacs}
   Flip(⟦ ⟨ID#v1⟩ := ⟨Exp#2⟩; ⟨Stat#3⟩ ⟧) →    ⟦ { ⟨Stat#3⟩ } ⟨ID#v1⟩ := ⟨Exp#2⟩; ⟧ ;
\end{hacs}
which will take uses of "#v1" inside "#3" and move them outside of their scope. This is not
immediately possible with HOAS without explicitly substituting the occurrences of the bound variable
with something else in the copy, which would expose the error. (You may think that this is an
obvious mistake that no one would make, but this sort of variable escaping is a real problem that
leads to bugs in compilers.)
%%
The use of HOAS in \HAX in fact allows the full range of methods used in higher-order
rewriting~\cite{Jouannaud:klop2005,Klop+:tcs1993}, a very powerful mechanism.

\begin{figure}[t!]
  \begin{figureunit}[
      \caption{Definitions of Call-by-Value Continuation Passing Style (\emph{examples/CPS.hx}).}
      \label{fig:cps}
    ]
    \inputhacs[texcl,mathescape,xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/CPS.hx}
  \end{figureunit}
\end{figure}

\begin{example}
  Next consider a real example from the literature~\cite{DanvyRose:rta1998}. Figure~\ref{fig:cps}
  shows two systems for transforming λ-terms to continuation-passing style with call-by-value
  evaluation semantics. The grammar uses higher-order binding to define the two forms of abstraction
  (with "λ" and "λ̅") using specific unicode characters. The overlined application is a scheme
  because it can be evaluated, which is specified by the rule in line 16 (where the outermost
  operator is the overlined application). The two blocks of rules in lines 18--24 and 26--33 specify
  two variants of continuation-style conversion: the traditional form and a one-pass variant,
  respectively. The two systems are equivalent, but the second never constructs overlined redices ,
  which is what makes it one pass, as there is no need to simplify any so-called ``administrative''
  (overlined) redices. Notice how the transformations define syntactic helper schemes, to make the
  recursive composition easier to express.
\end{example}

Finally, sometimes symbols serve as \emph{labels} or for some other purpose, when there is a need
for globally unique symbols. This creates a dilemma: writing "⟦x⟧" for some symbol sort, how to
distinguish between the actual variable written "x" and a variable that is automatically renamed to
be globally unique? By default, \HAX follows the following rules:
%%
\begin{itemize}

\item Every token that is parsed to be of a "symbol" sort is \emph{automatically renamed} to a
  \emph{fresh} variable name. (If used in a pattern, the fresh name will match the actual symbol
  that is present in the term, and thus in essence really be a placeholder for an existing symbol.)

\item If a rule is prefixed with the option "[global x]", then the symbol "x" will not be subject to
  this automatic conversion.

\end{itemize}


\section{Compile-time Computations}
\label{sec:comp}

There is sometimes a need to compute helper values, most commonly for counting. \HAX supports this
through a dedicated sort, "Computed", which has special syntax for operations on primitive values.
%%
The mechanism is quite simple: in rule replacements, arguments of sort "Computed" can contain
expressions in ⟦⟧s, which include
%%
\begin{itemize}
\item References to meta-variables of "Computed" and token sorts (the latter as strings).
\item Integers in decimal and \texttt{0x}-hexadecimal notation.
\item String literals enclosed in \verb|"|…\verb|"|.
\item Standard operators (summarized in Table~\ref{tab:ops}) and parentheses.
\item Conversion of tokens to integers (prefix the token meta-variable with "$"). %$
\end{itemize}
%%
Notice that for "Computed", meta-variables are part of the syntax: essentially "#x" is used instead
of ``"⟨Computed#x⟩",'' which is not permitted, inside the special syntax.

Also, since \emph{all} computed parts of a rule are evaluated as soon as the rule is expanded, only
use this for calculations that can be safely computed in any case where the \emph{pattern} of the
rule matches.

%Note that in print, the bottow two lines of this table overhang their shaded box
\begin{table}[h]
  \begin{figureunit}[
      \caption{Operations permitted in Computed syntax.}
      \label{tab:ops}
    ]
    \begin{displaymath}
      \begin{array}{c|p{.55\linewidth}}
        \Xhline{2\arrayrulewidth}
        \textit{Operators} & \textit{Explanation} \\
        \Xhline{2\arrayrulewidth}
        \texttt{?:} & \text{ternary test} \\
        \hline
        \texttt{=}~\texttt{≠}~\texttt{<}~\texttt{>}~\texttt{≤}~\texttt{≥} & \text{comparisons} \\
        \hline
        \texttt{*}~\texttt{/}~\texttt{\%} & \text{multiplication, division, modulo} \\
        \texttt{\&} & \text{bitwise and} \\
        \verb|<<|~\verb|>>| & \text{bitwise shift left and right} \\
        \hline
        \texttt{+}~\texttt{-} & \text{unary plus and minus}\\
        \verb|~| & \text{bitwise not} \\
        \texttt{escape~unescape} & \text{unary function that inserts/removes C string escapes} \\
        \texttt{length} & \text{unary function that returns length of (Unicode) string} \\
        \Xhline{2\arrayrulewidth}
      \end{array}
    \end{displaymath}
  \end{figureunit}
\end{table}

\begin{example}[count]\label{ex:count}
  Consider the list from Example~\ref{ex:flatten}. The following computes the length of the list,
  using a helper.
  %%
  \begin{hacs}
sort Computed | scheme ListLength(List) | scheme ListLength2(List, Computed) ;
ListLength(#) → ListLength2(#, ⟦0⟧) ;
ListLength2(⟦ ⟨Elem#1⟩ ⟨List#2⟩ ⟧, #n) →  ListLength2(#2, ⟦ #n + 1 ⟧) ;
ListLength2(⟦ ⟧, #n) → #n ;
  \end{hacs}
  %%
  Note how the declaration of the helper sets up the use of the "Computed" mechanism.
  %%
\end{example}

\begin{figure}[t]
  \begin{figureunit}[
      \caption{\emph{examples/MakeToken.hx}.}
      \label{fig:maketoken}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/MakeToken.hx}
  \end{figureunit}
\end{figure}

\begin{example}[string operations]
  Figure~\ref{fig:maketoken} illustrates working with strings. Notice the following:
  \begin{itemize}

  \item There is a strict distinction between the token "WORD" and the sort "Word". Something of
    sort "Word" can be written in syntax brackets---like ⟦X⟧ in the "Test" rule.

  \item A map attribute---called "dup" in the example---can have a token sort as the key sort, here
    "WORD".

  \item The "Test2" rules have two cases for processing a word: one for a word that is previously
    unmapped, and one for a word that was already mapped.

  \item A \emph{new token is constructed} for each word. This is achieved with a variant of the
    "⟨WORD…⟩" notation: normally, in such a construction, the "…" must be something of token sort
    "WORD"; however, as a special case, it is permissible to use an expression of "Computed" sort,
    as in lines 18 and 21 here.

  \item Lines 26--27 define the ``word concatenation helper'' of sort "Computed" with an expression
    that concatenates the two tokens as strings. This only works with arguments of token sort.

  \end{itemize}
\end{example}

Notice that it is \emph{not} presently possible to nest computations, \ie, insert a value in a
"Computed" expression that is computed by user functions (this will change in future versions of
\HAX). However, it is still possible to use helpers to achieve nested computations, as illustrated
by the simple desk calculator in the following example.

\begin{figure}[p]
  \begin{figureunit}[
      \caption{\emph{examples/Desk.hx}.}
      \label{fig:desk}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Desk.hx}
  \end{figureunit}
\end{figure}

\begin{example}
  Figure~\ref{fig:desk} implements a simple integer desk calculator. Notice how it uses the "$"
  marker to import a token as an integer value of sort "Computed". %$
\end{example}


\section{Examples}
\label{sec:examples}

Once the structure of a specification is clear, it is possible to start analyzing and manipulating the internal
representation.  This section presents some examples of this.

\begin{figure}[h]
  \begin{figureunit}[
      \caption{\emph{examples/IsCat.hx}: Finding cats.}
      \label{fig:cats}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right,texcl]{../samples/IsCat.hx}
  \end{figureunit}
\end{figure}

\begin{example}[finding cats]
  The small example in Figure~\ref{fig:cats} illustrates how to test for equality using a
  non-linear rule in line 12 combined with a ``catch-all'' default rule in line 13.

  It is not permissible to use "⟦cat⟧" directly in a pattern; patterns are restricted to
  \emph{syntactic cases} of the grammar. Also, note the definition of the "Boolean" sort with
  syntactic values rather than just constructors: this allows them to be printed.

  Here is a possible run using this command:
  %%
\begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs IsCat.hx}
...
$ \textcolor{blue}{./IsCat.run --scheme=IsCat --term="dog"}
False
$ \textcolor{blue}{./IsCat.run --scheme=IsCat --term="cat"}
True
  \end{code}
  %%$
\end{example}

\begin{figure}[p]
  \begin{figureunit}[
      \caption{\emph{examples/Symbols.hx}: Variations on symbols.}
      \label{fig:symbols}
    ]\small
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right,texcl]{../samples/Symbols.hx}
  \end{figureunit}
\end{figure}

\begin{example}
  Figure~\ref{fig:symbols} illustrates the different conventions for using plain tokens---here
  uppercase words---and using symbols---here lowercase words. In the comments, notice the difference
  in use in rule cases and as map keys and set members.
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs Symbols.hx}
...
$ \textcolor{blue}{./Symbols.run --scheme=Test --term="A A a a * A * a"}
A A 2 a a * s A 3 s_39 a * END END END s_46 s_46 s_46 
\end{code}
%%$
  Notice the following:
  \begin{itemize}
  \item Input tokens and symbols passed through, \eg, \texttt{A} and \texttt{a}.
  \item Repeated input tokens followed by count, \eg, \texttt{A 2} and \texttt{A 3}.
  \item Repeated input symbols followed by star, \eg,  \texttt{a *}.
  \item Rule symbols generated fresh for each use of a rule replacement, \eg, \texttt{s $≠$ s\_30 $≠$ s\_46}.
  \item Generated symbols consistent within each replacement, \eg, \texttt{s\_46}.
  \end{itemize}
\end{example}

\begin{figure}[ht]
  \begin{figureunit}[
      \caption{\emph{examples/WordSet.hx}: Sets of Words.}
      \label{fig:wordset}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right,texcl]{../samples/WordSet.hx}
  \end{figureunit}
\end{figure}

\begin{example}[set of words]
  %%
  One common task is to synthesize a set from some syntactic construct and subsequently search the
  set. Figure~\ref{fig:wordset} shows a small toy syntax that allows simple queries of word set
  membership.

  The example uses some new mechanisms for synthesizing the set:
  %%
  \begin{itemize}

  \item A helper "z" synthetic attribute contains a \emph{set} of word tokens, which is indicated by
    the attribute declaration "↑z{WORD}" in line~9.

  \item Line 10 associates a "z" set with all values of the syntactic sort "List".

  \item Lines 11 and 12 capture the synthesis of the set. Line 12 captures the simple case where a
    singleton list synthesizes a singleton set.

  \item Line 11 has a few more notations in play.  First, the \emph{pattern} part of the rule
    includes the inner pattern "↑z{:#ws}". This specifies that the special meta-variable ``":#ws"''
    captures all the existing members of the "z" set.  Second, the result of the rule is to add
    \emph{two} new things to the top level of the rule: "↑z{:#ws} ↑z{#w}". This adds \emph{both} the
    existing members (just matched) \emph{and} the one new member "#w" to the result set.

  \item Lines 24--26 are almost the same: the one difference is that 24 matches sets that contain
    the "#w" word, whereas 25--26 matches sets that do not because of the "¬" logical negation sign.
  \end{itemize}
  %%
  The example runs as follows:
  %%
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{hacs WordSet.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./WordSet.run --scheme=Check --term="a in a,b,b,a"}}
Yes, the list has a.
energon1[~]$ ^textcolor{blue}{^texttt{./WordSet.run --scheme=Check --term="Foo in Bar"}}
No, the list does not have Foo.
\end{code}
%%$
\end{example}

\begin{figure}[t!]
  \begin{figureunit}[
      \caption{\emph{examples/WordMap.hx}: Apply Word Substitution as Map.}
      \label{fig:wordmap}
    ]
    \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right,texcl]{../samples/WordMap.hx}
  \end{figureunit}
\end{figure}

\begin{example}[map of words]\label{ex:wordmap}
  Figure~\ref{fig:wordmap} shows how a map can be synthesized and then used as an environment. The
  pattern is similar to the set example, except this case not only synthesizes the map attribute "m"
  but also serves to ``copy'' it over to an inherited map---an environment---"e". Notice these
  extras:
  %%
  \begin{itemize}

  \item The map attribute is synthesized in lines 12--13, just like the set attribute was in the
    previous example. The only difference is that the map of course includes both a key and value.

  \item Line 23 simply captures all the ``mappings'' of the "m" attribute with the special
    ":#ms" pattern, which is then \emph{reused} to populate the "e" environment.

  \item Lines 26--34 combine the distribution of the inherited map with a recursive transformation
    that replaces words. The two rules for an initial "WORD" are mutually exclusive because the
    pattern in line 26 requires the word to be present with a mapping in the "e" attribute, whereas
    the pattern in line 31 requires that the word not be present.

  \end{itemize}
  Here is a run demonstrating the program:
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{hacs WordMap.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./WordMap.run --scheme=Substitute --term="a:b in a b b a"}}
b b b b
energon1[~]$ ^textcolor{blue}{^texttt{./WordMap.run --scheme=Substitute --term="k:v in a b c"}}
a b c
\end{code}
%%$
\end{example}

\begin{figure}[p]
  \begin{figureunit}[
      \caption{\emph{examples/WordSubst.hx}: Combining list, maps, and transformation.}
      \label{fig:wordsubst}
    ]\scriptsize
\begin{hacs}[xleftmargin=\parindent,xrightmargin=3em,numbers=right,texcl]
module org.crsx.hacs.samples.WordSubst {

// Grammar.
sort Units | ⟦ ⟨Unit⟩ ⟨Units⟩ ⟧ | ⟦⟧ ;
sort Unit | ⟦⟨Variable⟩=⟨NAT⟩⟧ | ⟦⟨Variable⟩⟧ | ⟦⟨NAT⟩⟧ | ⟦ { ⟨Units⟩ } ⟧ ;
sort Variable | symbol ⟦⟨ID⟩⟧ ;

token ID | [A-Za-z]+ ;
token NAT | [0-9]+ ;
space [\ \t\n\r] ;

// Helper Subst structure: lists of variable-NAT pairs.
sort Subst | MoreSubst(Variable, NAT, Subst) | NoSubst ;

// Append operation for Subst structures.
| scheme SubstAppend(Subst, Subst) ;
SubstAppend(MoreSubst(#var, #nat, #subst1), #subst2) → MoreSubst(#var, #nat, SubstAppend(#subst1, #subst2)) ;
SubstAppend(NoSubst, #subst2) → #subst2 ;

// Attributes.
attribute ↑subst(Subst) ;        // collected Subst structure
attribute ↓env{Variable:NAT} ;   // mappings to apply

// Top scheme.
main sort Units | scheme Run(Units) ;
Run(#units) → Run1(#units) ;

// Strategy: two passes.
// 1. force synthesis of subst attribute.
// 2. convert subst attribute to inherited environment (which forces replacement).

| scheme Run1(Units) ;
Run1(#units ↑subst(#subst)) → Run2(#units, #subst) ;

| scheme Run2(Units, Subst) ↓env ;
Run2(#units, MoreSubst(#var, #nat, #subst)) → Run2(#units, #subst) ↓env{#var : #nat} ;
Run2(#units, NoSubst) → Unitsenv(#units) ;

// Synthesis of subst.

sort Units | ↑subst ;
⟦ ⟨Unit #1 ↑subst(#subst1) ⟩ ⟨Units #2 ↑subst(#subst2)⟩ ⟧ ↑subst(SubstAppend(#subst1, #subst2)) ;
⟦ ⟧ ↑subst(NoSubst) ;

sort Unit | ↑subst ;
⟦v=⟨NAT#n⟩⟧ ↑subst(MoreSubst(⟦v⟧, #n, NoSubst)) ;
⟦v⟧ ↑subst(NoSubst) ;
⟦⟨NAT#n⟩⟧ ↑subst(NoSubst) ;
⟦ { ⟨Units#units ↑subst(#subst)⟩ } ⟧ ↑subst(#subst) ;

// Inheritance of env combined with substitution.

sort Units | scheme Unitsenv(Units) ↓env ;
Unitsenv( ⟦ ⟨Unit#1⟩ ⟨Units#2⟩ ⟧↑#s ) →  ⟦ ⟨Unit Unitenv(#1)⟩ ⟨Units Unitsenv(#2)⟩ ⟧↑#s ;
Unitsenv( ⟦ ⟧↑#s ) → ⟦ ⟧↑#s ;

sort Unit | scheme Unitenv(Unit) ↓env ;
Unitenv( ⟦v=⟨NAT#n⟩ ⟧↑#s) → ⟦v=⟨NAT#n⟩⟧↑#s ;
Unitenv( ⟦v⟧ ) ↓env{⟦v⟧:#n} → ⟦⟨NAT#n⟩⟧ ;
Unitenv( ⟦v⟧↑#s ) ↓env{¬⟦v⟧} → ⟦v⟧↑#s ;
Unitenv( ⟦⟨NAT#n⟩⟧↑#s ) → ⟦⟨NAT#n⟩⟧↑#s ;
Unitenv( ⟦ { ⟨Units#units⟩ } ⟧↑#s ) → ⟦ { ⟨Units Unitsenv(#units)⟩ } ⟧↑#s ;
}
\end{hacs}
  \end{figureunit}
\end{figure}

\begin{example}[word substitution]
  Figure~\ref{fig:wordsubst} shows a \HAX program to collect substitutions from a document and apply
  them to the entire document.  Notice the following:
  \begin{itemize}

  \item This example uses a typical two-pass strategy: first, one pass to collect the substitutions
    into a synthesized attribute, then a second pass where the full list of substitutions is applied
    everywhere.

  \item A choice has been made to synthesize the map as a \emph{data structure} instead of a native
    \HAX map (as in the previous Example~\ref{ex:wordmap}) because of the need here to \emph{append}
    two maps (in line 42), which is not supported for the native maps.  The synthesis happens in
    lines 41--49.

  \item The synthesized map is translated in list form into a native \HAX map before starting the
    second pass. Notice how "Run2" starts by recursing over the list of substitutions, inserting
    each into the carried inherited "env" map.  Since the map is consumed from left to right, the
    \emph{latest} substitution for any variable is always used.

  \item Since the inheritance schemes for "env" in lines 53--63 are doing a recursive traversal of
    the term, a benefit accrues from building the actual substitutions into the traversal.

  \item The inheritance rules carefully preserve the synthesized attributes only when the term does
    not change. In the present case, this is manifest by just the rule in line 59, not including the
    "↑#s" marker to capture and copy the synthesized attributes; in general, this should be
    considered for every situation.

  \end{itemize}
  %%
  Here is a run with this system:
  %%
\begin{code}[commandchars=\\\{\}]
energon1[~]$ \textcolor{blue}{hacs WordSubst.hx}
...
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="a=1 a"}
 a=1  1   
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="b a \{a=1 b=2\}"}
 2  1   {  a=1  b=2    }     
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="\{a=1 b=2 c=3\} a b c \{a=4\} a b c"}
  \{  a=1  b=2  c=3     \}   4  2  3   \{  a=4   \}   4  2  3         
\end{code}
  %%
  The last example shows how the latest substitution for "a" ``wins.''
  %%
\end{example}


\appendix\small

\section{Manual}\label{app:manual}

This appendix is an evolving attempt at giving a systematic description of \HAX.

\begin{manual}[grammar structure]\label{man:structure}
  A \HAX compiler is specified as a single \emph{.hx} module file with the following structure:
  %%
  \begin{hacs}[mathescape,xleftmargin=\parindent]
module $\text{\it\color{blue}modulename}$
{
  $\text{\it\color{blue}Declarations}$
}
  \end{hacs}
  %%
  where the \emph{modulename} should be a Java-style fully qualified class name with the last
  component capitalized and the same as the file base name, \eg, \verb|org.crsx.hacs.samples.First|
  is an allowed module name for a specification stored as \emph{First.hx}. The individual sections
  specify the compiler, and the possible contents are documented in the manual blocks below.
\end{manual}

\begin{manual}[lexical declarations]\label{man:token}
  %%
  A token is declared with the keyword "token" followed by the token (sort) name, a "|" (vertical
  bar), and a \emph{regular expression}, which has one of the following forms (in order of
  increasing precedence):
  %%
  \begin{enumerate}

  \item Several alternative regular expressions can be combined with further "|" characters.

  \item Concatenation denotes the regular expression recognizing concatenations of what matches the
    subexpressions.

  \item A regular expression (of the forms following this one) can be followed by a \emph{repetition
      marker}: "?" for zero or one, "+" for one or more, and "*" for zero or more.

  \item A simple word without special characters represents itself.

  \item A string in single or double quotes represents the contents of the string except that "\"
    introduces an \emph{escape code} that represents the encoded character in the string (see next item).

  \item A stand-alone "\" followed by an \emph{escape code} represents that character: escape codes
    include the usual C and Java escapes: "\n", "\r", "\a", "\f", "\t", octal escapes like "\177",
    special character escapes like "\\", "\'", \hacsc|\"|, and Unicode hexadecimal escapes like
    "\u27e9".

  \item A \emph{character class} is given in "[ ]", with these rules for the contents of the brackets:
   \begin{enumerate}
    \item If the first character is "^" then the character class is negated.
    \item If the first character (after "^") is "]" then that character is (not) permitted.
    \item A "\" followed by an \emph{escape code} represents the encoded character.
    \item The characters \verb|\'"| should always be escaped (this is a bug).
    \item Two characters connected with a "-" (dash) represent a single character in the indicated
      (inclusive) \emph{range}.
    \end{enumerate}
    Note that a character class cannot be empty. However, "[^]" is permitted and represents all
    characters.

  \item The "." (period) character represents the character class "[^\n]".

  \item A nested regular expression can be given in "( )".

  \item An entire defined token "T" can be included (by literal substitution, so recursion is not
    allowed) by writing "⟨T⟩" (the angle brackets are unicode characters U+27E8 and U+27E9). Tokens
    declared with "token fragment" can \emph{only} be used this way.

  \item The special declaration "space" defines what constitutes white space for the generated
    grammar. (Note that this does not influence what is considered space in the specification
    itself, even inside syntax productions.) A spacing declaration permits the special alternative
    "nested" declaration for nested comments, illustrated by the following, which defines usual
    C/Java style spacing with comments as used by \HAX itself:
\begin{hacs}[xleftmargin=\parindent]
space [ \t\f\r\n] | nested "/*" "*/" | "//" .* ;
\end{hacs}

  \end{enumerate}
  %%
  Notice that spacing is not significant in regular expressions, except (1) in character classes,
  (2) in literal strings, and (3) if escaped (as in "\ ").
  %%
\end{manual}

\begin{manual}[syntactic sorts]\label{man:syntax}
  %%
  Formally, \HAX uses the following notations for specifying the syntax to use for terms.
  %%
  \begin{enumerate}

  \item \HAX \emph{production names} are capitalized words. For example, "Exp" for the production of
    expressions.  The name of a production also serves as the name of its \emph{sort}, \ie, the
    semantic category that is used internally for abstract syntax trees with that root production.
    If particular instances of a sort need to be referenced later they can be \emph{disambiguated}
    with a "#"$i$ suffix, \eg, "Exp#2", where $i$ is an optional number or other simple word.

  \item A sort is declared by one or more "sort" declarations of the name, optionally followed by a
    number of \emph{abstract syntax production} alternatives, each starting with a~"|". A sort
    declaration sets the \emph{current sort} for subsequent declarations and, in particular, any
    stand-alone production alternatives. All sort declarations for a sort are cumulative.

  \item Double square brackets "⟦…⟧" (unicode U+27E6 and U+27E7) are used for \emph{concrete syntax}
    but can contain nested angle brackets "⟨…⟩" (unicode U+27E8 and U+27E9) with \emph{production
      references} like "⟨Exp⟩" for an expression (as well as several other things to be described
    later). For example, "⟦⟨Exp⟩+⟨Exp⟩⟧" describes the form where two expressions are separated by a
    "+" sign. Occurrences of tokens are referenced in the same way.

  \item Concrete syntax specification can include "¶" characters (Unicode U+00b6) to indicate where
    \emph{newlines} should be inserted in the printed output.

  \item A "@"$p$ for some natural number $p$ is a \emph{precedence indicator}, with higher numbers
    indicating higher precedence, \ie, tighter association. A precedence indicator can be added to a
    production reference (\ie, "⟨Exp@2⟩") or an entire concrete syntax production (\ie,
    "⟦⟨Exp⟩+⟨Exp⟩⟧@2"), indicating that either the appropriate subexpression or the entire
    alternative (as appropriate) is restricted to occur only at (at least) the specified precedence
    level.  (For details on the limitations of how the precedence and left-recursion mechanisms are
    implemented, see Appendix~\ref{app:limits}.)

  \item A "sugar ⟦…⟧→…" alternative specifies an equivalent form for existing syntax: anything
    matching the left alternative will be interpreted the same as the right one (which must have
    been previously defined); references must be disambiguated.

  \item If a production contains a reference to a token, where furthermore the token is defined such
    that it can end with a sequence of "_"$n$ units (an underscore followed by a count), then the
    sort case can be qualified as a "symbol" case, which implies:
    \begin{itemize}
    \item instances of the token can be used with "binds" (see below),
    \item the sort with the case can be used as the "as" sort of scopes (see below), and
    \item the sort with the case can be used as a key sort of map attributes (actual keys must be variables).
    \end{itemize}

  \end{enumerate}
  %%
\end{manual}

\begin{manual}[parsed terms]\label{man:parsed}
  The term model includes \emph{parsed terms}.
  %%
  \begin{enumerate}

  \item Double square brackets "⟦…⟧" (unicode U+27E6 and U+27E7) can be used for \emph{concrete
      terms}, provided the \emph{sort} is clear, either
    \begin{enumerate}
    \item by immediately prefixing with the sort (as in "Exp⟦1+2⟧"), or
    \item by using as the argument of a defined constructor (as "IsType(⟦mytype⟧)"), or
    \item by using as an attribute value, or
    \item by using as a top-level rule pattern or replacement term with a defined current sort.
    \end{enumerate}

  \item Concrete terms can contain nested raw terms (see below) in "⟨…⟩" (unicode U+27E8 and
    U+27E9). Such nested raw terms \emph{must} have an explicit sort prefix.

  \item The special term "error⟦…⟧" will print the error message embedded in "⟦…⟧", where one is
    permitted to embed "symbol"-declared variables in "⟨…⟩". Note that "error" terms will be
    evaluated \emph{when the rule is expanded}, thus only use "error" when matching the
    \emph{pattern} of the rule is sufficient to produce the error.

  \end{enumerate}
\end{manual}

\begin{manual}[raw terms, schemes, and rules]\label{man:raw}
  %%
  ``Raw'' declarations consist of the following elements:
  %%
  \begin{enumerate}

  \item A \emph{constructor} is a capitalized word (similar to a sort name but in a separate name
    space).

  \item A \emph{variable} is a lowercase word (subject to scoping, described below).

  \item A sort can be given a \emph{semantic production} as a "|" (bar) followed by a \emph{form},
    which consists of a constructor name, optionally followed by a list of the subexpression sorts
    in parentheses.

%%% "()"ed ","-separated list of
%%%    \emph{scope forms}, which each consist of a \emph{sort} optionally preceded by a \emph{binder
%%%      form}, which is a list of sorts followed by a "." (dot). Thus in the most general case, a
%%%    semantic production has the form
%%%    %%
%%%    \begin{equation*}
%%%      \texttt{|}~C~\texttt{(}
%%%      %~[S_{11},\cdots,S_{1n_1}]~S_1~\texttt{,}
%%%      …~\texttt{,}
%%%      ~S_{m1}\cdots S_{mn_m}~\texttt{.}~S_m
%%%      \texttt{)}
%%%    \end{equation*}
%%%    %%
%%%    with $C$ a constructor name and all $S_i$ and $S_{ij}$ sort names. The $S_i$ declares the
%%%    \emph{argument sort} for the $i$th argument of the construction term, and the $S_{ij}$ is the
%%%    \emph{binder sort} of the $j$th binder for the $i$th argument; $m$ is the \emph{arity} of the
%%%    construction and $n_i$ the \emph{rank} of the $i$th argument.

  \item A semantic production can be qualified as a "scheme", which marks the declared construction
    as a candidate for rewrite rules (defined below).

  \item A \emph{raw term} is either a \emph{construction}, a \emph{variable use}, or a
    \emph{meta-application}, as follows:
    %%
    \begin{enumerate}

    \item A \emph{construction} term is a constructor name followed by an optional parenthesized
      ","-separated list of \emph{scope arguments}, which each consist of a term optionally preceded
      by an optional \emph{binder list} of variables enclosed in "[]" (dot).  So in the most general
      case, a term looks like this:
     %% 
     \begin{equation*}
       C~\texttt{(}
       ~\texttt{[}\,x_{11}\texttt{,}…\texttt{,}x_{1n_1}\,\texttt{]}~t_1~\texttt{,}
       ~…\texttt{,}
       ~\texttt{[}\,x_{m1}\texttt{,}…\texttt{,}x_{mn_m}\,\texttt{]}~t_m
       ~\texttt{)}
     \end{equation*}
     %%
     The ``$C$-construction'' is said to have the \emph{subterms} $t_1,…,t_m$, and the arity $m$ and
     ranks $n_1…n_m$ must correspond to a semantic production.  If present, the binder prefix of
     each introduces the specified variables \emph{only} for the appropriate subterm modulo usual
     renaming, \ie, writing \texttt{A([x,y].x, [x,y].y)} and \texttt{A([a,b].a, [a,b].b)} and even
     \texttt{A([s,t].s, [t,s].s)} all denote the same term following the conventions of
     \emph{α-equivalence}.  In a scope argument $\texttt{[}x\texttt{]}t$ occurrences of $x$ in $t$
     are said to be \emph{bound} by the binder.

    \item A \emph{variable use} term is a variable, subject to the usual lexical scoping rules.

    \item A \emph{meta-application} term is a \emph{meta-variable}, consisting of a "#" (hash)
      followed by a number or word and optionally by a meta-argument list of ","-separated terms
      enclosed in "[]". Examples include "#t1" (with no arguments), "#[a,b,c]", and "#1[OK,#]".

    \end{enumerate}

  \item A term can have a \emph{sort prefix}. So the term
    $$"Type Unif(Type #t1, Type Float)"$$
    is the same as "Unif(#t1,Float)", provided "Unif" was declared with the raw production
    "|Unif(Type,Type)".

  \item A term can include embedded parsed terms. However, these must in general have a sort prefix,
    except when they are arguments to defined constructors.

  \item A \emph{rewrite rule} is a pair of terms separated by "→" (arrow, U+2192), with a few
    additional constraints on the rule $p→t$:
    \begin{itemize}

    \item $p$ must be a \emph{pattern}, which means it must be a construction term that has been
      declared as a "scheme" (syntactic or raw) and with the restriction that all contained
      arguments to meta-applications must be distinct bound variables.
      
    \item $t$ must be a \emph{contraction}, which means that all meta-applications in $t$ must have
      meta-variables that also occur in $p$ with the same number of meta-arguments.

    \end{itemize}
    Rule declarations must either occur with the appropriate current sort or have a pattern with a
    sort prefix.

  \item One rule per scheme can be prefixed with the qualifier "default". If so, the pattern cannot
    have any structure: all subterms of the pattern scheme construction must be plain
    meta-applications. Such a default rule is applied \emph{after} it has been ensured that all
    other rules fail for the scheme.

  \item Finally, a rule can be prefixed with the word "rule" for clarity.

  \end{enumerate}
  %%
  Rules are used for \emph{rewriting}, a definition of which is beyond the scope of this document;
  please refer to the literature on higher order rewriting for details~\cite{Jouannaud:klop2005,Klop+:tcs1993}.
  %%
\end{manual}

\begin{manual}[attributes and synthesis rules]\label{man:attributes}\leavevmode
  %%
  \begin{enumerate}

  \item Attributes are declared by "attribute" declarations followed by an \emph{attribute form} of
    one of the following shapes:
    %%
    \begin{enumerate}
    \item "↑name(ValueSort)" defines that the synthesized attribute "name" has "ValueSort" values;
    \item "↑name{KeySort}" defines that the synthesized attribute "name" is a set of "KeySort" values;
    \item "↑name{KeySort:ValueSort}" defines that the synthesized attribute "name" is a map from
      "KeySort" to "ValueSort" values;
    \item "↓name(ValueSort)", "↓name{KeySort}", and "↓name{KeySort:ValueSort}" similarly for
      inherited attributes;
    \end{enumerate}

  \item One can add a simple \emph{synthesized attribute} after a raw data term as
    "↑"\emph{id}"("\emph{value}")", where the \emph{id} is an attribute name and the \emph{value}
    can be any term of the appropriate sort.

  \item Simple \emph{inherited attributes} are added similarly after a raw scheme term as
    "↓"\emph{id}"("\emph{value}")".

  \item An \emph{inherited symbol table attribute extension} is added to a raw scheme term as
    "↓"\emph{id}"{"\emph{symbol}":"\emph{value}"}", where the \emph{symbol} is either a variable
    or a constant (of the appropriate sort).

  \item A \emph{synthesized attribute reference} has the simple form "↑"\emph{id}";" and declares
    that the current sort synthesizes \emph{id} attributes.

  \item A scheme declaration can include \emph{inherited attribute references} of the form
    "↓"\emph{id}, which declares that the scheme inherits the \emph{id} attributes.

  \item A \emph{synthesis rule} is a special rule of the form $t↑name(t')$, where the term $t$ may
    contain subterms with attribute constraints. The rule specifies how terms of the current sort
    and shape $t$ synthesize \emph{id} attributes.

  \item In \emph{rules}, one can use the special forms "↑#m", which captures \emph{all} synthesized
    attribute values; "↑t{:#ms}" ("↓t{:#ms}"), which captures the full set of keys or key-value
    mappings of the "t" synthesized (inherited) attribute.

  \end{enumerate}
  %%
  Inherited attributes are managed with regular rules (for schemes) with inherited attribute
  constraints and extensions.
  %%
\end{manual}

\begin{manual}[building and running]\label{man:run}\leavevmode
  To translate a \HAX script to an executable, run the \emph{hacs} command, which generates a number
  of files under a \emph{build} subdirectory, as well as the main script with a \emph{.run}
  extension.  The script accepts a number of options:
  %% 
  \begin{enumerate}

  \item \verb"--sort="\emph{Sort} sets the expected sort (and thus parser productions) for the input to
    \emph{Sort}. The input is read, normalized, and printed.

  \item \verb"--scheme="\emph{Constructor} sets the computation for the compiler to \emph{Constructor},
    which must be a unary raw scheme; the argument sort of \emph{Constructor} defines the parser
    productions to use.  The input is read, wrapped in the action, normalized, and printed.

  \item \verb"--term="\emph{text} uses the \emph{text} as the input.

  \item \verb"--input="\emph{file} (or just the \emph{file}) reads the input from \emph{file}.

  \item \verb"--output="\emph{file} sends the input to \emph{file} (the default is the standard output).

  \item \verb"--errors" (or \verb'-e') reports details of errors found by subprocesses.

  \item \verb"--keep" (or \verb'-k') does not remove temporary generated files.

  \item \verb"--interpret" uses the much slower \emph{interpreted} version of \HAX and activates the
    following options.

  \item \verb"--verbose="\emph{n} sets the verbosity of the underlying \CRSX rewrite engine to $n$. The
    default is 0 (quiet) but 1--3 are useful (above 3 you get a lot of low-level diagnostic output).

  \item \verb"--parse-verbose" activates (very!) verbose output from JavaCC of the parsing.

  \end{enumerate}
  %%
  You must provide one of \verb"--sort" or \verb"--scheme", and one of \verb"--term" and \verb"--input".

  Notice that the \emph{.run} script has absolute references to the files in the \emph{build}
  directory, so the latter should be moved with care.
\end{manual}


%%% \section{Bonus Examples}\label{appbonus}
%%% 
%%% Some additional \emph{untested} examples. \emph{Note: still need to be worked through.}
%%% 
%%% \begin{example}[\emph{hacs/samples/Bool.hx}]\leavevmode
%%% \inputhacs{../samples/Bool.hx}
%%% \end{example}
%%% 
%%% \begin{example}[\emph{hacs/samples/Deriv.hx}]\leavevmode
%%% \inputhacs{../samples/Deriv.hx}
%%% \end{example}


\section{Common Errors}\label{app:errors}

This appendix lists some of the more common of what can be called the ``error messages'' of
\HAX. Note that most of these only occur when \HAX is run with the \verb'--error' option.

\begin{error}[\HAX syntax]\leavevmode
  \begin{code}
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
 Encountered " "." ". "" at line 35, column 6.
Was expecting one of:
    <MT_Repeat> ...
    "%Repeat" ...
    <MT_Attributes> ...
  \end{code}
  This error message from the \verb'hacs' command indicates a simple syntax errors in the \emph{.hx}
  file.
\end{error}

\begin{error}[user syntax]\leavevmode
  \begin{code}
Exception in thread "main" java.lang.RuntimeException:
  net.sf.crsx.CRSException: net.sf.crsx.parser.ParseException:
mycompiler.crs: Parse error in embedded myDecSome term at line 867, column 42:
 ⟦ $TA_Let2b ⟨Dec (#d)⟩{ ⟨DecSome (#ds)⟩} ⟧ at line 867, column 42
 Encountered " "\u27e9" "\u27e8Dec (#d)\u27e9 "" at line 867, column 53
…
  \end{code}
  %%$
  This indicates a concrete syntax error in some parsed syntax---inside "⟦…⟧"---in the \emph{.hx}
  file. The offending fragment is given in double angles in the message. Check that it is correctly
  entered in the \HAX specification in a way that corresponds to a syntax production. Note that the
  line/column numbers refer to the generated \emph{build/…Rules.crs} file, which is not immediately
  helpful (this is a known bug). In error messages a sort is typically referenced as a lowercase
  prefix followed by the sort name---here "myDecSome" indicates that the problem is with parsing the
  "DecSome" sort of the "My" parser.
\end{error}

\begin{error}[precedence error]\leavevmode
  \begin{code}
Java Compiler Compiler Version 6.0_1 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file OrParser.jj . . .
Error: Line 170, Column 1: Left recursion detected: "N_Exp1... --> N_Exp2... --> N_Exp1..."
Detected 1 errors and 0 warnings.
  \end{code}
  %%$
  This suggests that a production breaks the precedence rule that all subterm precedence markers
  must be at least as high as the entire production's precedence marker, in this case between the
  "Exp@1" and "Exp@2" prededence markers, so presumably one of the rules for "Exp" with "@2" allows
  an "Exp" with "@1" as a first subterm.
\end{error}

\begin{error}[JavaCC noise]\leavevmode
  \begin{code}
Java Compiler Compiler Version ??.??_?? (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file FirstHx.jj . . .
Warning: Choice conflict involving two expansions at
         line 3030, column 34 and line 3033, column 8 respectively.
         A common prefix is: "{" <T_HX_VAR>
         Consider using a lookahead of 3 or more for earlier expansion.
Warning: Line 4680, Column 18: Non-ASCII characters used in regular expression.
Please make sure you use the correct Reader when you create the parser,
 one that can handle your character set.
File "TokenMgrError.java" does not exist.  Will create one.
File "ParseException.java" does not exist.  Will create one.
File "Token.java" does not exist.  Will create one.
File "SimpleCharStream.java" does not exist.  Will create one.
Parser generated with 0 errors and 1 warnings.
Note: net/sf/crsx/samples/gentle/FirstParser.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
  \end{code}
  These are ``normal'' messages from JavaCC. Yes, the choice conflict is annoying but is in fact safe.
\end{error}

\begin{error}[missing library]\leavevmode
  \begin{code}
gcc -std=c99 -g    -c -o crsx_scan.o crsx_scan.c
crsx.c:11:30: fatal error: unicode/umachine.h: No such file or directory
  \end{code}
  The \HAX tools only use one library in C: ICU. You should get the \emph{libicu-dev} package (or
  similar) for your system.
\end{error}

\begin{error}[meta-variable mistake]\leavevmode
  \begin{code}
Error in rule Tiger-Ty99_9148-1: contractum uses undefined meta-variable (#es)
Errors prevent normalization.
make: *** [pr3.crs-installed] Error 1
  \end{code}
  A rule uses the meta-variable "#es" in the replacement without defining it in the corresponding
  pattern.
\end{error}

\begin{error}[]\leavevmode
  \begin{code}
/home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
cookmain: crsx.c:528: bufferEnd: Assertion
   `(((childTerm)->descriptor == ((void *)0)) ? 0 :
        (childTerm)->descriptor->arity) == bufferTop(buffer)->index' failed.
/bin/sh: line 1: 14278 Aborted
  (core dumped) /home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
  \end{code}
  This indicates an arity error: a raw term in the \emph{.hx} file does not have the right number of
  arguments.
\end{error}

\begin{error}\leavevmode
  \begin{code}
// $Sortify
// $[Load, ".../build/edu/nyu/csci/cc/fall14/Pr2Solution.hx", "pr2solutionMeta_HxModule"]
Exception in thread "main" edu.nyu.csci.cc.fall14.TokenMgrError:
   Lexical error at line 184, column 31.  Encountered: "t" (116), after : "Call"
  \end{code}
  This indicates an undefined symbol of sort error in the \emph{.hx} file: the symbol
  starting with "Callt" is either undefined or used in a location where it does not match the
  required sort.
\end{error}

\begin{error}\leavevmode
  \begin{code}
// $Sortify
// $[Load, ".../build/edu/nyu/csci/cc/fall14/Pr2Solution.hx", "pr2solutionMeta_HxModule"]
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
   Encountered " ")" ") "" at line 255, column 112.
Was expecting one of:
    "," ...
  \end{code}
  This indicates an incorrect number of arguments in the \emph{.hx} file: here insufficient
  arguments (encountering a parenthesis instead of comma); a similar but opposite error is given
  when excess arguments are present.
\end{error}

\begin{error}\leavevmode
  \begin{code}
/home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
cookmain: crsx.c:528: bufferEnd: Assertion
   `(((childTerm)->descriptor == ((void *)0)) ? 0 :
        (childTerm)->descriptor->arity) == bufferTop(buffer)->index' failed.
/bin/sh: line 1: 14278 Aborted
  (core dumped) /home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
  \end{code}
  This indicates an arity error: a raw term in the \emph{.hx} file does not have the right number of
  arguments.
\end{error}

\begin{error}[]\leavevmode
  \begin{code}
« $Print-Check[
...
»
  \end{code}
  %%$
  This \emph{.run} script error indicates a request for a \verb'--scheme' "Check", which is not in
  fact declared as a "scheme" in the \emph{.hx} file.
\end{error}


\section{Limitations}\label{app:limits}

\begin{itemize}

\item At most one "nested" choice is permitted per "token" declaration.

\item It is not possible to use binders and left recursion in the same production with the same
  precedence.

\item Only \emph{immediate} left recursion is currently supported, \ie, left recursion should be
  within a single production. Specifically,
  \begin{hacs}
    sort A | ⟦ ⟨A⟩ stuff ⟧ | ⟦ other-stuff ⟧ ;
  \end{hacs}
  and 
  \begin{hacs}
    sort A | ⟦ ⟨A@1⟩ stuff ⟧@1 | ⟦ other-stuff ⟧@2 ;
  \end{hacs}
  are allowed, but
  \begin{hacs}
    sort A | ⟦ ⟨B⟩ a-stuff ⟧ | ⟦ other-a-stuff ⟧ ;
    sort B | ⟦ ⟨A⟩ b-stuff ⟧ | ⟦ other-b-stuff ⟧ ;
  \end{hacs}
  and 
  \begin{hacs}
    sort A  | ⟦ ⟨A@1⟩ stuff ⟧@2 | ⟦ other ⟧ ;
  \end{hacs}
  are not: prohibited cases involve indirect recursion (the latter case, where the inner left
  recursive precedence "@1" is less than the outer precedence "@2").

\item Productions can share a prefix but only within productions for the same sort, and the prefix
  must be precisely identical unit for unit, \ie,
  \begin{hacs}
    sort S | ⟦ ⟨A⟩ then ⟨B⟩ then C ⟧
           | ⟦ ⟨A⟩ then ⟨B⟩ or else D ⟧ ;
  \end{hacs}
  is fine, but
  \begin{hacs}
    sort S | ⟦ ⟨A⟩ then ⟨B⟩ then C ⟧
           | ⟦ ⟨A⟩ ⟨ThenB⟩ or else D ⟧ ;
    sort ThenB | ⟦ then ⟨B⟩ ⟧;
  \end{hacs}
  is not.

\item It is not possible to left-factor a binder (making it impossible for multiple binding
  constructs to have the same binder prefix).

\item Variables embedded in "error⟦…⟧" instructions must start with a lowercase letter.

\item When using the "symbol" qualifier on a reference to a token, then the token \emph{must} be
  defined such that it allows ending with a sequence of "_"$n$ for $n$ any natural number.

\item Symbols inside of "⟦…⟧" and raw variables outside the "⟦⟧" must still have different names or
  they will be identified.

\item Special terms like "error⟦…⟧" cannot be used as raw subterms.

\item Synthesized attribute patterns with pattern-matching in the attributes may not always work.

\item The "Computed" sort only works if there is at least one data or scheme constructor that
  \emph{returns} a value of "Computed" sort.

\end{itemize}


\bibliography{crs}


\end{document}


%% $Log: hacs-gently.tex,v $
%% Revision 1.29  2014/02/11 15:49:11  krisrose
%% Hash.crs fixed to allow linear use of hash.
%%
%% Revision 1.28  2014/01/26 21:14:34  krisrose
%% Compiled NumericEqual primitive fixed for rounding errors.
%%
%% Revision 1.27  2014/01/21 18:40:46  krisrose
%% Regenerated rulecompiler.
%%
%% Revision 1.26  2014/01/16 14:07:17  krisrose
%% Update compiled $[Decimal] to also handle double.
%%
%% Revision 1.25  2013/12/05 04:10:03  krisrose
%% Manual fix.
%%
%% Revision 1.24  2013/12/03 21:41:55  krisrose
%% Update of hacs.zip.
%%
%% Revision 1.23  2013/12/02 12:21:21  krisrose
%% Option fixes.
%%
%% Revision 1.22  2013/11/25 06:35:09  krisrose
%% HACS cleanup.
%%
%% Revision 1.21  2013/11/21 21:10:55  krisrose
%% Duplicate table row removed.
%%
%% Revision 1.20  2013/11/21 06:19:39  krisrose
%% HACS documentation rough version done.
%%
%% Revision 1.19  2013/11/21 04:01:32  krisrose
%% Newline support in HACS.
%%
%% Revision 1.18  2013/11/20 05:08:17  krisrose
%% HACS functional.
%%
%% Revision 1.17  2013/11/19 21:00:12  krisrose
%% first.hx refactored for hacs-gently runs again.
%%
%% Revision 1.16  2013/11/18 02:28:07  krisrose
%% HACS fully functional.
%%
%% Revision 1.15  2013/11/17 03:20:20  krisrose
%% reify option respects simple-terms.
%% Main .dr not output when modules generated.
%%
%% Revision 1.14  2013/10/30 04:28:28  krisrose
%% HACS synthesized attributes almost ready!
%%
%% Revision 1.13  2013/10/21 18:02:12  krisrose
%% HACS attribute grammar fixed.
%%
%% Revision 1.12  2013/10/17 16:51:43  krisrose
%% Locify avoids $[PassLocationProperties] when possible.
%%
%% Revision 1.11  2013/10/12 21:35:40  krisrose
%% Location and HACS work.
%% Checkpoint before major attribute distribution fix.
%%
%% Revision 1.10  2013/10/06 15:38:28  krisrose
%% First attempt at VariableUse with propserties.
%% Preparing for HACS for unmetaness.
%%
%% Revision 1.9  2013/09/30 11:06:42  krisrose
%% All HACS examples work again.
%%
%% Revision 1.8  2013/09/29 05:49:58  krisrose
%% Attributes next...
%%
%% Revision 1.7  2013/09/27 10:05:12  krisrose
%% Document RegExps.
%%
%% Revision 1.6  2013/09/25 17:14:15  krisrose
%% Change default action to Drop.
%%
%% Revision 1.5  2013/09/20 08:32:04  krisrose
%% Formatting.
%%
%% Revision 1.4  2013/09/20 08:17:57  krisrose
%% Drop diagnostic output.
%%
%% Revision 1.3  2013/09/20 08:11:58  krisrose
%% jar refresh
%%
%% Revision 1.2  2013/09/20 08:03:50  krisrose
%% Ready for the NYU students...
%%
%% Revision 1.1  2013/09/18 14:42:24  krisrose
%% Moved 'dragon' to 'gentle'.
%%
%% New.

%%---------------------------------------------------------------------
% Tell Emacs that this is a LaTeX document and how it is formatted:
% Local Variables:
% mode:latex
% fill-column:100
% TeX-master: t
% End:
