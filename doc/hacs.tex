%% Introduction to HACS.
%%
\documentclass[11pt]{article} %style: font size.
\input{setup}

\usepackage{catchfile}
\CatchFileDef{\VERSION}{../VERSION}{}
\def\setversion VERSION=#1 #2\relax{\def\version{#1\xspace}}\expandafter\setversion\VERSION\relax
%\usepackage[draft]{showlabels}

%% Style.
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{charter}
\SelectTips{eu}{}
\bibliographystyle{plainurl}

%% Topmatter.
\title{
  \emph{Introduction to}\\
  Compiler Generation Using \HAX%
  \thanks{This version describes \HAX version \version, printed \today.}
}
\author{
  Kristoffer H. Rose\\
  Two Sigma Investments/New York University
}

\begin{document}
\maketitle

\begin{abstract}\noindent
  Higher-order Attribute Contraction Schemes---or \HAX---is a language for programming compilers.
  With \HAX it is possible to create a fully functional compiler from a single source file.  This
  document explains how to get \HAX up and running, and walks through the code of a simple example
  with each of the main stages of a compiler as specified in \HAX: lexical analysis, syntax
  analysis, semantic analysis, and code generation.
\end{abstract}

\compacttableofcontents


\section{Introduction}\label{sec:intro}

\HAX abbreviates \emph{Higher-order Attribute Contraction Schemes}, which is a formal system for
symbolic rewriting extended with programming idioms commonly used when coding compilers. \HAX is
developed as a front-end to the \CRSX higher-order rewriting engine~\cite{crsx}, however, \HAX users
need not be concerned with the details of higher-order rewriting (even if those are, in fact, most
interesting).

A compiler written in \HAX consists of a single \emph{specification file} with a series of formal
sections, each corresponding to a stage of the compiler.  Each section is written in a formal style
suitable for that stage of the compiler. Specifically, \HAX supports the following notations:
%%
\begin{description}

\item[Regular Expressions.] Used to describe how an input text is partitioned into
  \emph{tokens}. The regular expressions of \HAX follows common
  conventions~\cite{Aho+:2006}. Section~\ref{sec:tokens} gives details of this notation.

\item[Context Free Grammars.] \HAX uses a form of BNF~\cite{NaurEtal:cacm1960} \emph{context-free
    grammars}; the notation has been tweaked to look more like \emph{templates} to allow for reuse
  of the notation in rewrite rules later. \HAX includes simple mechanisms for automatic resolution
  of operator precedence and productions using immediate recursion such that the transformation from
  token stream to abstract syntax can be formalized. Details in Section~\ref{sec:syntax}.

\item[Recursive Translation Schemes.] Simple translations in general, and code generation in
  particular, are traditionally achieved by \emph{recursive translation} from one abstract form to
  another.  \HAX includes special notations for defining such translations, described in
  Section~\ref{sec:schemes}, as well as a full programming language for defining auxiliary so-called
  ``semantic'' sorts and operators, detailed in Section~\ref{sec:eval}.

\item[Attribute Grammars.] Analyses can often be conveniently described in the style of
  \emph{syntax-directed definitions}~\cite{Aho+:2006}, originally introduced as \emph{attribute
    grammars}~\cite{Knuth:mst1968}, which describe how properties propagate through the abstract
  syntax tree.  Section~\ref{sec:collect} details how the basic propagation rules work for
  synthesized attributes, Section~\ref{sec:sdd} explains how inherited atributes are integrated such
  that full syntax-directed definitions can be encoded.

\item[Higher-order Abstract Syntax.] Most programming languages use \emph{lexical scoping}, and more
  and more compiler internal and target representations manipulate lexical scopes as part of their
  normal operation~\cite{MarlowPeyton-Jones:2010,Morrisett+:popl1998}. \HAX supports this by
  providing explicit support for \emph{higher-order abstract syntax}, and integrating this with
  support for attributes with mappings from variables to values for modeling \emph{symbol tables}.

\end{description}
%%
A typical compiler written with \HAX will typically involve multiple instances of each of the above,
as each used language, including intermediate and target languages, are typically specified using
grammars and have language-specific analyses, with the transformations between them specified as
translation schemes.

In the remainder of this document we introduce the most important features of the \HAX language by
explaining the relevant parts of the included \emph{First.hx} example, inspired by
\cite[Figure~1.7]{Aho+:2006}, as well as several other minor examples. %
We first show in Section~\ref{sec:run} how to install \HAX and run the example, before we go on to
how to write specifications. %
We explain lexical analysis in Section~\ref{sec:tokens}, %
syntax analysis in Section~\ref{sec:syntax}, %
basic recursive translation schemes in Section~\ref{sec:schemes}, %
semantic sorts, operations, and data, in Section~\ref{sec:eval}, %
bottom-up semantic analysis in Section~\ref{sec:collect}, %
general syntax-directed definitions in Section~\ref{sec:sdd}, %
and higher order abstract syntax in Section~\ref{sec:hoas}. %
In Section~\ref{sec:comp} we explain how primitive values can be manipulated, %
and in Section~\ref{sec:examples} we give several examples of how everything can be combined.
%%
Finally, %
Appendix~\ref{app:manual} has a reference manual, %
Appendix~\ref{app:errors} explains some of the (still) cryptic error messages, %
and Appendix~\ref{app:limits} lists some current limitations.

\paragraph*{Acknowledgements.} The author would like to thank my coteacher at NYU, Eva Rose, our
graders, Ma Weicheng and José Pablo Cambronero, as well as our students in the compiler construction
class, for constructive comments to and insightful questions on \HAX.\footnote{A special shout-out
  goes to John Downs for starting the Emacs \texttt{M-x hacs-mode} syntax highligting
  mode~\cite{git:hacsel} and Tyler Palsulich for a \HAX mode for Sublime
  Text~\cite{git:hacs-sublime}.}
%%
The implementation of \HAX would not have been remotely possible without the CRSX team at IBM:
Morris Matsa, Scott Boag, and especially Lionel Villard, who suffered through understanding and then
programming both core fragments of and using the CRSX system that is still used underneath \HAX.
%%
And finally \HAX owes its sanity to a collaboration with Cynthia Kop, both as an intern with the
Watson team in 2011, which created the polymorphic sort checker, and in her thesis
work~\cite{Kop:2012}, which led to our continuing collaboration.


\section{Getting Started}
\label{sec:run}

In this section we walk through the steps for getting a functional \HAX installation on your
computer.\footnote{Please report any problems with this procedure to \emph{hacs-bugs@crsx.org}.}

\begin{requirements}
  To run the \HAX examples here you need a *nix system (including a shell and the usual utilities)
  with these common programs: a Java development environment (at least Java~1.6~SE SDK, with "java"
  and "javac" commands); and a standard *nix development setup including GNU Make and a C99
  compiler. In addition, the setup process needs internet access to retrieve the \CRSX base
  system~\cite{crsx}, JavaCC parser generator~\cite{JavaCC}, and \emph{icu} Unicode C
  libraries~\cite{ICU}. Finally, these instructions are written for and tested on Ubuntu and Debian
  GNU/Linux systems using the "bash" shell; if your system is different some adaptation may be
  needed.
\end{requirements}

\begin{commands}[install \HAX]\label{com:all}
  %%
  If you have an old version of \HAX installed, then it is best to remove it first with a command like
  %%
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{rm -fr ~/.hacs hacs}}
\end{code}
  %%$
  Now retrieve the \emph{hacs-\version.zip} archive, extract it to a new directory, and install it, for
  example with the following commands:\footnote{User input is \textcolor{blue}{blue}.}
  %%
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{wget http://crsx.org/hacs-^version.zip}}
energon1[~]$ ^textcolor{blue}{^texttt{unzip hacs-^version.zip}}
energon1[~]$ ^textcolor{blue}{^texttt{cd hacs}}
energon1[hacs]$ ^textcolor{blue}{^texttt{make FROMZIP=1 install install-support}}
\end{code}
  %%
  These commands will need Internet access, using the wget command to retrieve support
  libraries.\footnote{Specifically, \HAX needs the \CRSX system, JavaCC parser generator, and ICU4C
    Unicode library. The setup is documented in \emph{src/Env.mk}.} The above command will install
  \HAX in a \emph{.hacs} subdirectory of your home directory. You can change this with the option
  \verb|prefix=|…  to (all uses of) \emph{make}; if so then make sure to replace occurrences of
  \verb|$HOME/.hacs| everywhere below with your chosen directory.

  The main \emph{make} command will take some time the first time (several minutes) and should end
  without error.

  Please check that your new installation works with these commands:
\begin{code}[commandchars=\^\{\}]
energon1[hacs]$ ^textcolor{blue}{^texttt{cd}}
energon1[~]$ ^textcolor{blue}{^texttt{mkdir myfirst}}
energon1[~]$ ^textcolor{blue}{^texttt{cd myfirst}}
energon1[~]$ ^textcolor{blue}{^texttt{cp $HOME/.hacs/share/doc/hacs/examples/First.hx .}}
energon1[~]$ ^textcolor{blue}{^texttt{$HOME/.hacs/bin/hacs First.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./First.run --scheme=Compile \}}
           ^textcolor{blue}{^texttt{--term="^{initial := 1; rate := 1.0; position := initial + rate * 60;^}"}}
  LDF T,  #1 
    STF initial, T
    LDF T_77,  #1.0 
    STF rate, T_77
    LDF T_1,  initial 
    LDF T_1_60,  rate 
    LDF T_2,  #60 
    MULF  T_2_21 ,  T_1_60 ,  T_2 
    ADDF  T_96 ,  T_1 ,  T_2_21 
    STF position, T_96
\end{code}
  %%
  Congratulations---you just built your first compiler!\footnote{Please do not mind the spacing --
    that is how \HAX prints in its present state.}
  %%
  Below we'll asume that you have issued the following command to make the main \emph{hacs} command
  available for use:
  %%
\begin{code}[commandchars=\^\{\}]
energon1[hacs]$ ^textcolor{blue}{^texttt{alias hacs=$HOME/.hacs/bin/hacs}}
\end{code}
  %%
  (It may be worth including this command in your setup, or including the \verb|$HOME/.hacs/bin|
  directory in your \verb|$PATH|.)
  %%
\end{commands}

\begin{example}[module wrapper]\label{ex:wrapper}%
  The source file for the \emph{First.hx} file used in the example above has the structure
  %%
  \begin{hacs}[mathescape,xleftmargin=\parindent]
/* Our first compiler. */
module org.crsx.hacs.samples.First
{
  // Sections.
  $\text{\it Lexical Analysis (Section~\ref{sec:tokens})}$
  $\text{\it Syntax Analysis (Section~\ref{sec:syntax})}$
  $\text{\it Semantic Analysis (Sections \ref{sec:schemes}, \ref{sec:collect} and \ref{sec:sdd})}$
  $\text{\it Code Generator (Section~\ref{sec:examples})}$
  $\text{\it Main (Section~\ref{sec:schemes})}$
}
  \end{hacs}
  Notice that \HAX permits C/Java style comments, and that the final component of the module name is
  the same as the base of the filename.
\end{example}

\begin{notation}
  The structure shown in Example~\ref{ex:wrapper} is formally explained in the appendix
  Manual~\ref{man:structure}, and the way to run \HAX in appendix Manual~\ref{man:run}.
\end{notation}

\begin{notation}[special Unicode characters]\label{man:unicode}
  \HAX uses a number of special symbols from the standard Unicode repertoire of characters, shown in
  Table~\ref{tab:unicode}.
\end{notation}

\begin{table}[h]
  \begin{displaymath}
    \begin{tabular}{c|c|c}
      \hline
      \hline
      \emph{Glyph} & \emph{Code Point} & \emph{Character} \Bigstrut \\
      \hline
      {¬} & U+00AC & logical negation sign \\
      {×} & U+00D7 & multiplication sign \\
      {÷} & U+00F7 & division sign \\
      {¶} & U+00B6 & paragraph sign \\
      \hline
      ↑ & U+2191 & upwards arrow \\
      → & U+2192 & rightwards arrow \\
      ↓ & U+2193 & downwards arrow \\
      \hline
      ⟦ & U+27E6 & mathematical left white square bracket \\
      ⟧ & U+27E7 & mathematical right white square bracket \\
      ⟨ & U+27E8 & mathematical left angle bracket \\
      ⟩ & U+27E9 & mathematical right angle bracket \\
      \hline
    \end{tabular}
  \end{displaymath}
  \caption{Unicode special characters used by \HAX.}
\label{tab:unicode}
\end{table}


\section{Lexical Analysis}
\label{sec:tokens}

Lexical analysis is the process of splitting the input text into tokens. \HAX uses a rather standard
variation of \emph{regular expressions} for this. This section explains the most common rules; the
appendix Manual~\ref{man:token} gives the formal rules.

\begin{example}[tokens and white space]\label{ex:lexical}
  %%
  Here is a \HAX fragment for setting up the concrete syntax of integers, basic floating point
  numbers, identifiers, and white space, for use by our simple language:
  %%
  \begin{hacs}[xleftmargin=\parindent,numbers=right,texcl]
// White space convention.
space [ \t\n] ;

// Basic nonterminals.
token INT      | ⟨DIGIT⟩+ ;
token FLOAT    | ⟨DIGIT⟩* "." ⟨DIGIT⟩+ ;
token ID       | ⟨LOWER⟩+ ('_'? ⟨INT⟩)* ;

// Special categories of letters.
token fragment DIGIT  | [0-9] ;
token fragment LOWER  | [a-z] ;
  \end{hacs}
  %% 
\end{example}

The example illustrates the following particulars of \HAX lexical expressions.

\begin{notation}[lexical syntax]\leavevmode
  %%
  \begin{enumerate}

  \item Declarations generally start with a keyword or two and are terminated by a ";" (semicolon).

  \item "token" declarations in particular have the "token" keyword followed by the token name and a
    regular expression between a "|" (vertical bar) and a ";" (semicolon). It defines the token as a
    \emph{terminal symbol} that can be used in other token declarations as well as the syntax
    productions described in the next section.

  \item Token names must be words that start with an upper case letter.

  \item A regular expressions is a sequence of units, corresponding to the concatenation of
    sequences of characters that match each one.  Each unit can be a \emph{character class} such as
    "[a-z]", which matches a single character in the indicated range (or, more generally, in one of
    a sequence of individual characters and ranges), a \emph{string} such as \hacsc|"."| or
    \hacsc|'foo'| (either kind of quotes is allowed), or a \emph{reference} to a token or fragment
    such as "⟨Lower⟩", enclosed in the special Unicode mathematical angle brackets (see
    Table~\ref{tab:unicode}).

  \item A "token fragment" declaration means that the defined token can only be used in other token
    declarations, and not in syntax productions.

  \item Every regular expression component can be followed by a repetition marker "?", "+", or~"*",
    and regular expressions can be \emph{grouped} with parentheses.

  \item The regular expression for white space is setup by "space" followed by the regular
    expression of what to skip -- here spaces, tabs, and newlines, where \HAX uses backslash for
    escaping in character classes with usual C-style language escapes.

  \end{enumerate}
  %%
  In addition, we have followed the convention of naming proper grammar terminals with ALL-CAPS
  names, like "INT", so they are easy to distinguish from non-terminals below. (Token declarations
  are not grammar productions: tokens cannot be recursively defined, and a token referenced from
  another token is merely a reference to the character sequences allowed by what is referenced.)
\end{notation}

Notice that while it is possible to make every keyword of your language into a named token in this
way, this is not necessary, as keywords can be given as literals in syntax productions, covered in
the next section.

\begin{commands}[lexical analysis]
  The fragment above is part of \emph{First.run} from Section~\ref{sec:intro}, which can thus be
  used as a lexical analyzer.  This is achieved by passing the \emph{First.run} command two
  arguments: a \emph{token sort} and a \emph{token term}.\footnote{The command has more options that
    we shall introduce as we need them.}  Execution proceeds by parsing the string following
  the syntax of the token. We can, for example, check the lexical analysis of a number:
\begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --sort=FLOAT --term=34.56}
34.56
\end{code}
  If there is an error, the lexical analyzer will inform us of this:
\begin{code}[commandchars=^\{\}]
$ ^textcolor{blue}{./First.run --sort=INT --term=34.56}
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
  Encountered " <T_FLOAT> "34.56 "" at line 1, column 1.
Was expecting:
    <T_INT> ...
\end{code}
  (where the trail of Java exceptions has been truncated: the important information is in the first
  few lines).
\end{commands}

As the example illustrates, all the token declarations together define how the processed stream of
characters is partitioned into terminal symbols without any need for consulting a grammar.
%%
The reference manual in Appendix~\ref{app:manual} gives further details.


\section{Syntax Analysis}
\label{sec:syntax}

Once we have tokens, we can use \HAX to program a syntax analysis with a grammar that specifies how
the input text is decomposed according to a \emph{concrete syntax} and how the desired
\emph{abstract syntax tree} (AST) is constructed from that. Notice that \HAX does not provide a
``parse tree'' in the traditional sense, \ie, a tree that represents the full concrete syntax parse:
only the AST is built.  Grammars are structured following the \emph{sorts} of AST nodes, with
concrete syntax details managed through precedence annotations and ``syntactic sugar''
declarations. The complete specification for grammars is in the appendix Manual~\ref{man:syntax}.

\begin{example}\label{ex:syntax}
  %%
  Here is another extract from our \emph{First.hx} example, with a syntax analysis grammar. Our
  small example source language merely has blocks, assignment statements, and a few forms of
  expression, like so:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right]
main sort Stat  | ⟦ ⟨ID⟩ := ⟨Exp⟩ ; ⟨Stat⟩ ⟧
                | ⟦ { ⟨Stat⟩ } ⟨Stat⟩ ⟧
                | ⟦⟧ ;

sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
          | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
          | ⟦ ⟨INT⟩ ⟧@2
          | ⟦ ⟨FLOAT⟩ ⟧@2
          | ⟦ ⟨ID⟩ ⟧@2
          | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;
  \end{hacs}
  %%
  The grammar structures the input as two sorts: "Stat" for statements and "Exp" for expressions.
  %%
\end{example}

The example grammar above captures the \HAX version of several standard parsing notions:
%%
\begin{description}

\item[Literal syntax] is indicated by the double ``syntax brackets,'' $⟦…⟧$.  Text inside $⟦…⟧$
  consists of three things only: spaces, literal character ``words,'' and references to
  non-terminals and predefined tokens inside $⟨…⟩$.  In this way, literal syntax is similar to macro
  notation or ``quasi-quotation'' of other programming languages.

\item[Syntactic sugar] is represented by the "sugar" part of the "Exp" sort declaration, which
  states that the parser should accept an "Exp" in parenthesis, identified as "#", and replace it
  with just that same "Exp", indicated by the "→ Exp#" part.  This avoids any need to think of
  parentheses in the generated AST as well as in rules below.

\item[Precedence rules] are represented by the "@"-annotations, which assign precedence and
  associativity to each operator. In our example, we have marked all references to the "Exp"
  non-terminal inside the productions for "Exp" with the lowest permitted priority in each case. So
  the first rule in line 5 says that the "+" operator is restricted on the right to only expressions
  with at least priority 1 (but not restricted on the left, so it is left recursive). The outer "@1"
  in line 6 states that all "*" expressions have priority 1, and the inner "@"-annotations allow
  left subexpressions of "*" with at least priority 1 (so "*" is also left recursive) whereas right
  subexpressions must have at least priority~2. Notice that left (right) recursion is identified by
  the \emph{leftmost (rightmost) unit in the production having the outer precedence}.

\end{description}
%%
The precedence notation allows us to define one sort per ``sort of abstract syntax tree node.''
This allows us to use a single kind of AST node to represent all the ``levels'' of expression, which
helps the subsequent steps.

{\small\begin{remark}
This is traditionally done with additional ``helper'' productions. We could, for example, recognize
the same "Exp" language as in the example by something like the following concrete \HAX
specification (where we have also made the sugar concrete):
%% 
\begin{hacs}[xleftmargin=\parindent]
sort Exp0  | ⟦ ⟨Exp0⟩ + ⟨Exp1⟩ ⟧ | ⟦ ⟨Exp1⟩ ⟧ ;
sort Exp1  | ⟦ ⟨Exp1⟩ * ⟨Exp2⟩ ⟧ | ⟦ ⟨Exp2⟩ ⟧ ;
sort Exp2  | ⟦ ⟨Int⟩ ⟧ | ⟦ ⟨Float⟩ ⟧ | ⟦ ⟨ID⟩ ⟧ | ⟦ ( ⟨Exp⟩ ) ⟧ ;
\end{hacs}%|
%%
However, this grammar generates a different result tree, where the nodes have the four different
sorts used instead of all being of the single "Exp" sort that the precedence annotations make
possible.  The transformed system also illustrates how \HAX deals with left recursion with
"@"-annotations: each becomes an instance of \emph{immediate left recursion}, which is eliminated
automatically using standard techniques.
\end{remark}}

Also note that the notation is admittedly dense: this is intentional, as we generalize this very
same notation to serve all the formalisms of the following sections.  Here are the formal rules.

\begin{notation}[syntax analysis]\leavevmode
  \begin{enumerate}

  \item Each sort is defined by a "sort" declaration followed by a number of \emph{productions}, each
    introduced by a "|" (bar). (The first "|" corresponds to what is usually written ``::='' or ``→''
    in BNF grammars.)  All productions for a sort define cases for that sort as a nonterminal, called
    the ``target'' non-terminal.

  \item Sort names must be words that start with an upper case letter.

  \item Concrete syntax is enclosed in "⟦…⟧" (``double'' or ``white'' brackets). Everything inside
    double brackets should be seen as \emph{literal syntax}, even "\" (backslash), \emph{except} for
    \HAX white space (corresponding to "[ \t\n\r]"), which is ignored, and references in "⟨…⟩" (angle
    brackets), which are special.

  \item References to \emph{terminals} (tokens) and \emph{nonterminals} (other productions) are
    wrapped in "⟨…⟩" (angle brackets).

  \item \emph{Precedence} is indicated with "@"$n$, where higher numbers $n$ designate higher
    (tighter) precedence. After every top-level "⟦⟧" and inside every "⟨⟩"-reference to the target
    non-terminal there is a precedence, which defaults to "@0".  The precedence of self references at
    the beginning and end of a production must be \emph{greater than or equal to} the outer
    precedence; at most one of the ends can have precedence equal to the outer one.

  \item The special "sugar" declaration expresses that the concrete syntax can use parentheses to
    raise the precedence of the enclosed expression to~2: it is the first example of a \emph{rewrite
      rule} with a "→" that we see, where we remark that the expression is marked "#" so we can use
    "Exp#" to the right of the "→" to indicate that the result of simplifying concrete syntax with
    parenthesis.  (In fact the general rule is that when an "→" is used then all sort specifiers must
    be ``disambiguated'' with distinct markers like "#" or "#5" in this way.)

  \end{enumerate}
  %% 
  Of all these rules, the one thing that is unique to parsing is the precedence notation with "@".
  When specifying a grammar then \emph{every} target non-terminal reference has a precedence, which
  determines how ambiguous terms should be parsed. So imagine that every "⟨⟩" contains a "@" marker,
  defaulting to "@0", and that every "⟦⟧" is terminated with a "@" marker, again defaulting to "@0".
\end{notation}

Notice that \HAX will do three things automatically:
%%
\begin{enumerate}
\item Eliminate immediate left recursion, such as found in the example.
\item Split the productions into subproductions according to the precedence assignments.
\item Left factor the grammar, which means that productions within a sort may start with a common
  prefix.
\end{enumerate}
%%
However, this is \emph{not} reflected in the generated trees: they will follow the grammar as
specified, so you do not need to be aware that this conversion happens.

\begin{commands}
  %%
  We can parse an expression from the command line:
  %%
\begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --sort=Exp --term="(2+(3*(4+5)))"}
2 + 3 * ( 4 + 5 )
\end{code}
  %%$
  Notice that the printout differs slightly from the input term as it has been ``resugared'' from
  the AST with minimal insertion of parentheses.
  %%
\end{commands}


\section{Abstract Syntax and Recursive Translation Schemes}
\label{sec:schemes}

In this section we explain how recursive translation schemes are expressed over the abstract syntax
implied by a grammar. The formal specification for this section is part of appendix
Manual~\ref{man:parsed} and \ref{man:raw}.

\begin{example}\label{ex:ast}%
  Consider the following subset of the example expression grammar of Example~\ref{ex:syntax}:
  %%
  \begin{hacs}[xleftmargin=\parindent]
sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
          | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
          | ⟦ ⟨INT⟩ ⟧@2
          | ⟦ ⟨FLOAT⟩ ⟧@2
          | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;
  \end{hacs}
  %%
  This grammar serves two purposes: to describe all token sequences that can be parsed, and to
  describe the structures that are generated from them. If we erase all the information that is
  purely there for parsing, we get
  %%
  \begin{hacs}[xleftmargin=\parindent]
sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp  ⟩ ⟧
          | ⟦ ⟨Exp  ⟩ * ⟨Exp  ⟩ ⟧
          | ⟦ ⟨INT⟩ ⟧
          | ⟦ ⟨FLOAT⟩ ⟧
          ;
  \end{hacs}
  %%
  We call this the \emph{abstract syntax} for the "Exp" sort, because all the helper information
  for the parser has been removed, leaving only the essential, structural information.
\end{example}

The abstract syntax, illustrated by the example, is relevant because the output of a \HAX-generated
parser is an \emph{abstract syntax tree}, or \emph{AST}, and thus all subsequent processing with
\HAX is based on this simplified structure.

Formally, the abstract syntax is obtained as follows:
%%
\begin{itemize}
\item Erase all "@"$n$ precedence markers.
\item Remove "sugar" productions.
\end{itemize}
%%
What remains are minimal productions with the essential information---the AST.

{\small\begin{remark}%
    The dragon book~\cite{Aho+:2006} achieves a similar but even bigger leap by associating the
    grammar with explicit precedence, written
    \begin{align*}
      E &→ E+T \mid T \\
      T &→ T*F \mid F \\
      F &→ (~E~) \mid \textbf{int} \mid \textbf{float}
    \end{align*}
    with the abstract syntax
    \begin{displaymath}
      E → E+E \mid E*E \mid \textbf{int} \mid \textbf{float}
    \end{displaymath}
    which additionally ``folds'' the $T$ and $F$ productions into $E$.
  \end{remark}}

Once we have such a description of the AST, then we can start writing code that operates on the
AST. This is where the notion of \emph{syntax-directed translation} comes in (although it would
perhaps be even better called ``abstract syntax-directed translation'').

\begin{definition}
  A scheme is \emph{syntax-directed} if it has one case per abstract syntax production.
\end{definition}

In practice, syntax-directed translation schemes defined in \HAX have one \emph{rule} per abstract
syntax production, as we shall see.

\begin{example}[scheme over syntax]\label{ex:leftmost}
  Consider our abstract expression grammar from Example~\ref{ex:ast}, and suppose that we are to
  define a new scheme called "Leftmost", which for an expression returns the leftmost number of the
  expression. We achieve this by first declaring the scheme as follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort Exp | scheme Leftmost(Exp) ;
  \end{hacs}
  %%
  The declaration states that the scheme takes one parameter of sort "Exp" in ()s, and delivers a
  result that is also of sort "Exp". (Later we shall see translations that convert from one sort to
  another.)

  To have "Leftmost" actually do something, we need to write a set of rules of the form
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(…) →    …;
  \end{hacs}
  %%
  where the two "…" in each case are replaced with a \emph{pattern} and a \emph{result}, in this
  case both of sort "Exp". The patterns are obtained directly from the abstract syntax productions
  simply by marking every "⟨⟩"-embedded token or nonterminal with a ``disambiguation'' mark, "#"$n$,
  giving us the following:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)         →   …;
Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)         →   …;
Leftmost(⟦⟨INT#⟩⟧)                    →   …;
Leftmost(⟦⟨FLOAT#⟩⟧)                  →   …;
  \end{hacs}
  %%
  Now we can express the right side of each translation rule using the now named fragments of the
  pattern, where we must remember that the result should always be of "Exp" sort:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)         →   Leftmost(Exp#1) ;
Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)         →   Leftmost(Exp#1) ;
Leftmost(⟦⟨INT#⟩⟧)                    →   ⟦⟨INT#⟩⟧ ;
Leftmost(⟦⟨FLOAT#⟩⟧)                  →   ⟦⟨FLOAT#⟩⟧ ;
  \end{hacs}
  %%
  In the first two rules, we pass the "#1" fragment, which is of "Exp" sort, into "Leftmost", which
  is guaranteed (by the declaration) to return something of "Exp" sort. In the last two rules, we
  explicitly return the argument form of sort "Exp". Notice that in the last two rules, the pattern
  sets up "#" to be of sort "INT" and "FLOAT", respectively, thus we cannot use these directly as
  the result, as they have the wrong sort.
\end{example}

We call a syntax-directedscheme like "Leftmost" a \emph{semantic} translation scheme because it is
declared without any new syntax, \ie, without any use of "⟦⟧".

\begin{commands}[invoke scheme]
  The "Leftmost" scheme above is also included in \emph{First.hx}.  Since it operates on a single
  syntactic expression, we can invoke the "Leftmost" scheme from the command line as follows:
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{./First.run --scheme=Leftmost --sort=Exp --term="2*3+4"}
 2
  \end{code}
  %%$
  We specify the sort of our input expression here because "Leftmost" takes an "Exp" argument, which
  is different from the main "Stat" sort.
\end{commands}

The notation for defining a syntax-directed schema is as follows.

\begin{notation}[syntax-directed schemes]\leavevmode
  \begin{enumerate}

  \item Set the result "sort" and add a declaration for the scheme. A scheme "S" of result sort "R"
    with argument sorts "A" and "B" is declared by defining
    \begin{hacs}
  sort R |  scheme S(A, B);
  \end{hacs}
  A scheme is named with a capitalized word (the same as sort names), optionally followed
  by some ``arguments'' in "()"s, where the declaration gives the sort for each argument.

\item To make the scheme "S" syntax-directed in, say, "A", then create a separate rule for each
  ``pattern case'' of "A", which is just each abstract syntax production for "A" with a "#"$n$
  marker after the token or nonterminal name to identify the subexpression of that token or
  non-terminal for use on the right side of the~"→". So if we have
  \begin{hacs}
  sort A  |  ⟦ a ⟨E⟩ b ⟧  |  ⟦ ⟨G⟩ c ⟨H⟩ ⟧ ;
  \end{hacs}
  then we should have
  \begin{hacs}
  sort R;
  S(⟦ a ⟨E#1⟩ b ⟧, #2)        →   …;
  S(⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, #3)    →   …;
  \end{hacs}
  The "#"-markers are called \emph{meta-variables} and are tied to one specific sort in a rule,
  whether it is by its marker inside syntax or position as an argument. In the example this means
  that in the first rule, "#1" is of sort "E" and "#2" of sort "B", whereas in the second rule,
  "#1" is of sort "G", "#2" of sort "H", and here "#3" is of sort "B". (Rules are very like "sugar"
  productions except we can have more than one rule for a given construct -- sugar is limited to a
  single rule that cannot depend on the inner structure of the construct.)

\item Each rule should be equipped with a result (right of the~"→") of the result sort, which can
  use the ``meta-variable'' "#"-named fragments using any (syntactic or semantic) mechanism we have
  for building something of the result sort. So if, for example, we have
  \begin{hacs}
  sort R  |  ⟦ x ⟨E⟩ ⟧  |  scheme Other(B);
  \end{hacs}
  then we can write
  \begin{hacs}
  sort R;
  S(⟦ a ⟨E#1⟩ b ⟧, #2)        →   ⟦ x ⟨E#1⟩ ⟧ ;
  S(⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, #3)    →   Other(#3) ;
  \end{hacs}

  %% \item Finally notice that If it helps readability then non-syntax meta-variables as well as syntax
  %%   units can be explicitly sorted, so the rules of the previous item can be written
  %%   \begin{hacs}
  %%   S(A⟦ a ⟨E#1⟩ b ⟧, B#2)        →   R⟦ x ⟨E#1⟩ ⟧ ;
  %%   S(A⟦ ⟨G#1⟩ c ⟨H#2⟩ ⟧, B#3)    →   Other(B#3) ;
  %%   \end{hacs}

\end{enumerate}
\end{notation}

\begin{example}[default rules]
  Since the last two cases of our "Leftmost" system in Example~\ref{ex:leftmost} above really just
  return the entire term again, we can instead write the system as
  %%
  \begin{hacs}
  sort Exp | scheme Leftmost(Exp) ;
  Leftmost(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧)  →     Leftmost(#1) ;
  Leftmost(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧)  →     Leftmost(#1) ;
  default Leftmost(#)  →  # ;
  \end{hacs}
  %%
  which explicitly calls out that if none of the two special cases apply then the scheme just
  returns the contained expression.
  %%
\end{example}

Up to this point we have consistently used the syntactic brackets "⟦…⟧" exclusively for ``stuff in
the input,'' essentially \emph{input data}, and semantic constructors (like "Leftmost" above) for
``things that can be computed,'' or \emph{functions}. As it turns out, \HAX does not, in fact,
insist on this separation. In particular, we can define ``syntactic schemes,'' which introduce new
syntax that has simplification rules associated with it.

Specifically, syntactic schemes are schemes that are defined using "⟦…⟧" notation. They are very
similar to "sugar" declarations, except that they can have multiple rules with pattern matching to
select which to apply, in contrast to sugar declarations can only have one generic case with a
single unconstrained meta-variable.

\begin{example}[syntactic scheme]\label{ex:flatten}
  Consider the syntactic list (data) sort
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort List | ⟦ ⟨Elem⟩ ⟨List⟩ ⟧ | ⟦⟧ ;
  \end{hacs}
  %%
  When working with complex lists expressions, we may sometimes need to \emph{flatten} nested
  lists. This can, of course, be done with a usual syntax-directed semantic scheme and a lot of
  nested "⟦⟨⟩⟧" constructions. However, we can instead choose to define a \emph{syntactic scheme},
  which is a scheme that is expressed in syntactic form but is in fact defined by rewrite
  rules. Flattening of lists can, for example, be defined as follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
sort List | scheme ⟦ { ⟨List⟩ } ⟨List⟩ ⟧ ;
⟦ { ⟨Elem#1⟩ ⟨List#2⟩ } ⟨List#3⟩ ⟧ → ⟦ ⟨Elem#1⟩ { ⟨List#2⟩ } ⟨List#3⟩ ⟧ ;
⟦ { } ⟨List#⟩ ⟧ → # ;
  \end{hacs}
  %%
  This creates a scheme that we can informally write as "⟦{_}_⟧" with the understanding that the
  two occurrences of ``"_"'' should be filled with lists as prescribed by the syntax specification
  in the "scheme" declaration. Notice that the two rules differ on the \emph{content} of the braces,
  and are clearly designed to fully eliminate all possible contents of the braces: this is
  essential, and we say that the scheme should be \emph{complete}. To be precise: the first "_"
  position in the "⟦{_}_⟧" function definition is filled differently in the two rules, namely once
  with each of the data shapes of lists -- indeed the rules are syntax-directed in the first list
  argument.
\end{example}

Syntactic schemes are very useful for working with output structures, the flattening scheme of
Example~\ref{ex:flatten} for example makes it much easier to manipulate sequences of, for example,
assembler instructions.

\begin{figure}[p]
  \inputhacs[texcl,mathescape,xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Stack.hx}
  \caption{\emph{samples/Stack.hx}.}
  \label{fig:stack}
\end{figure}

\begin{example}[stack code compiler]\label{ex:stack}
  Figure~\ref{fig:stack} shows the \HAX script \emph{Stack.hx}, which contains a compiler from
  simple expressions to stack code. Lines \ref{code:stack:gram1}--\ref{code:stack:gram2} contain a
  simple expression grammar, as already discussed. Lines
  \ref{code:stack:code1}--\ref{code:stack:code2} contain a separate grammar, this time for the
  output stack "Code" (the special "¶" marks indicate where to insert newlines when printing code,
  and are not part of the syntax). Lines \ref{code:stack:flat1}--\ref{code:stack:flat2} contain a
  flattening syntax scheme (as in Example~\ref{ex:flatten}) for sequences of instructions.  Finally,
  lines \ref{code:stack:comp1}--\ref{code:stack:comp2} contain the syntax-directed translation from
  expressions to stack code. We use it in the usual way,
  %%
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs Stack.hx}
HACS \version
…
$ \textcolor{blue}{./Stack.run --scheme=Compile --term="(1+2)*(3+4)"}
  PUSH 1 
  PUSH 2 
  ADD 
  PUSH 3 
  PUSH 4 
  ADD 
  MULT 
  \end{code}
  %%
  Simple code generation tasks can be handled with recursive schemes such as this one, however,
  serious compilation tasks will require proper attributes and semantic helper structures, detailed
  in the following sections.
  %%
\end{example}


\section{Semantic Data, Operators, and Evaluation}
\label{sec:eval}

For most compilers, simple recursive translations such as presented above, do not suffice. We shall
need more complex programming tools to support more sophisticated analysis and transformations.

Specifically, note that rules for syntax-directed schemes are similar to definitions by case as in
functional programming. However, there are important differences that we shall detail here, which
sometimes make \HAX pick rules in ways that are different from what a functional programming
language would do.

As discussed in the previous section, it is possible to have functions written with syntactic
brackets, and similarly data written with semantic constructors, called ``semantic data.''  Semantic
data forms are introduced as non-syntax (so-called ``raw'') notations that can be used in patterns.

\begin{example}[semantic data constants]\label{ex:unif}
  %%
  Another fragment of the \emph{First.hx} example has the semantic sorts and operations that are
  used. For our toy language that just means the notion of a \emph{type} with the way that types are
  ``unified'' to construct new types.
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent,numbers=right]
// Types to associate to AST nodes.
sort Type | Int | Float ;

// The Type sort includes a scheme for unifying two types.
| scheme Unif(Type,Type) ;
Unif(Int, Int) → Int;
Unif(#1, Float) → Float;
Unif(Float, #2) → Float;
  \end{hacs}
  %% 
  The code declares a new sort, "Type", which is a \emph{semantic sort} because it does not include
  any syntactic cases: all the possible values (as usual listed after leading "|"s) are simple
  \emph{term structures} written without any ⟦⟧s.  Structures are written with a leading
  ``constructor,'' which should be a capitalized word (the same as sort and scheme names),
  optionally followed by some arguments in "()"s, where the declaration gives the sort for each
  argument (here there are none).

  The rules for the "Unif" scheme above can, for example, be used to simplify a composite term as
  follows:
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=\parindent]
Unif(Unif(Int, Float), Int) → Unif(Float, Int) → Float
  \end{hacs}
  %%
  Note how overlaps are allowed but please do verify determinacy, \ie, if a particular combination
  of arguments can be subjected to two rules then they should give the same result! In this example
  it happens because the term "Unif(Float,Float)" can be rewritten by both the rule in line 7 and 8,
  but it does not matter, because the result is the same. Also note that \emph{the order of the
    rules does not matter}: the three rules in lines 6--8 above can be given in any order.
  %% 
\end{example}

\begin{figure}[p]
  \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/SZ.hx}
  \caption{Peano numbers with addition and lists (\emph{SZ.hx}).}
  \label{fig:sz}
\end{figure}

\begin{example}
  Consider the complete \HAX script in Figure~\ref{fig:sz}. Lines 3--12 define a syntax of
  expressions with lists, sums, external references, and the Peano convention that "0" stands for
  itself and "s"$n$ stands for $n+1$. Line 15 defines a new semantic data sort, "Value", which
  represents the same information but outside of syntax brackets. Line 18 defines a scheme as
  before, except now it is defined in lines 19--23 also over the data forms with arguments. In lines
  25--31 we give a traditional syntax directed translation from the syntactic "Exp" format to the
  semantic "Value" form. However, if we try to run the "Load" scheme of the script on a single
  example like this:
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs SZ.hx}
…
$ \textcolor{blue}{./SZ.run --scheme=Load --term="s 0+s 0"}
« $Print2-SZ$Value[SZ$Value_Succ[SZ$Value_Succ[SZ$Value_Zero]], 0] »
  \end{code}
  we get the right result, which is the Peano numeral for $2$, however, \emph{it is not printed
    properly}: the "Succ" constructors show up in internal form. This is because \emph{semantic data
    structures have no syntax so cannot be printed}. The proper way is to use the "Calc" script,
  which translates back to external form, like this:
  \begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs SZ.hx}
…
$ \textcolor{blue}{./SZ.run --scheme=Calc --term="s 0+s 0"}
 s  s  0   
  \end{code}
  we get the right result. This uses the "Calc" scheme defined in lines 34--35 and the "Unload"
  scheme from line~37.
\end{example}

However, while \HAX definitions for schemes and data sorts look like function definitions and
algebraic data types, they are evaluated differently than those would be in functional programming.
Specifically, \HAX allows rules to match and be applied \emph{before} the matched arguments have
been evaluated, which can give results that are surprising to the uninitiated. Specifically, \HAX is
neither ``eager'' nor ``lazy,'' although it is closer to the latter.

Consider this context:
\begin{hacs}[xleftmargin=\parindent]
sort Bool | True | False | scheme Or(Bool, Bool);
\end{hacs}
We may think that we can define the "Or" scheme as follows
\begin{hacs}[texcl,xleftmargin=\parindent]
Or(False, False) → False ;        //1
default Or(#1, #2) → True ;       //2 \textcolor{red}{problematic}
\end{hacs}
This is only correct under one condition: that the arguments to "Or" are never computed by other
schemes. Consider, for example, the term
\begin{hacs}[xleftmargin=\parindent]
Or(Or(False, False), False)
\end{hacs}
For this term, \HAX is allowed to decide that "//1" cannot be immediately applied, because \emph{at
  this time} the first parameter is different from "False" , and \HAX may then decide to instead use
"//2". This is probably not what is intended.

The best way to avoid this is to fully expand the observed parameters. For "Or" that leads to the
classic definition,
\begin{hacs}[xleftmargin=\parindent]
Or(False, #2) → #2 ;
Or(True, #2) → True ;
\end{hacs}
With rules that check for equality between several parameters this requires further care. Consider,
for example, the following additions that search a list for a specific boolean value:
\begin{hacs}[texcl,xleftmargin=\parindent]
sort Bools | Cons(Bool, Bools) | Nil ;
sort Bool | scheme Find(Bools, Bool) ;
Find(Cons(#b, #bs), #b) → True ;                      //3
Find(Cons(#b, #bs), #b2) → Find(#bs, #b2) ;           //4 \textcolor{red}{problematic}
default Find(#bs, #b) → False ;                       //5 \textcolor{red}{problematic}
\end{hacs}
These rules are problematic for two reasons. The first is that \HAX can \emph{always} pick rule
"//4" over rule "//3", and in this way give a false negative. We discuss this below. Second, it is
also problematic because if the list is computed, then the "default" rule can be accidentally
used. Say you have an expression like
\begin{hacs}[xleftmargin=\parindent]
Find(Append(...), True)
\end{hacs}
(with some suitable definition of "Append"). Because this does not immediately match "//3+4", \HAX
can decide \emph{at that time} to apply rule "//5", which of course is wrong.

A fix for the second issue is to instead use
\begin{hacs}[texcl,xleftmargin=\parindent]
Find(Nil, #b) → False ;                                         //6
Find(Cons(#b, #bs), #b) →   True ;                              //7 \textcolor{red}{careful}
default Find(Cons(#b, #bs), #b2) →   Find(#bs, #b2) ;           //8
\end{hacs}
This avoids issues where the list parameter is unevaluated: they will not match any rule, so no
wrong simplification is done.

However, it still requires that you must be careful that \emph{both} of the pieces that are compared
in "//7" are \emph{always} in data form. To see why, consider an expression like
\begin{hacs}[xleftmargin=\parindent]
Find(Cons(Or(...), ...), True)
\end{hacs}
Again this does not immediately fit "//7" without evaluating the "Or(...)"  subterm, so \HAX may
instead use "//8", leading to a false negative result.

The fix to this is to guarantee that elements of a list are always data, and not unevaluated. One
way to achieve this is to use a special constructor when building such lists. If the expression is
\begin{hacs}[xleftmargin=\parindent]
Find(EvalCons(Or(...), ...), True)
\end{hacs}
with
\begin{hacs}[xleftmargin=\parindent]
sort Bools | scheme EvalCons(Bool, Bools) ;
EvalCons(T, #bs) →   Cons(T, #bs) ;
EvalCons(F, #bs) →   Cons(F, #bs) ;
\end{hacs}
then the problem does not occur, because the "Or" is forced to be evaluated before the value is
stored in the list. If you then use "EvalCons" instead of "Cons" everywhere, then "//6-8" are safe
to use. Note that "//3-5" are still not safe because they may observe an unevaluated "EvalCons",
which would still allow picking the wrong rule. The "EvalCons" approach has the advantage of stating
explictly what is forced.

Finally, the above is so common that it is supported with a special declaration that can be added to
achieve the same effect: if the rules are written as
\begin{hacs}[xleftmargin=\parindent]
Find(Nil, #b) → False ;
[data #b]  Find(Cons(#b, #bs), #b) →   True ;
default Find(Cons(#b, #bs), #b2) →   Find(#bs, #b2) ;
\end{hacs}
with the \emph{option prefix} "[data #b]", then the rule will force complete evaluation of the "#b"
component before it is determined that the second rule does not apply and thus that the "default"
rule may be used. Indeed it is seen as good practice to use the "data" option for all instances
where two subterms are compared for equality.


\section{Synthesizing Information}
\label{sec:collect}

\HAX has special support for assembling information in a ``bottom-up'' manner, corresponding to how
\emph{synthetic attributes} are used in compiler specifications written as syntax-directed
definitions, \aka SDD or \emph{attribute grammars}.  In this section we explain \emph{how} you
convert any SDD synthetic attribute definition into a \HAX one, introducing the necessary \HAX
formalisms as we go.  The material here is specified in the appendix Manual~\ref{man:attributes}.

\begin{example}
  Consider the following single definition of the synthesized attribute $t$ for expressions $E$:
  %% 
  \begin{equation}
    \begin{array}{l|l}
      \hline
      \hline
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut\\
      \hline
      E → E_1 + E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \Bigstrut\\
      \hline
    \end{array}
    \tag{E1}
  \end{equation}
  %% 
  The rule is ``S-attributed'' because it exclusively relies on synthesized attributes. This allows us
  to express it directly in \HAX as follows.
  %% 
  \begin{enumerate}

  \item The first thing to do is declare the attribute and associate it with the $E$ sort.
    \begin{hacs}
   attribute ↑t(Type);
   sort E | ↑t ;
    \end{hacs}
    the "↑" indicates ``synthesized'' because the attribute moves ``up'' in the tree. The
    declaration of the attribute indicates with "(Type)" that the \emph{value} of the synthesized
    attribute is a "Type".  Attributes are always named with lower case names.
  
  \item The second thing to do is to create patterns from the abstract syntax production, just like
    in the previous section: the pattern for the single production is
    \begin{hacs}
   ⟦ ⟨E#1⟩ + ⟨E#2⟩ ⟧
    \end{hacs}
    where we have used the subscripts from \thetag{E1} as "#"-disambiguation marks.

  \item Next add in \emph{synthesis patterns} for the attributes we are reading.  Each attribute
    reference like $E_1.t$ becomes a pattern like "⟨E#1 ↑t(#t1)⟩", where the meta-variables like
    "#t1" should each be unique.  For our example, this gives us
    \begin{hacs}
   ⟦ ⟨E#1 ↑t(#t1)⟩ + ⟨E#2 ↑t(#t2)⟩ ⟧
    \end{hacs}
    which sets up "#t1" and "#t2" as synonyms for $E_1.t$ and $E_2.t$, respectively.

  \item Finally, add in the actual synthesized attribute, using the same kind of pattern at the
    \emph{end} of the rule (and add a ";"), and we get
    \begin{hacs}
   ⟦ ⟨E#1 ↑t(#t1)⟩ + ⟨E#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2)) ;
    \end{hacs}
    %% 
    This is read ``When considering an "E" (the current sort) which has the (abstract syntax) shape
    $⟦⟨E⟩+⟨E⟩⟧$ where furthermore the first expression has a value matching "#t1" for the
    synthesized attribute "t", and the second expression has a value matching "#t2" for the
    synthesized attribute "t", then the entire expression should be assigned the value
    "Unif(#t1,#t2)" for the synthesized attribute~"t".''

  \end{enumerate}
  %% 
  If we assume that "Unif" refers to the semantic scheme defined in Example~\ref{ex:unif}, then we
  are done.
\end{example}

\begin{notation}[value synthesis rules]\leavevmode
  \begin{enumerate}

  \item Synthesized simple attributes are declared with declarations like $"attribute"↑a(S);$ with
    $a$ a lower case attribute name and $S$ the sort name.

  \item The synthesized attribute $a$ is associated with a sort by adding the pseudo-production
    ${|}↑a$ to the sort declaration.

  \item Synthesis rules as discussed here have the form $p↑a(r)$ where $p$ is like a pattern in a
    rule but with the change that it must be a \emph{data} instance (not a scheme), $a$ is an
    attribute name, and $r$ should be a replacement of the value sort of~$a$.

  \end{enumerate}
\end{notation}

\begin{example}\label{ex:collect}
  %%
  In Example~\ref{ex:syntax}, we presented the abstract syntax of the small language processed by
  \emph{First.hx}. A type analysis of the expressions of the language (for now excluding variables)
  might look as follows as a standard SDD (syntax-directed definition), where we use $E$ for the
  "Exp" non-terminal, and one attribute: $E.t$ is the synthesized "Type" of the expression $E$.  In
  the notations of \cite{Aho+:2006}, the SDD can be specified something like this:
  %% 
  \begin{equation*}
    \begin{array}{l|l}
      \hline
      \hline
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut\\
      \hline
      E → E_1 + E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \Bigstrut\\[\jot]
      \quad\mid E_1 \ast E_2 & E.t = \op{Unif}(E_1.t, E_2.t) \\[\jot]
      \quad\mid \textbf{int} & E.t = \op{Int} \\[\jot]
      \quad\mid \textbf{float} & E.t = \op{Float} \\[\jot]
      \hline
    \end{array}
  \end{equation*}
  %%
  where we assume as before that $\op{Unif}$ is defined as discussed in Example~\ref{ex:unif}.
  %%
  We can convert this SDD to the following \HAX (using the proper names for the sorts as actually
  found in \emph{First.hx}):
  %%
\begin{hacs}[xleftmargin=\parindent,numbers=right,texcl]
attribute ↑t(Type);       // synthesized type

sort Exp | ↑t ;           // expressions have an associated synthesized type, $E.t$

// Synthesis rules for $E.t$.
⟦ ⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨Exp#1 ↑t(#t1)⟩ * ⟨Exp#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨INT#⟩ ⟧ ↑t(Int);
⟦ ⟨FLOAT#⟩ ⟧ ↑t(Float);
\end{hacs}
  %%
  Line 1 declares the value of the synthesized "t" attribute to be a "Type".
  %%
  Line 3 associates the synthetic attribute "t" to the "Exp" sort: all synthetic attributes are
  associated with one or more abstract syntax sorts.
  %%
  The remaining lines 5--9 are \emph{synthesis rules} that show for each form of "Exp" what the
  value should be, based on the values passed ``up'' from the subexpressions; these are generated
  from the abstract syntax patterns and synthesis semantic rules, as discussed above.
\end{example}

\begin{figure}[p]
  \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Bool.hx}
  \caption{Synthesizing the value of a Boolean expression.}
  \label{fig:bool}
\end{figure}

\begin{example}\label{ex:bool}
  Figure~\ref{fig:bool} shows an implementation of Boolean algebra implemented with synthesized
  attributes. Notice how the main "Eval" scheme is defined to ``request'' the value of the
  synthesized attribute, which then triggers the evaluation of the boolean expression.
\end{example}

Finally, note that if you have multiple synthetic attributes then a synthesis rule \emph{only} adds
one new attribute to the program construct in question, it does not remove any other attributes
already set for it (we'll have examples of such systems below).


\section{Full Syntax-Directed Definitions with Environments}
\label{sec:sdd}

We have now described how to use ``top-down'' recursive schemes with positional parameters and
``bottom-up'' synthesis of named attributes with simple values. The last component used in
traditional compiler specification is the use of \emph{inherited} named attributes, which are
distributed top-down, like parameters. \HAX supports this with a hybrid combination of implicit
named parameters for recursive schemes, that we describe here. The section introduces features of
\HAX covered in the appendix Manual~\ref{man:attributes}.

One of the main uses of inherited attributes in formal compiler specifications is \emph{symbol
  management}. We shall focus here on the \HAX notion of \emph{environment}, which fills this niche
and cannot be easily achieved with usual positional arguments to recursive schemes.

\begin{example}
  In compiler construction formalization we express this using an SDD with an inherited attribute that
  for each node in the AST associates the appropriate symbol table for that node. Consider the
  following three simple semantic rules, which demonstrate this approach:
  %% 
  \begin{equation*}
    \begin{array}{l|ll}
      \hline
      \hline
      \textsc{Production}  & \textsc{Semantic Rules} \Bigstrut&\\
      \hline
      S → T_1 ~ \textbf{id}_2 = E_3; S_4
      &
      E_3.e = S.e; 
      S_4.e = \op{Extend}(S.e, \textbf{id}_2.sym, T_1) \Bigstrut\quad
      &\thetag{1}
      \\[\jot]
      E → E_1 + E_2
      &
      E_1.e=E.e; E_2.e=E.e
      &\thetag{2}
      \\[1pt]
      \quad~\mid \textbf{id}_1
      &
      E.t = \op{Lookup}(E.e, \textbf{id}_1.sym)
      &\thetag{3}
      \\[\jot]
      \hline
    \end{array}
  \end{equation*}
  %% 
  Rule \thetag{1} handles declarations in our toy language: it creates a new environment (or symbol
  table) that \emph{extends} the environment from the context, the \emph{inherited} environment~$S.e$,
  with a new coupling from the symbol of $\textbf{id}_2$ to the type~$T_1$: the notation means that
  the new environment $S_4.e$ contains the new mapping as well as all the bindings from $S.e$
  (\emph{except} any previous mapping of the symbol $\textbf{id}_2$).
  %% 
  Rule \thetag{2} merely expresses that the $e$ attribute is inherited from the context to both
  subexpressions of addition expresions.
  %% 
  Finally, rule \thetag{3} uses the environment to attach a synthesized attribute $t$ onto an $E$
  which contains an \textbf{id} token (but is not otherwise used in these rules). Specifically, the
  notation is meant to suggest that the type value is obtained by a \emph{lookup} of the mapping for
  the symbol of $\textbf{id}_1$.

  Here are the steps to follow to translate the above SDD fragment to \HAX.
  %% 
  \begin{enumerate}

  \item The first thing to do is to encode the grammar (assuming an existing $T$ sort of types and
    "ID" tokens as before):
    \begin{hacs}
   main sort S  | ⟦ ⟨T⟩ ⟨ID⟩ = ⟨E⟩ ; ⟨S⟩ ⟧ ;
   sort E  | ⟦ ⟨E⟩ + ⟨E@1⟩ ⟧   | ⟦ ⟨ID⟩ ⟧@1 ;
\end{hacs}
%% 
The "ID" sort is declared as a "symbol" sort so we can use it in maps next; the rest of the
syntax specification follows the rules given previously. We assume that the "T" sort is defined
elsewhere with the possible types.

\item Once the grammar is defined, we can declare the new attribute:
  \begin{hacs}
   attribute ↓e{ID : T} ;
\end{hacs}
Like synthesized attributes, inherited attributes are always given lower case names.
The "{ID : T}" part declares the value of the attribute to be a \emph{mapping} from values of
"ID" sort to values of "T" sort. Such mappings take the rôle of symbol tables from traditional
compilers.

\item The second thing to do is to associate the inherited attribute to one or more \emph{recursive
    schemes}, which will be responsible for propagating that inherited attribute over values of a
  certain sort.  This has to be done by inventing a separate scheme for each combination of a sort
  and an inherited attribute:
  \begin{hacs}
   sort S | scheme Se(S) ↓e ;
   sort E | scheme Ee(E) ↓e ;
\end{hacs}
As can be seen, the scheme generates results of the "S" and "E" sorts, in each case taking a
single argument of the same sort, and for each indicating with a "↓" that the scheme
\emph{carries} the associated inherited attribute. Notice that, unlike for synthesized
declarations, there is no "|" in front because of the attribute itself because it is specific to
the scheme, not a declaration for the sort.

\item Next we encode the simplest rule, \thetag{2}. As in the previous section, we observe that
  \thetag{2} operates on sums, which leads to a pattern like the following, using the subscripts
  from~\thetag{2}:
  \begin{hacs}
   ⟦ ⟨E#1⟩ + ⟨E#2⟩ ⟧
\end{hacs}
We can now insert the pattern into the scheme, similarly to how we did for recursive
syntax-directed schemes:
\begin{hacs}
   Ee(⟦⟨E#1⟩ + ⟨E#2⟩⟧)
\end{hacs}
This clearly respects the sort constraints we defined above, with "Ee" being applied to an "E"
expression. Since there are no complicated dependencies in \thetag{2}, we are almost done: we just
have to create a rule where we on the right side of the "→" apply the "Ee" scheme recursively to
the subexpressions that should inherit the "e" attribute, which are the two marked subexpression,
\emph{inside the syntax markers}:
\begin{hacs}
   Ee(⟦⟨E#1⟩ + ⟨E#2⟩⟧)  →   ⟦⟨E Ee(#1)⟩ + ⟨E Ee(#2)⟩⟧ ;
\end{hacs}
Notice that there is no explicit mention of the "e" attribute, only the \emph{implicit} copying
that follows from the use of the "Ee" scheme. The recursive  arrangement of the "Ee" wrappers
implies the two attribute equations $E_1.e=E.e$ and $E_2.e=E.e$ from \thetag{2}.

\item Let us do \thetag{3}. It defines the synthesized attribute $t$ on \textbf{id}-expressions
  using a ``Lookup'' meta-function that is meant to suggest that we extract the mapping from the
  $E.e$ environment inherited from the context. We start with a template like this:
  \begin{hacs}
   Ee(⟦⟨ID#1⟩⟧)  →  ⟦⟨ID#1⟩⟧ ;
\end{hacs}
which just states that the $e$ attribute is inherited ``into'' a name subterm (through "Ee"),
essentially making $E.e$ available, but the rule does not use the inherited attribute.
In particular, this rule fails to actually set the $t$ attribute. To fix it, we first have to capture
the ``Lookup'' in the inherited environment. \HAX has a special notation for this: write the
pattern as follows:
\begin{hacs}
   Ee(⟦⟨ID#1⟩⟧) ↓e{#1 : #t}  →     …
\end{hacs}
The pattern of this rule is equipped with a \emph{mapping constraint} on the "e" inherited
attribute, which corresponds to the "Lookup" notation of \thetag{3} above. It will match when "Ee"
is applied to a name, called "#1", which is \emph{also} mapped by the associated "e" attribute
to a type, which we call~"#t". After we have captured the mapped type this way, we can complete
the rule by explicitly associating it to the "t" attribute associated with "ID" expressions.
\begin{hacs}
   Ee(⟦⟨ID#1⟩⟧) ↓e{#1 : #t}  →     ⟦⟨ID#1⟩⟧ ↑t(#t) ;
\end{hacs}
Notice how mappings like "e" use "{}" notation for both declarations and constraints, and simply
valued attributes like "t" use~"()".

\item Finally, we can encode \thetag{1}. It starts as
  \begin{hacs}
   Se(⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E#3⟩; ⟨S#4⟩⟧)  →      ⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E Ee(#3)⟩; ⟨S Se(#4)⟩⟧ ;
\end{hacs}
which merely establish  the basic rules that the $S.e$ attribute inherited from the context (through
the left hand "Se") is passed to both $E_3$ and $S_4$ through the right "Ee" and "Se",
respectively. This captures everything except the ``Extend'' notation. \HAX supports this by
allowing mapping constraints also on the right side of the "→" acting as \emph{extensions} of the
map.
\begin{hacs}
   Se(⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E#3⟩; ⟨S#4⟩⟧)
   →   ⟦⟨T#1⟩ ⟨ID#2⟩ = ⟨E Ee(#3)⟩; ⟨S Se(#4) ↓e{#2 : #1}⟩⟧ ;
  \end{hacs}
  We recommend that you study carefully how the "⟦…⟧" and "⟨…⟩" nest: the first wrap syntax
  fragments, and the second wrap raw (non-syntax) fragments inside syntax. Attribute constraints are
  always in the raw fragments.

\end{enumerate}
\end{example}

\begin{table}[h]
  \begin{displaymath}\small
    \begin{array}{c|p{.4\textwidth}|p{.4\textwidth}}
      \hline\hline
      \textsc{Notation} & \textsc{In Pattern} & \textsc{In Result} \\
      \hline
      D"↑a(#)"
      & $D$ must synthesize "a"-value captured in "#"
      & $D$ will synthesize "a"-value captured in "#"
      \\
      D"↑a{:#}"
      & --- "a"-environment captured in ":#"
      & --- "a"-environment captured in ":#"
      \\
      D"↑a{#k : #v}"
      & --- --- which includes binding "#k" to "#v"
      & --- --- which includes binding "#k" to "#v"
      \\
      D"↑a{¬#k}"
      & --- --- which does not have binding for "#k"
      & n/a
      \\
      D"↑a{}"
      & n/a
      & $D$ synthesizes an empty environment in "a"
      \\
      D"↑#"
      & \emph{All} $D$-synthesized attributes captured in "#"
      & \emph{All} attributes captured in "#" $D$-synthesized
      \\
      \hline
      F"↓a(#)"
      & $F$ must inherit "a"-value captured in "#"
      & $F$ will inherit "a"-value captured in "#"
      \\
      F"↓a{:#}"
      & --- "a"-environment captured in ":#"
      & --- "a"-environment captured in ":#"
      \\
      F"↓a{#k : #v}"
      & --- --- which includes binding "#k" to "#v"
      & --- --- which includes binding "#k" to "#v"
      \\
      F"↓a{¬#k}"
      & --- --- which does not have binding for "#k"
      & n/a
      \\
      F"↓a{}"
      & n/a
      & $F$ inherits an empty environment in "a"
      \\
      \hline
    \end{array}
  \end{displaymath}
  \caption{Attribute constraints.}\label{tab:attributes}
\end{table}

\begin{notation}[attributes]\leavevmode
  \begin{enumerate}

  \item Attributes are defined with attribute declarations:
    %%
    \begin{displaymath}
      "attribute"
      ~
      \left\{
      \begin{matrix}
        {↑} \\
        {↓}
      \end{matrix}
      \right\}
      ~
      a
      ~
      \left\{
      \begin{matrix}
        (\,S\,) \\
        \{\,S'\,\} \\
        \{\,S' : S\,\}
      \end{matrix}
      \right\}
      \,;
    \end{displaymath}
    %%
    where each braced unit represents a choice:
    %%
    \begin{itemize}
    \item The arrow determines whether we are defining a synthetic ("↑") or inherited ("↓")
      attribute.
    \item $a$ is the attribute name, a lower case indentifier.
    \item The last unit gives the category and sort(s) of the attribute: $(S)$ is a simply valued
      attribute of sort $S$, $\{S'\}$ an attribute with a set of $S'$-members, and $\{S':S\}$ is an
      attribute with a map from $S'$-values to $S$-values. For sets and maps, the $S'$ sort must be
      either a token sort or a sort with a single "symbol" case (these are explained in the next
      section).
    \end{itemize}

  \item With $D$ denoting data terms and $F$ ``function'' terms, \ie, applied schemes, we have the
    conventions of Table~\ref{tab:attributes} in \HAX rules.  Note that one can have several
    constraints on a single (data or function) term, as long as they are all properly declared (for
    the appropriate sort or scheme, respectively.

  \end{enumerate}
\end{notation}

\begin{figure}[p]
  \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/LetrecMap.hx}
  \caption{Synthesizing and then inheriting an environment.}
  \label{fig:letrec}
  \vspace*{2em}
  \begin{equation*}
    \begin{array}{r@{\,}l|lr}
      \hline
      \hline
      \multicolumn{2}{c|}{\textsc{Production}}  & \textsc{Semantic Rules} &\Bigstrut\\
      \hline\Bigstrut
      S &→ \textbf{id} := E_1; S_2
      & E_1.e = S.e; S_2.e = \op{Extend}(S.e, \textbf{id}.sym, E_1.t) &\thetag{S1}
      \\[\jot]
      &\mid \{~S_1~\}~S_2 & S_1.e = S.e; S_2.e = S.e &\thetag{S2}
      \\[\jot]
      &\mid ε & &\thetag{S3}
      \\[\jot]
      \hline\Bigstrut
      E &→ E_1 + E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E1}\\[\jot]
      &\mid E_1 \ast E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E2}\\[\jot]
      &\mid \textbf{int} & E.t = \op{Int}&\thetag{E3}\\[\jot]
      &\mid \textbf{float} & E.t = \op{Float}&\thetag{E4}\\[\jot]
      &\mid \textbf{id} & E.t = \text{if}~\op{Defined}(E.e,\textbf{id}.sym)&\thetag{E5}\\
      && \qquad\quad\text{then}~\op{Lookup}(E.e,\textbf{id}.sym)\\
      && \qquad\quad\text{else}~\op{TypeError}
      \\[\jot]
      \hline
    \end{array}
  \end{equation*}
  \caption{SDD for type checking.}
  \label{fig:sdd}
\end{figure}

\begin{example}
  Figure~\ref{fig:letrec} illustrates how an environment can be synthesized and then inherited. The
  \HAX script implements variable substitutions of mutually recursive bindings, which is essentially achieved
  by two ``passes,'' one for synthesizing an environment that contains the collected bindings, and a
  second pass that actually distributes and applies the collected (now inherited) environment to the
  target variable.
\end{example}

The examples we have shown so far have very simple attribute dependencies. Let us finish with an
example with more complicated attribute dependencies.

\begin{figure}[p]
\begin{hacs}[numbers=right,texcl]
sort Type  | Int | Float | TypeError
           | scheme Unif(Type,Type) ;

Unif(Int, Int) → Int;
Unif(#t1, Float) → Float;
Unif(Float, #t2) → Float;
default Unif(#1,#2) → TypeError; // fall-back

attribute↑t(Type);  // synthesized expression type
sort Exp | ↑t;

⟦ (⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ (⟨Exp#1 ↑t(#t1)⟩ * ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨INT#⟩ ⟧ ↑t(Int);
⟦ ⟨FLOAT#⟩ ⟧ ↑t(Float);
// Missing case: variables -- handled by Ee below.

attribute↓e{ID:Type};  // inherited type environment
sort Exp | scheme Ee(Exp) ↓e ;  // propagates e over Exp

// These rules associate t attribute with variables (missing case above).
Ee(⟦v⟧) ↓e{⟦v⟧ : #t} → ⟦ v ⟧ ↑t(#t);
Ee(⟦v⟧) ↓e{¬⟦v⟧} → error⟦Undefined identifier ⟨v⟩⟧;

Ee(⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧ ↑#syn) → ⟦ ⟨Exp Ee(#1)⟩ + ⟨Exp Ee(#2)⟩ ⟧ ↑#syn ;
Ee(⟦⟨Exp#1⟩ * ⟨Exp#2⟩⟧ ↑#syn) → ⟦ ⟨Exp Ee(#1)⟩ * ⟨Exp Ee(#2)⟩ ⟧ ↑#syn ;
Ee(⟦⟨INT#⟩⟧ ↑#syn) → ⟦⟨INT#⟩⟧ ↑#syn ;
Ee(⟦⟨FLOAT#⟩⟧ ↑#syn) → ⟦⟨FLOAT#⟩⟧ ↑#syn ;

sort Stat | scheme Se(Stat) ↓e ;  // propagates e over Stat

Se(⟦v := ⟨Exp#1⟩; ⟨Stat#2⟩⟧ ↑#syn) → SeB(⟦v := ⟨Exp Ee(#1)⟩; ⟨Stat#2⟩⟧ ↑#syn);
{
  | scheme SeB(Stat) ↓e;  // helper scheme for assignment after expression typeanalysis
  SeB(⟦v := ⟨Exp#1 ↑t(#t1)⟩; ⟨Stat#2⟩ ⟧ ↑#syn)
   → ⟦v := ⟨Exp#1⟩; ⟨Stat Se(#2) ↓e{⟦v⟧:#t1}⟩⟧ ↑#syn ;
}

Se (⟦ { ⟨Stat#1⟩ } ⟨Stat#2⟩ ⟧ ↑#syn) → ⟦ { ⟨Stat Se(#1)⟩ } ⟨Stat Se(#2)⟩ ⟧ ↑#syn ;

Se (⟦ ⟧ ↑#syn) → ⟦ ⟧ ↑#syn ;
\end{hacs}
  \caption{\HAX code for type analysis.}
  \label{fig:sdd-hacs}
\end{figure}

\begin{example}
  Figure~\ref{fig:sdd} shows a more realistic SDD for type checking, and Figure~\ref{fig:sdd-hacs}
  shows the corresponding \HAX. Here are the steps we followed to obtain this script:
  %% 
  \begin{enumerate}

  \item Lines 1--7 define the "Type" semantic data sort along with the helper "Unif" semantic
    function as previously, except here extended with a "TypeError" case.

  \item Lines 9 and 10 defines the synthesized type $t$ on expression data, and lines 12--15 give
    the type synthesis rules except for typing variables, which we will describe later. This makes
    it clear that \emph{type synthesis cannot happen until variable occurrences are typed}. In the
    SDD, this corresponds to all the $t$-assignments except the one in \thetag{E5}, which depends
    on~$E.e$.

  \item Line 18 declares the inherited environment attribute, and line 19 associates it with the
    recursive scheme "Ee" on expressions.

  \item Lines 21--28 give the environment propagation rules for expressions. Specifically notice how
    there are two cases for identifier, line 22 and 23, corresponding to whether the identifier is
    defined in the environment or not, with the latter resulting in an "error" message in special
    \HAX form. Also notice how the recursive rules in lines 25--28 take care to preserve all
    synthesized attributes on the terms that are traversed by using a catch-all "↑#syn" constraint.

  \item In total, lines 9--28 fully capture rules \thetag{E1--E5}. The \HAX specification adds the
    condition on evaluation that we must first apply the environment and then synthesize the type.

  \item Line 30 declares a recursive scheme carrying the "e" attribute over statements, and lines 39
    and 41 are easy simple recursive rules for the propagation of "e" corresponding to
    \thetag{S2--S3}.

  \item Rule \thetag{S1} is slightly more complicated, because the inherited attribute has
    non-trivial dependencies. We must know the dependency relationship of the attributes to devise a
    \emph{recursive strategy} for the attribute evaluation.  Recall that we have the following
    (realistic) dependency for \thetag{1}: ``The $E_2.t$ attribute cannot be computed until
    \emph{after} $E_2.e$ has been instantiated (and recursively propagated).'' In that case we have
    to evaluate \thetag{S1} in two steps:
    \begin{enumerate}
    \item Do $E_2.e = S.e$, establishing the precondition for allowing the system to compute $E_2.t$.
    \item When the system has computed $E_2.t$ then do $S_3.e=\op{Extend}(S.e,\textbf{id}_1.sym,E_2.t)$.
    \end{enumerate}
    These two steps are achieved by having an extra carrier scheme, "SeB", which is locally
    declared, so that "Se" and "SeB" can handle the two steps above: first "Se", in line 32, copies
    "e" \emph{just} to $E_1$ (from \thetag{S1}), and then \emph{chains} to "SeB".

  \item The helper scheme, "SeB" declared in line 34, is set up to \emph{wait} for the synthesized
    $t$ attribute (line 35) to be computed for $E_1$ and only \emph{then} replace the term with a
    new one where we compute the $S_2$ part with an extended environment (line 36).
    
  \end{enumerate}
  %%
  Also note that because the "Name" sort is a "symbol" sort, we should \emph{directly} use the
  variable "v" (which is a legal "ID" token) in the rules instead of "⟨Name#v⟩" or such.  However,
  we still take care to distinguish a symbol that is part of the syntax, like "v", from the other
  symbols that are part of the formalism, like "S" and "Se": "v" is always enclosed in $⟦⟧$s

  The environment helpers get translated to native \HAX environment patterns from
  Table~\ref{tab:attributes} as follows:
  %% 
  \begin{itemize}

  \item A ``$\op{Defined}(N.e, x)$'' test is encoded by having two rules: one for the ``true''
    branch with the constraint "↓e{⟦x⟧}" in a pattern, and one for the ``false'' case with the
    constraint "↓e{⟦¬x⟧}" in the pattern.

  \item ``$\op{Lookup}(N.e, x)$'' is encoded by adding a constraint "↓e{⟦x⟧:#}" in a pattern, which
    then binds the meta-variable "#" to the result of the lookup. (This will imply the ``defined''
    pattern discussed above.)

  \item ``$\op{Extend}(N.e, x, V)$'' is encoded by adding a constraint "↓e{⟦x⟧:V}" in the
    replacement.

  \end{itemize}
  %% 
\end{example}


\section{Higher Order Abstract Syntax}
\label{sec:hoas}

In the examples so far, we have analyzed and translated abstract syntax trees with structure and
symbols. However, we have not yet constructed \emph{new} structures with new scopes, or copied
fragments with scopes inside. These are the features where the ``H'' of \HAX matter.
%%
Specifically, \HAX provides support for manipulating scoped terms using \emph{higher-order abstract
  syntax} (HOAS) \cite{PfenningElliot:pldi1988} through some new constructs:
%%
\begin{itemize}

\item Outside syntax, \HAX recognizes identifiers starting with a lower case letter as \emph{variables}.

\item In grammar productions, for a symbol sort $S$ and a \HAX variable $x$, we can use the special
  reference $⟨S\,"binds"\,x⟩$ to define that this particular instance of $S$ is a \emph{binder},
  with the variable~$x$ as the label to indicate the scoping.

\item In a grammar production with a $⟨S\,"binds"\,x⟩$ reference, there can be one reference of the
  form $⟨S'[x\,"as"\,S'']⟩$. This is like a reference $⟨S'⟩$ but with the added information that all
  occurrences of the binder labeled $x$ \emph{must} occur inside the $S'$ subterm, and furthermore
  the occurrences will have the sort~$S''$. If $S''$ is not the same sort as $S$, then there must be
  a grammar production like $"sort"\,S''\,|\,⟦⟨S⟩⟧$.

\item In rules, you \emph{must} use the native variable form (no $⟨⟩$s) for binders.

\item In patterns (left of $→$), you should write references to scoped subterms with a special
  notation where scoped meta-variables are ``applied'' to the bound variables in $[]$s, looking
  something like this: $⟦…⟨S'\#[⟦x⟧]⟩…⟧$.

\item In replacements (right of $→$), you should always apply a scoped metavariable to the same
  number of arguments in $[]$s as the corresponding pattern sets up. The result of such an
  application is to replace all bound occurrences of the variable that was indicated in the pattern
  with whatever is used in the replacement.

\end{itemize}
%%
The formal rules are rather dense, however, as we shall show with some examples, this is really just
a compact way of encoding the usual notions of binders and parameter substitution that are familiar
from ordinary programming.

\begin{example}\label{ex:hoas}
  %%
  Consider the following variation of the grammar in Example~\ref{ex:syntax}, which makes the
  scoping rules of our little assignment language explicit.
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right]
main sort Stat  | ⟦ ⟨Name binds v⟩ := ⟨Exp⟩ ; ⟨Stat[x as Exp]⟩ ⟧
                | ⟦⟧ ;

sort Exp  | ⟦ ⟨Exp⟩ + ⟨Exp@1⟩ ⟧
          | ⟦ ⟨Exp@1⟩ * ⟨Exp@2⟩ ⟧@1
          | ⟦ ⟨INT⟩ ⟧@2
          | ⟦ ⟨FLOAT⟩ ⟧@2
          | ⟦ ⟨Name⟩ ⟧@2
          | sugar ⟦ ( ⟨Exp#⟩ ) ⟧@2 → Exp# ;

sort Name  | symbol ⟦⟨ID⟩⟧ ;
  \end{hacs}
  %% 
  The HOAS constructs are only present in line 1 of the grammar, and the difference may seem
  irrelevant. However, consider these simple rules that \emph{duplicate} and \emph{append}
  statements (such a rule may be useful in loop hoisting code, for example):
  %%
  \begin{hacs}[xleftmargin=\parindent,xrightmargin=2em,numbers=right,firstnumber=last]
sort Stat | scheme Duplicate(Stat) | scheme Append(Stat,Stat) ;
Duplicate(#S) →  Append(#S, #S) ;
Append(⟦⟧, #S2) → #S2 ;
Append(⟦ old := ⟨Exp#2⟩; ⟨Stat#3[old]⟩ ⟧, #S2)
  → ⟦ new := ⟨Exp#2⟩; ⟨Stat Append(#3[new], #S2⟩ ⟧ ;
  \end{hacs}
  %%
  Notice how the pattern of the last rule in line 15 explicitly calls out the used binding: the
  actual variable that is used in the program is referenced in the pattern as "old" and we ask the
  pattern matching to also keep track of all the occurrences inside the scope, which is~"#3", by
  writing it as "#3[old]". With this setup, we can now \emph{systematically replace the symbol} with
  a new and fresh one, which we call "new" in the replacement in line 16, but which the system will
  in fact replace with a new and unique name, both the binder \emph{and all the in-scope
    occurrences} inside~"#3". In this way, we ensure that there is no so-called ``variable
  capture,'' \ie, that occurrences of the "old" name inside "#S2" accidentally get double-bound.

\end{example}

Interestingly, HOAS also makes it impossible to have variables ``escape'' from their scope. In the
old grammar from Example~\ref{ex:syntax} we could write a rule like
\begin{hacs}
   Flip(⟦ v1 := ⟨Exp#2⟩; ⟨Stat#3⟩ ⟧) →    ⟦ { ⟨Stat#3⟩ } v1 := ⟨Exp#2⟩; ⟧ ;
\end{hacs}
which will take uses of "v1" inside "#3" and move them outside of their scope. This is not
immediately possible with HOAS without explicitly substituting the occurrences of the bound variable
with something else in the copy, which would expose the error. (You may think that this is an
obvious mistake that noone would make but variable escaping like this is a real problem that leads
to bugs in compilers.)
%%
The use of HOAS in \HAX in fact allows the full range of methods used in higher-order
rewriting~\cite{Jouannaud:klop2005,Klop+:tcs1993}, and is a very powerful mechanism.

\begin{figure}[p]
  \inputhacs[texcl,mathescape,xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/CPS.hx}
  \caption{Definitions of Call-by-Value Continuation Passing Style}
  \label{fig:cps}
\end{figure}

\begin{example}
  Let us take a real example from the literature~\cite{DanvyRose:rta1998}.  Figure~\ref{fig:cps}
  shows two systems for transforming λ-terms to continuation-passing style with call-by-value
  evaluation semantics. The grammar uses higher-order binding to define the two forms of abstraction
  (with "λ" and "λ̅") using specific unicode characters. The overlined application is a scheme
  because it can be evaluated, which is specified by the rule in line 16 (where the outermost
  operator is the overlined application). The two blocks of rules in lines 18--24 and 26--33 specify
  two variants of continuation style conversion: the traditional form and a one-pass variant,
  respectively. The two systems are equivalent but the second never constructs overlined redices
  (which is what makes it one pass, as no so-called ``administrative'' (overlined) redices need to
  be simplified. Notice how each transformation defines helper schemes, which are syntactic schemes,
  to make the recursive composition easier to express.
\end{example}

Finally, we sometimes use symbols as \emph{labels} or for some other purpose, where we wish to have
globally unique symbols. This creates a dilemma: when we write "⟦x⟧" for some symbol sort, how do we
distinguish between the actual variable written "x" and a variable which is automatically renamed to
be globally unique? By default, \HAX follows the following rules:
%%
\begin{itemize}

\item Every token that is parsed to be of a "symbol" sort is \emph{automatically renamed} to a
  \emph{fresh} variable name. (If used in a pattern, the fresh name will match the actual symbol
  that is present in the term, and thus in essence really be a placeholder for an existing symbol.)

\item If a rule is prefixed with the option "[global x]", then the symbol "x" will not be subject to
  this automatic conversion.

\end{itemize}




%% TODO: CPS transform.
%%\begin{example}[CPS]\label{ex:cps}
%%  
%%\end{example}

%% TODO: symbolic derivation example
%%\begin{example}[symbolic derivation]\label{ex:deriv}
%%  
%%\end{example}


\section{Compile-time Computations}
\label{sec:comp}

We may sometimes need to compute helper values, most commonly for counting. \HAX supports this
through a special sort, "Computed", which has special syntax for operations on primitive values.
%%
The mechanism is quite simple: in rule replacements, arguments of sort "Computed" can contain
expressions in ⟦⟧s, which include
%%
\begin{itemize}
\item References to meta-variables of sort "Computed".
\item Integers in decimal and \texttt{0x}-hexadecimal notation.
\item Strings enclosed in \texttt{"…"}.
\item Standard operators and parentheses.
\item Conversion of tokens to integers.
\end{itemize}
%%
Notice that meta-variables are part of the syntax: essentially "⟨Computed#x⟩" and "#x" mean the same
inside the special syntax.  The allowed operators are summarized in Figure~\ref{fig:ops}.

\begin{table}[h]
  \begin{displaymath}
    \begin{array}{c|p{.33\linewidth}}
      \hline
      \textit{Operators} & \textit{Explanation} \\
      \hline
      \texttt{+}~\texttt{-} & \text{addition, subtraction} \\
      \texttt{|}~\verb|^|~\verb|\| & \text{bitwise or, exclusive or, clear} \\
      \texttt{@} & \text{string concatenation} \\
      \hline
      \texttt{*}~\texttt{/}~\texttt{\%} & \text{multiplication, divison, modulo} \\
      \texttt{\&} & \text{bitwise and} \\
      \verb|<<|~\verb|>>| & \text{bitwise shift left and right} \\
      \hline
      \texttt{+}~\texttt{-} & \text{unary plus and minus}\\
      \verb|~| & \text{bitwise not} \\
      \texttt{escape~unescape} & \text{unary function that insert/remove C string escapes} \\
      \texttt{length} & \text{unary function that returns length of string} \\
      \hline
    \end{array}
  \end{displaymath}
  \caption{Operations permitted in Computed syntax.}
  \label{fig:ops}
\end{table}

\begin{example}[count]\label{ex:count}
  Consider the list from Example~\ref{ex:flatten}. The following computes the length of the list, using a helper.
  %%
\begin{hacs}
sort Computed | scheme ListLength(List) | scheme ListLength2(List, Computed) ;
ListLength(#) → ListLength2(#, ⟦0⟧) ;
ListLength2(⟦ ⟨Elem#1⟩ ⟨List#2⟩ ⟧, #n) →  ListLength2(#2, ⟦ #n + 1 ⟧) ;
ListLength2(⟦ ⟧, #n) → #n ;
\end{hacs}
  %%
\end{example}

\begin{figure}[p]
  \inputhacs[xleftmargin=\parindent,xrightmargin=2em,numbers=right]{../samples/Desk.hx}
  \caption{\emph{samples/Desk.hx}.}
  \label{fig:desk}
\end{figure}

Notice that it is \emph{not} presently possible to nest computations, \ie, insert a value in a
"Computed" expression which is computed by user functions (this will change). However, it is still
possible to use helpers to achieve nested computations, as illustrated by the simple desk calculator
in Figure~\ref{fig:desk}, which also shows the use of "$" to import a token as an integer value. %$


\section{Examples}
\label{sec:examples}

Once the structure of a specification is clear, we can start analyzing and manipulating our internal
representation.  In this section we work through some examples of this.

\begin{remark}
  Before we start, here is a short checklist of recommended practices for \HAX processing:
  %% 
  \begin{itemize}

  \item Separate token names from sort names, for example by using ALL CAPS for token names.

  \item If you use some kind of identifier then declare it as a special sort like
    \begin{displaymath}
      "sort Symbol | symbol ⟦⟨SYMBOL⟩⟧ ;"
    \end{displaymath}
    with "SYMBOL" defined as a token that always allows numeric extensions, like
    \begin{displaymath}
      "token SYMBOL | [a-z]+ ('_'"~"[0-9]+)* ;"
    \end{displaymath}
    When you do this then you are allowed to use symbols of the specified kind in patterns, \ie, a
    rule like
    \begin{displaymath}
      "F(⟦x_1⟧) →   ⟦ x_1 + x_2 ⟧ ;"
    \end{displaymath}
    will allow "F" to be applied to \emph{any} "Symbol", with "x_1" being the \emph{representative} of
    that symbol for the purpose of the rule. Similarly, "x_2" will correspond to a \emph{globally fresh} symbol,
    as it only occurs left of the "→".

  \end{itemize}
\end{remark}

\begin{figure}[t]
\begin{hacs}[numbers=right,texcl]
module org.crsx.hacs.samples.IsCat {

token WORD | [A-Za-z]+ ;
main sort Word | ⟦⟨WORD⟩⟧ ;

sort Boolean | ⟦True⟧ | ⟦False⟧ ;

sort Boolean | scheme IsCat(Word) ;
IsCat(#word) → IsSameWord(#word, ⟦cat⟧) ;

sort Boolean | scheme IsSameWord(Word, Word) ;
IsSameWord(#, #) → ⟦True⟧ ;
default IsSameWord(#1, #2) → ⟦False⟧ ;
}
\end{hacs}
\caption{\emph{IsCat.hx}: Finding cats.}
\label{fig:cats}
\end{figure}

\begin{example}[finding cats]
  The small example in Figure~\ref{fig:cats} illustrates how to test for equality using a
  non-linear rule in line 12 combined with a ``catch-all'' default rule in line 13.

  Note that we cannot use "⟦cat⟧" directly in a pattern: patterns are restricted to \emph{syntactic
    cases} of the grammar.  Also note that we have here defined the "Boolean" sort to have syntactic
  values rather than just constructors: this allows us to print them.

  Here is a possible run using this command:
  %%
\begin{code}[commandchars=\\\{\}]
$ \textcolor{blue}{hacs IsCat.hx}
...
$ \textcolor{blue}{./IsCat.run --scheme=IsCat --term="dog"}
False
$ \textcolor{blue}{./IsCat.run --scheme=IsCat --term="cat"}
True
\end{code}
%%$
\end{example}

\begin{figure}[p]
\begin{hacs}[numbers=right,texcl]
module org.crsx.hacs.samples.WordSet {

// Simple word membership query.
main sort Query | ⟦ ⟨WORD⟩ in ⟨List⟩ ⟧ ;
sort List | ⟦ ⟨WORD⟩, ⟨List⟩ ⟧ | ⟦ ⟨WORD⟩ ⟧ ;
token WORD | [A-Za-z0-9]+ ;

// Collect set of words.
attribute ↑z{WORD} ;
sort List | ↑z ;
⟦ ⟨WORD#w⟩, ⟨List#rest ↑z{:#ws}⟩ ⟧ ↑z{:#ws} ↑z{#w} ;
⟦ ⟨WORD#w⟩ ⟧ ↑z{#w} ;

// We'll provide the answer in clear text.
sort Answer
| ⟦Yes, the list contains ⟨WORD⟩.⟧
| ⟦No, the list does not contain ⟨WORD⟩.⟧
;

// Check is main query scheme, which gives an Answer.
sort Answer | scheme Check(Query) ;

// The main program needs the synthesized list before it can check membership.
Check( ⟦ ⟨WORD#w⟩ in ⟨List#rest ↑z{#w}⟩ ⟧ ) → ⟦Yes, the list contains ⟨WORD#w⟩.⟧ ;
Check( ⟦ ⟨WORD#w⟩ in ⟨List#rest ↑z{¬#w}⟩ ⟧ ) → ⟦No, the list does not contain ⟨WORD#w⟩.⟧ ;
}
\end{hacs}
\caption{\emph{WordSet.hx}: Sets of Words.}
\label{fig:wordset}
\end{figure}

\begin{example}[set of words]
  %%
  One common task is to synthesize a set from some syntactic construct, and subsequently search the
  set. Figure~\ref{fig:wordset} shows a small toy syntax allowing simple queries of word set
  membership.

  The example uses some new mechanisms for synthesizing the set:
  %%
  \begin{itemize}

  \item A helper "z" synthetic attribute contains a \emph{set} of word tokens, which is indicated by
    the attribute declaration "↑z{WORD}" in line~9.

  \item We associate a "z" set with all values of the syntactic sort "List" in line 10.

  \item Lines 11 and 12 capture the synthesis of the set. Line 12 captures the simple case where a
    singleton list synthesizes a singleton set.

  \item Line 11 has a few more notations in play.  First, the \emph{pattern} part of the rule
    includes the inner pattern "↑z{:#ws}". This specifies that the special meta-variable ``":#ws"''
    captures all the existing members of the "z" set.  Second, the result of the rule is to add
    \emph{two} new things to the top level of the rule: "↑z{:#ws} ↑z{#w}". This adds \emph{both} the
    existing members (just matched) \emph{and} the one new member "#w" to the result set.

  \item Lines 24 and 25 are almost the same: the one difference is that 24 matches sets that contain
    the "#w" word, whereas 25 matches sets that do not because of the "¬" logical negation sign.
  \end{itemize}
  %%
  We can run the example as follows:
  %%
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{hacs WordSet.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./WordSet.run --scheme=Check --term="a in a,b,b,a"}}
Yes, the list contains a.
energon1[~]$ ^textcolor{blue}{^texttt{./WordSet.run --scheme=Check --term="Foo in Bar"}}
No, the list does not contain Foo.
\end{code}
%%$
\end{example}

\begin{figure}[p]
\begin{hacs}[numbers=right,texcl]
module org.crsx.hacs.samples.WordMap {

// Simple word map over list.
main sort Query | ⟦ ⟨Map⟩ in ⟨List⟩ ⟧ ;
sort List | ⟦ ⟨WORD⟩ ⟨List⟩ ⟧ | ⟦ ⟧ ;
sort Map | ⟦ ⟨WORD⟩ : ⟨WORD⟩ , ⟨Map⟩ ⟧ | ⟦ ⟨WORD⟩ : ⟨WORD⟩ ⟧ ;
token WORD | [A-Za-z0-9]+ ;

// Collect word mapping.
attribute ↑m{WORD:WORD} ;
sort Map | ↑m ;
⟦ ⟨WORD#key⟩ : ⟨WORD#value⟩, ⟨Map#map ↑m{:#ms}⟩ ⟧ ↑m{:#ms} ↑m{#key:#value} ;
⟦ ⟨WORD#key⟩ : ⟨WORD#value⟩ ⟧ ↑m{#key:#value} ;

// Main program takes a Query and gives a List.
sort List | scheme Substitute(Query) ;

// Environment for mappings during List processing.
attribute ↓e{WORD:WORD} ;
sort List | scheme ListE(List) ↓e ;

// The main program needs the synthesized map before it can substitute.
Substitute( ⟦ ⟨Map#map ↑m{:#ms}⟩ in ⟨List#list⟩ ⟧ ) → ListE( #list ) ↓e{:#ms} ;

// Replace any mapped words.
ListE( ⟦ ⟨WORD#word⟩ ⟨List#words⟩ ⟧ ↑#syn ) ↓e{#word : #replacement}
→ 
⟦ ⟨WORD#replacement⟩ ⟨List ListE(#words)⟩ ⟧↑#syn
;

ListE( ⟦ ⟨WORD#word⟩ ⟨List#words⟩ ⟧ ↑#syn ) ↓e{¬#word}
→ 
⟦ ⟨WORD#word⟩ ⟨List ListE(#words)⟩ ⟧↑#syn
;

ListE( ⟦ ⟧ ↑#syn ) → ⟦ ⟧ ↑#syn ;
}
\end{hacs}
\caption{\emph{WordMap.hx}: Apply Word Substitution as Map.}
\label{fig:wordmap}
\end{figure}

\begin{example}[map of words]\label{ex:wordmap}
  Figure~\ref{fig:wordmap} shows how a map can be synthesized and then used as an environment. The
  pattern is similar to the set example, except here we not only synthesize the map attribute "m"
  but also ``copy'' it over to an inherited map -- an environment -- "e". Notice these extras:
  %%
  \begin{itemize}

  \item The map attribute is synthesized in lines 12--13 just like the set attribute was in the
    previous example, the only difference is that the map of course includes both a key and value.

  \item In line 23 we simply capture all the ``mappings'' of the "m" attribute with the special
    ":#ms" pattern, which we then \emph{reuse} to populate the "e" environment.

  \item We actually combine the distribution of the inherited map with a recursive transformation
    that replaces words, in lines 26--34. The two rules for an initial "WORD" are mutually exclusive
    because the pattern in line 26 requires that the word is present with a mapping in the "e"
    attribute, whereas the pattern in line 31 requires that the word is not present.

  \end{itemize}
  Here is a run demonstrating the program:
\begin{code}[commandchars=\^\{\}]
energon1[~]$ ^textcolor{blue}{^texttt{hacs WordMap.hx}}
energon1[~]$ ^textcolor{blue}{^texttt{./WordMap.run --scheme=Substitute --term="a:b in a b b a"}}
b b b b
energon1[~]$ ^textcolor{blue}{^texttt{./WordMap.run --scheme=Substitute --term="k:v in a b c"}}
a b c
\end{code}
%%$
\end{example}

\begin{figure}[p]\scriptsize
\begin{hacs}[numbers=right,texcl]
module org.crsx.hacs.samples.WordSubst {

// Grammar.
sort Units | ⟦ ⟨Unit⟩ ⟨Units⟩ ⟧ | ⟦⟧ ;
sort Unit | ⟦⟨Variable⟩=⟨NAT⟩⟧ | ⟦⟨Variable⟩⟧ | ⟦⟨NAT⟩⟧ | ⟦ { ⟨Units⟩ } ⟧ ;
sort Variable | symbol ⟦⟨ID⟩⟧ ;

token ID | [A-Za-z]+ ;
token NAT | [0-9]+ ;
space [\ \t\n\r] ;

// Helper Subst structure: lists of variable-NAT pairs.
sort Subst | MoreSubst(Variable, NAT, Subst) | NoSubst ;

// Append operation for Subst structures.
| scheme SubstAppend(Subst, Subst) ;
SubstAppend(MoreSubst(#var, #nat, #subst1), #subst2) → MoreSubst(#var, #nat, SubstAppend(#subst1, #subst2)) ;
SubstAppend(NoSubst, #subst2) → #subst2 ;

// Attributes.
attribute ↑subst(Subst) ;        // collected Subst structure
attribute ↓env{Variable:NAT} ;   // mappings to apply

// Top scheme.
main sort Units | scheme Run(Units) ;
Run(#units) → Run1(#units) ;

// Strategy: two passes.
// 1. force synthesis of subst attribute.
// 2. convert subst attribute to inherited environment (which forces replacement).

| scheme Run1(Units) ;
Run1(#units ↑subst(#subst)) → Run2(#units, #subst) ;

| scheme Run2(Units, Subst) ↓env ;
Run2(#units, MoreSubst(#var, #nat, #subst)) → Run2(#units, #subst) ↓env{#var : #nat} ;
Run2(#units, NoSubst) → Unitsenv(#units) ;

// Synthesis of subst.

sort Units | ↑subst ;
⟦ ⟨Unit #1 ↑subst(#subst1) ⟩ ⟨Units #2 ↑subst(#subst2)⟩ ⟧ ↑subst(SubstAppend(#subst1, #subst2)) ;
⟦ ⟧ ↑subst(NoSubst) ;

sort Unit | ↑subst ;
⟦v=⟨NAT#n⟩⟧ ↑subst(MoreSubst(⟦v⟧, #n, NoSubst)) ;
⟦v⟧ ↑subst(NoSubst) ;
⟦⟨NAT#n⟩⟧ ↑subst(NoSubst) ;
⟦ { ⟨Units#units ↑subst(#subst)⟩ } ⟧ ↑subst(#subst) ;

// Inheritance of env combined with substitution.

sort Units | scheme Unitsenv(Units) ↓env ;
Unitsenv( ⟦ ⟨Unit#1⟩ ⟨Units#2⟩ ⟧↑#s ) →  ⟦ ⟨Unit Unitenv(#1)⟩ ⟨Units Unitsenv(#2)⟩ ⟧↑#s ;
Unitsenv( ⟦ ⟧↑#s ) → ⟦ ⟧↑#s ;

sort Unit | scheme Unitenv(Unit) ↓env ;
Unitenv( ⟦v=⟨NAT#n⟩ ⟧↑#s) → ⟦v=⟨NAT#n⟩⟧↑#s ;
Unitenv( ⟦v⟧ ) ↓env{⟦v⟧:#n} → ⟦⟨NAT#n⟩⟧ ;
Unitenv( ⟦v⟧↑#s ) ↓env{¬⟦v⟧} → ⟦v⟧↑#s ;
Unitenv( ⟦⟨NAT#n⟩⟧↑#s ) → ⟦⟨NAT#n⟩⟧↑#s ;
Unitenv( ⟦ { ⟨Units#units⟩ } ⟧↑#s ) → ⟦ { ⟨Units Unitsenv(#units)⟩ } ⟧↑#s ;
}
\end{hacs}
\caption{\emph{WordSubst.hx}: Combining list, maps, and transformation.}\label{fig:wordsubst}
\end{figure}

\begin{example}[word substitution]
  Figure~\ref{fig:wordsubst} shows a \HAX program to collect substitutions from a document and apply
  them to the entire document.  Notice the following:
  \begin{itemize}

  \item The strategy is a typical two-pass one: first one pass to collect the substitutions into a
    synthesized attribute, then a second pass where the full list of substitutions is applied
    everywhere.

  \item We have chosen to synthesize the map as a \emph{data structure} instead of a native \HAX map
    as in the previous Example~\ref{ex:wordmap} because we here need to \emph{append} two maps (in
    line 42), which is not supported for the native maps.  The synthesis happens in lines 41--49.

  \item We translate the synthesized map in list form into a native \HAX map before starting the
    second pass: notice how "Run2" starts by recursing over the list of substitutions, inserting
    each into the carried inherited "env" map.  Since the map is consumed from left to right, the
    \emph{latest} substitution for any variable is always used.

  \item Since the inheritance schemes for "env" in lines 53--63 are doing a recursive traversal of
    the term, we benefit by building the actual substitutions into the traversal.

  \item In the inheritance rules we are careful to preserve the synthesized attributes only when the
    term does not change. In our case this is manifest by just the rule in line 59 not including the
    "↑#s" marker to capture and copy the synthesized attributes; in general this should be
    considered for every situation.

  \end{itemize}
  %%
  Here is a run with this system:
  %%
\begin{code}[commandchars=\\\{\}]
energon1[~]$ \textcolor{blue}{hacs WordSubst.hx}
...
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="a=1 a"}
 a=1  1   
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="b a \{a=1 b=2\}"}
 2  1   {  a=1  b=2    }     
energon1[~]$ \textcolor{blue}{./WordSubst.run --scheme=Run --term="\{a=1 b=2 c=3\} a b c \{a=4\} a b c"}
  \{  a=1  b=2  c=3     \}   4  2  3   \{  a=4   \}   4  2  3         
\end{code}
  %%
  The last example shows how the latest substitution for "a" ``wins.''
  %%
\end{example}


%%% \section{Advanced Features}
%%% \label{sec:advanced}
%%% 
%%% After the analysis we are ready for generating code.
%%% 
%%% \begin{figure}[p]\small
%%% \begin{hacs}[numbers=right]
%%% token T | T ('_' ⟨INT⟩)? ; // temporary
%%% 
%%% // Concrete syntax & abstract syntax sorts.
%%% 
%%% sort I_Progr | ⟦⟨I_Instr⟩ ⟨I_Progr⟩⟧ | ⟦⟧ ;
%%% 
%%% sort I_Instr | ⟦⟨Tmp⟩ = ⟨I_Arg⟩ + ⟨I_Arg⟩; ¶⟧
%%%              | ⟦⟨Tmp⟩ = ⟨I_Arg⟩ * ⟨I_Arg⟩; ¶⟧
%%%              | ⟦⟨Tmp⟩ = ⟨I_Arg⟩; ¶⟧
%%%              | ⟦⟨Name⟩ = ⟨Tmp⟩; ¶⟧ ;
%%% 
%%% sort I_Arg | ⟦⟨Name⟩⟧
%%%            | ⟦⟨FLOAT⟩⟧
%%%            | ⟦⟨INT⟩⟧
%%%            | ⟦⟨Tmp⟩⟧ ;
%%% 
%%% sort Tmp | symbol ⟦ ⟨T⟩ ⟧ ;
%%% 
%%% // Translation scheme.
%%% 
%%% attribute ↓tmpType{Tmp:Type} ;
%%% 
%%% sort I_Progr ;
%%% 
%%% | scheme ⟦ ICG ⟨Stat⟩ ⟧ ↓tmpType ;
%%% ⟦ ICG id := ⟨Exp#2 ↑HasType(#t2)⟩; ⟧
%%%   → ⟦ { ⟨I_Progr ⟦ICGExp T ⟨Exp#2⟩⟧ ↓tmpType{T:#t2}⟩ } id = T; ⟧ ;
%%% ⟦ ICG { } ⟧ → ⟦ ⟧;
%%% ⟦ ICG { ⟨Stat#s⟩ ⟨Stats#ss⟩ } ⟧ → ⟦ { ICG ⟨Stat#s⟩ } ICG { ⟨Stats#ss⟩ } ⟧ ;
%%% 
%%% | scheme ⟦ ICGExp ⟨Tmp⟩ ⟨Exp⟩ ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨INT#1⟩ ⟧ → ⟦ T = ⟨INT#1⟩; ⟧ ;
%%% ⟦ ICGExp T ⟨FLOAT#1⟩ ⟧ → ⟦ T = ⟨FLOAT#1⟩; ⟧ ;
%%% ⟦ ICGExp T id ⟧ → ⟦ T = id; ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨Exp#1⟩ + ⟨Exp#2⟩ ⟧
%%%   → ⟦ {ICGExp T_1 ⟨Exp#1⟩} {ICGExp T_2 ⟨Exp#2⟩} T = T_1 + T_2; ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨Exp#1⟩ * ⟨Exp#2⟩ ⟧
%%%   → ⟦ {ICGExp T_1 ⟨Exp#1⟩} {ICGExp T_2 ⟨Exp#2⟩} T = T_1 * T_2; ⟧ ;
%%% 
%%% // Helper to flatten code sequence.
%%% | scheme ⟦ {⟨I_Progr⟩} ⟨I_Progr⟩ ⟧;
%%% ⟦ {} ⟨I_Progr#3⟩ ⟧ → #3 ;
%%% ⟦ {⟨I_Instr#1⟩ ⟨I_Progr#2⟩} ⟨I_Progr#3⟩ ⟧ → ⟦ ⟨I_Instr#1⟩ {⟨I_Progr#2⟩} ⟨I_Progr#3⟩ ⟧;
%%% \end{hacs}%
%%%   \caption{Intermediate Code Generation.}
%%%   \label{fig:icgen}
%%% \end{figure}
%%% 
%%% \begin{example}\label{ex:icgen}
%%%   %%
%%%   A further part of \emph{First.hx} is the translation from abstract syntax to the intermediate
%%%   representation, shown in Fig.~\ref{fig:icgen}. The fragment contains the usual components: a
%%%   syntax specification, rewrite schemes, and rewrite rules for the "ICG" scheme.
%%% 
%%%   The code only uses two new features: "¶" markers in the syntax to indicate newlines, and rules
%%%   that introduce \emph{fresh} variables (of "Tmp" sort): when the replacement of a rule uses a
%%%   symbol, which was not in the pattern, then this corresponds to \emph{generating} a new globally
%%%   unique symbol. So each time the rule
%%% \begin{hacs}
%%%    ⟦ ICG id := ⟨Exp#2 ↑HasType(#t2)⟩; ⟧
%%%      → ⟦ { ⟨I_Progr ⟦ICGExp T ⟨Exp#2⟩⟧ ↓tmpType{T:#t2}⟩ } id = T; ⟧ ;
%%% \end{hacs}
%%%   is used, "T" denotes a new so-called ``fresh'' symbol.  When printed, the various incarnations of
%%%   "T" will be named "T_1", "T_86", \etc
%%%   %%
%%% \end{example}
%%% 
%%% \begin{figure}[p]\small
%%% \begin{hacs}[numbers=right]
%%% /* 7. CODE GENERATOR. */
%%% 
%%% sort A_Progr | ⟦ ⟨A_Instr⟩ ⟨A_Progr⟩ ⟧ | ⟦⟧ ;
%%% 
%%% sort A_Instr | ⟦ LDF ⟨Tmp⟩, ⟨A_Arg⟩ ¶⟧
%%%              | ⟦ STF ⟨Name⟩, ⟨Tmp⟩ ¶⟧
%%%              | ⟦ ADDF ⟨A_Arg⟩, ⟨A_Arg⟩, ⟨A_Arg⟩ ¶⟧
%%%              | ⟦ MULF ⟨A_Arg⟩, ⟨A_Arg⟩, ⟨A_Arg⟩ ¶⟧ ;
%%% 
%%% sort A_Arg | ⟦ #⟨FLOAT⟩ ⟧ | ⟦ #⟨INT⟩ ⟧ | ⟦ ⟨Name⟩ ⟧ | ⟦ ⟨Tmp⟩ ⟧ ;
%%% 
%%% sort A_Progr | scheme ⟦ CG ⟨I_Progr⟩ ⟧ ;
%%% 
%%% ⟦ CG ⟧ → ⟦⟧ ;
%%% 
%%% ⟦ CG T = ⟨I_Arg#1⟩ + ⟨I_Arg#2⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ ADDF T, [⟨I_Arg#1⟩], [⟨I_Arg#2⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% ⟦ CG T = ⟨I_Arg#1⟩ * ⟨I_Arg#2⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ MULF T, [⟨I_Arg#1⟩], [⟨I_Arg#2⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%%   
%%% ⟦ CG T = ⟨I_Arg#1⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ LDF T, [⟨I_Arg#1⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% ⟦ CG name = T ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ STF name, T CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% sort A_Arg | scheme ⟦ [⟨I_Arg⟩] ⟧ ;
%%% ⟦ [T] ⟧ → ⟦ T ⟧ ;
%%% ⟦ [name] ⟧ → ⟦ name ⟧ ;
%%% ⟦ [⟨FLOAT#1⟩] ⟧ → ⟦ #⟨FLOAT#1⟩ ⟧ ;
%%% ⟦ [⟨INT#1⟩] ⟧ → ⟦ #⟨INT#1⟩ ⟧ ;
%%% \end{hacs}
%%%   \caption{Code Generation.}
%%%   \label{fig:cgen}
%%% \end{figure}
%%% 
%%% \begin{example}\label{ex:cgen}
%%%   The seventh part of \emph{First.hx} is the final translation "CG" from the intermediate
%%%   representation to assembly code. This uses no new features, and is shown in Fig.~\ref{fig:cgen},
%%%   however, it is still worth a sanity check, walking through the "CG" scheme and checking that all
%%%   syntactic cases are covered.
%%% \end{example}
%%% 
%%% \begin{remark}[concrete \emph{vs.} raw syntax]
%%%   In the presentation we have chosen to use \emph{concrete syntax} even for semantic
%%%   operations. This has the advantage of allowing direct invocation of even complex structured
%%%   calculations from the command line but it does ``pollute'' the syntax of the defined language.
%%%   (Production versions of \HAX (not yet released) will have the option of generating parsers that
%%%   ignore concrete syntax of schemes when running the compiler.) It is sometimes practical to define
%%%   ``bridge schemes'' that make schemes available both in syntax and raw; we give an example of this
%%%   in the following section.
%%% \end{remark}
%%% 
%%% \begin{example}
%%%   The \emph{First.hx} example defines the "Compile" scheme as a top level function to be used from
%%%   the command line. This looks as follows:
%%% \begin{hacs}[xleftmargin=\parindent,numbers=right]
%%% sort AProgr | scheme Compile(Stat);
%%% Compile(#) → ⟦ CG ICG TA ⟨Stat#⟩ ⟧ ;
%%% \end{hacs}
%%%   The "Compile" scheme reflects how our compiler is structured, and in particular that the input is
%%%   a "Stat" and the output an "AProgr", which stands for ``assembly program.''
%%%   (You will see later what the right side of the "→" here means.)  This is the reason for the
%%%   \verb|--scheme=Compile| option we invoked back in the getting started section.  Such wrapper raw
%%%   schemes must have a single argument.
%%% \end{example}

\clearpage
\appendix\small

\section{Manual}\label{app:manual}

This appendix is an evolving attempt at giving a systematic description on \HAX.

\begin{manual}[grammar structure]\label{man:structure}
  A \HAX compiler is specified as a single \emph{.hx} module file with the following structure:
  %%
  \begin{hacs}[mathescape,xleftmargin=\parindent]
module $\text{\it\color{blue}modulename}$
{
  $\text{\it\color{blue}Declarations}$
}
  \end{hacs}
  %%
  where the \emph{modulename} should be a Java style fully qualified class name with the last
  component capitalized and the same as the file base name, \eg, \verb|org.crsx.hacs.samples.First|
  is an allowed module name for a specification stored as \emph{First.hx}. The individual sections
  specify the compiler, and the possible contents is documented in the manual blocks below.
\end{manual}

\begin{manual}[lexical declarations]\label{man:token}
  %%
  A token is declared with the keyword "token" followed by the token (sort) name, a "|" (vertical
  bar), and a \emph{regular expression}, which has one of the following forms (with increasing order
  of precedence):
  %%
  \begin{enumerate}

  \item Several alternative regular expressions can be combined with further "|" characters.

  \item Concatenation denotes the regular expression recognizing concatenations of what matches the
    subexpressions.

  \item A regular expression (of the forms following this one) can be followed by a \emph{repetition
      marker}: "?" for zero or one, "+" for one or more, and "*" for zero or more.

  \item A simple word without special characters stands for itself.

  \item A string in single or double quotes stands for the contents of the string except that "\"
    introduces an \emph{escape code} that stands for the encoded character in the string (see next item).

  \item A stand-alone "\" followed by an \emph{escape code} stands for that character: escape codes
    include the usual C and Java escapes: "\n", "\r", "\a", "\f", "\t", octal escapes like "\177",
    special character escapes like "\\", "\'", \hacsc|\"|, and Unicode hexadecimal escapes like
    "\u27e9".

  \item A \emph{character class} is given in "[ ]", with these rules:
   \begin{enumerate}
    \item if the first character is "^" then the character class is negated;
    \item if the first (after "^") character is "]" then that character is (not) permitted;
    \item a "\" followed by an \emph{escape code} stands for the encoded character;
    \item The characters \verb|\'"| should always be escaped (this is a bug);
    \item two characters connected with a "-" (dash) stands for a single character in the indicated
      (inclusive) \emph{range}.
    \end{enumerate}
    Note that a character class cannot be empty, however, "[^]" is permitted and stands for all
    characters.
    

  \item The "." (period) character stands for the character class "[^\n]".

  \item A nested regular expression can be given in "( )".

  \item An entire other token "T" can be included (by literal substitution, so recursion is not
    allowed) by writing "⟨T⟩" (the angle brackets are unicode characters U+27E8 and U+27E9). Tokens
    declared with "token fragment" can \emph{only} be used this way.

  \item The special declaration "space" defines what constitutes white space for the generated
    grammar. (Note that this does not influence what is considered space in the specification
    itself, even inside syntax productions.) A spacing declaration permits the special alternative
    "nested" declaration for nested comments, illustrated by the following, which defines usual
    C/Java style spacing with comments as used by \HAX itself:
\begin{hacs}[xleftmargin=\parindent]
space [ \t\f\r\n] | nested "/*" "*/" | "//" .* ;
\end{hacs}

  \end{enumerate}
  %%
  Notice that spacing is not significant in regular expressions, except (1) in character classes,
  (2) in literal strings, and (3) if escaped (as in "\ ").
  %%
\end{manual}

\begin{manual}[syntactic sorts]\label{man:syntax}
  %%
  Formally, \HAX uses the following notations for specifying the syntax to use for terms.
  %%
  \begin{enumerate}

  \item \HAX \emph{production names} are capitalized words, so we can for example use "Exp" for the
    production of expressions.  The name of a production also serves as the name of its \emph{sort},
    \ie, the semantic category that is used internally for abstract syntax trees with that root
    production.  If particular instances of a sort need to be referenced later they can be
    \emph{disambiguated} with a "#"$i$ suffix, \eg, "Exp#2", where $i$ is an optional number or
    other simple word.

  \item A sort is declared by one or more "sort" declarations of the name optionally followed by a
    number of \emph{abstract syntax production} alternatives, each starting with a~"|". A sort
    declaration sets the \emph{current sort} for subsequent declarations and in particular any
    stand-alone production alternatives. All sort declarations for a sort are cumulative.

  \item Double square brackets "⟦…⟧" (unicode U+27E6 and U+27E7) are used for \emph{concrete syntax}
    but can contain nested angle brackets "⟨…⟩" (unicode U+27E8 and U+27E9) with \emph{production
      references} like "⟨Exp⟩" for an expression (as well as several other things that we will come
    to later).  We for example write "⟦⟨Exp⟩+⟨Exp⟩⟧" to describe the form where two expressions are
    separated by a "+" sign.

  \item Concrete syntax specification can include "¶" characters to indicate where \emph{newlines}
    should be inserted in the printed output.

  \item A trailing "@"$p$ for some precedence integer $p$ indicates that either the subexpression or
    the entire alternative (as appropriate) should be considered to have the indicated precedence,
    with higher numbers indicating higher precedence, \ie, tighter association.  (For details on the
    limitations of how the precedence and left recursion mechanisms are implemented, see
    Appendix~\ref{app:limits}.)

  \item "sugar ⟦…⟧→…" alternatives specify equivalent forms for existing syntax: anything matching
    the left alternative will be interpreted the same as the right one (which must have been
    previously defined); references must be disambiguated.

  \item If a production contains only a reference to a token, where furthermore the token is defined
    such that it can end with a sequence of "_"$n$ units (an underscore followed by a count), then
    the sort can be qualified as a "symbol" sort, which has two effects:
    \begin{itemize}
    \item rules can use instances of the sort instead of sort references, and
    \item instances of the sort can be used for scoped binders.
    \end{itemize}

  \end{enumerate}
  %%
\end{manual}

\begin{manual}[parsed terms]\label{man:parsed}
  The term model includes \emph{parsed terms}.
  %%
  \begin{enumerate}

  \item Double square brackets "⟦…⟧" (unicode U+27E6 and U+27E7) can be used for \emph{concrete
      terms}, provided the \emph{sort} is clear, either
    \begin{enumerate}
    \item by immediately prefixing with the sort (as in "Exp⟦1+2⟧"), or
    \item by using as the argument of a defined constructor (as "IsType(⟦mytype⟧)"), or
    \item by using as an attribute value, or
    \item by using as a top level rule pattern or replacement term with a defined current sort.
    \end{enumerate}

  \item Concrete terms can contain nested raw terms in "⟨…⟩" (unicode U+27E8 and U+27E9). Such
    nested raw terms \emph{must} have an explicit sort prefix.

  \item The special term "error⟦…⟧" will print the error message embedded in "⟦…⟧", where one is
    permitted to embed "symbol"-declared variables in "⟨…⟩".

  \end{enumerate}

\end{manual}

\begin{manual}[raw terms, schemes, and rules]\label{man:raw}
  %%
  ``Raw'' declarations consist of the following elements:
  %%
  \begin{enumerate}

  \item A \emph{constructor} is a capitalized word (similar to a sort name but in a separate name
    space).

  \item A \emph{variable} is a lower case word (subject to scoping, described below).

  \item A sort can be given a \emph{semantic production} as a "|" (bar) followed by a \emph{form},
    which consists of a constructor name, optionally followed by a list of the subexpression sorts
    in parenthesis.

%%% "()"ed ","-separated list of
%%%    \emph{scope forms}, which each consist of a \emph{sort} optionally preceded by a \emph{binder
%%%      form}, which is a list of sorts followed by a "." (dot). Thus in the most general case, a
%%%    semantic production has the form
%%%    %%
%%%    \begin{equation*}
%%%      \texttt{|}~C~\texttt{(}
%%%      %~[S_{11},\cdots,S_{1n_1}]~S_1~\texttt{,}
%%%      …~\texttt{,}
%%%      ~S_{m1}\cdots S_{mn_m}~\texttt{.}~S_m
%%%      \texttt{)}
%%%    \end{equation*}
%%%    %%
%%%    with $C$ a constructor name and all $S_i$ and $S_{ij}$ sort names. The $S_i$ declares the
%%%    \emph{argument sort} for the $i$th argument of the construction term, and the $S_{ij}$ is the
%%%    \emph{binder sort} of the $j$th binder for the $i$th argument; $m$ is the \emph{arity} of the
%%%    construction and $n_i$ the \emph{rank} of the $i$th argument.

  \item A semantic production can be qualified as a "scheme", which marks the declared construction
    as a candidate for rewrite rules (defined below).

  \item A \emph{raw term} is either a \emph{construction}, a \emph{variable use}, or a
    \emph{meta-application}, as follows
    %%
    \begin{enumerate}

    \item A \emph{construction} term is a constructor name followed by an optional "()"ed
      ","-separated list of \emph{scope arguments}, which each consist of a term optionally preceded
      by an optional \emph{binder list} of variables enclosed in "[]" (dot).  So in the most general
      case, a term looks like this:
     %% 
     \begin{equation*}
       C~\texttt{(}
       ~\texttt{[}\,x_{11}\texttt{,}…\texttt{,}x_{1n_1}\,\texttt{]}~t_1~\texttt{,}
       ~…\texttt{,}
       ~\texttt{[}\,x_{m1}\texttt{,}…\texttt{,}x_{mn_m}\,\texttt{]}~t_m
       ~\texttt{)}
     \end{equation*}
     %%
     The ``$C$-construction'' is said to have the \emph{subterms} $t_1,…,t_m$, and the arity $m$ and
     ranks $n_1…n_m$ must correspond to a semantic production.  If present, the binder prefix of
     each introduces the specified variables \emph{only} for the appropriate subterm modulo usual
     renaming, \ie, writing \texttt{A([x,y].x, [x,y].y)} and \texttt{A([a,b].a, [a,b].b)} and even
     \texttt{A([s,t].s, [t,s].s)} all denote the same term following the conventions of
     \emph{α-equivalence}.  In a scope argument $\texttt{[}x\texttt{]}t$ we say that occurrences of
     $x$ in $t$ are \emph{bound} by the binder.

    \item A \emph{variable use} term is a variable, subject to the usual lexical scoping rules.

    \item A \emph{meta-application} term is a \emph{meta-variable}, consisting of a "#" (hash)
      followed by a number or word and optionally by a meta-argument list of ","-separated terms
      enclosed in "[]". Examples include "#t1" (with no arguments), "#[a,b,c]", and "#1[OK,#]".

    \end{enumerate}

  \item A term can have a \emph{sort prefix}. So the term
    $$"Type Unif(Type #t1, Type Float)"$$
    is the same as "Unif(#t1,Float)" provided "Unif" was declared with the raw production
    "|Unif(Type,Type)".

  \item A \emph{rewrite rule} is a pair of terms separatede by "→" (arrow, U+2192), with a few
    additional constraints: in the rule $p→t$, $p$ must be a \emph{pattern}, which means it must be
    a construction term that has been declared as a "scheme" (syntactic or raw) and with the
    restriction that all contained arguments to meta-applications must be bound variables, and all
    meta-applications in $t$ must have meta-variables that also occur in $p$ with the same number of
    meta-arguments.

    Rule declarations must either occur with the appropriate current sort or have a pattern with a
    sort prefix.

  \item One rule per scheme can be prefixed with the qualifier "default". If so then the pattern can
    have no structure: all subterms of the pattern scheme construction must be plain
    meta-applications. Such a default rule is applied \emph{after} it has been ensured that all
    other rules fail for the scheme.

  \item Finally, a rule can be prefixed with the word "rule" for clarity.

  \end{enumerate}
  %%
  Rules are used for \emph{rewriting}, a definition of which is beyond the scope of this document;
  please refer to the literature on higher order rewriting for details~\cite{Jouannaud:klop2005,Klop+:tcs1993}.
  %%
\end{manual}

\begin{manual}[attributes and synthesis rules]\label{man:attributes}\leavevmode
  %%
  \begin{enumerate}

  \item Attributes are declared by "attribute" declarations followed by an \emph{attribute form} of
    one of the following shapes:
    %%
    \begin{enumerate}
    \item "↑name(ValueSort)" defines that the synthesized attribute "name" has "ValueSort" values;
    \item "↑name{KeySort}" defines that the synthesized attribute "name" is a set of "KeySort" values;
    \item "↑name{KeySort:ValueSort}" defines that the synthesized attribute "name" is a map from
      "KeySort" to "Valuesort" values;
    \item "↓name(ValueSort)", "↓name{KeySort}", and "↓name{SymbolSort:ValueSort}" similarly for
      inherited attributes;
    \end{enumerate}

  \item One can add a simple \emph{synthesized attributes} after a raw data term as
    "↑"\emph{id}"("\emph{value}")", where the \emph{id} is an attribute name and the
    \emph{value} can be any term.

  \item Simple \emph{inherited attributes} are added similarly after a raw scheme term as
    "↓"\emph{id}"("\emph{value}")".

  \item An \emph{inherited symbol table attribute extension} is added to a raw scheme term as
    "↓"\emph{id}"{"\emph{symbol}":"\emph{value}"}", where the \emph{symbol} is either a variable
    or a constant (of the appropriate sort).

  \item A \emph{synthesized attribute reference} has the simple form "↑"\emph{id}";" and declares
    that the current sort synthesizes \emph{id} attributes.

  \item A scheme declaration can include \emph{inherited attribute references} of the form
    "↓"\emph{id}, which declares that the scheme inherits the \emph{id} attributes.

  \item A \emph{synthesis rule} is a special rule of the form $t↑name(t')$, where the term $t$ may
    contain subterms with attribute constraints. The rule specifies how terms of the current sort
    and shape $t$ synthesize \emph{id} attributes.

  \item In \emph{rules} one can use the special forms "↑#m", which captures \emph{all} synthesized
    attribute values; "↑t{:#ms}" ("↓t{:#ms}"), which captures the full set of keys or key-value mappings of the
    "t" synthesized (inherited) attribute.

  \end{enumerate}
  %%
  Inherited attributes are managed with regular rules (for schemes) with inherited attribute
  constraints and extensions.
  %%
\end{manual}

\begin{manual}[building and running]\label{man:run}\leavevmode
  To translate a \HAX script to an executable, run the \emph{hacs} command, which generates a number
  of files under a \emph{build} subdirectory, as well as the main script with a \emph{.run}
  extension.  The script accepts a number of options:
  %% 
  \begin{enumerate}

  \item \verb"--sort="\emph{Sort} sets the expected sort (and thus parser productions) for the input to
    \emph{Sort}. The input is read, normalized, and printed.

  \item \verb"--scheme="\emph{Constructor} sets the computation for the compiler to \emph{Constructor},
    which must be a unary raw scheme; the argument sort of \emph{Constructor} defines the parser
    productions to use.  The input is read, wrapped in the action, normalized, and printed.

  \item \verb"--term="\emph{text} use the \emph{text} as the input.

  \item \verb"--input="\emph{file} (or just the \emph{file}) reads the input from \emph{file}.

  \item \verb"--output="\emph{file} sends the input to \emph{file} (the default is the standard output).

  \item \verb"--errors" reports details of errors found by subprocesses.

  \item \verb"--verbose="\emph{n} sets the verbosity of the underlying \CRSX rewrite engine to $n$. The
    default is 0 (quiet) but 1--3 are useful (above 3 you get a lot of low level diagnostic output).

  \item \verb"--parse-verbose" activates (very!) verbose output from JavaCC of the parsing.

  \end{enumerate}
  %%
  You must provide one of \verb"--sort" or \verb"--scheme", and one of \verb"--term" and \verb"--input".

  Notice that the \emph{.run} script has absolute references to the files in the \emph{build}
  directory, so the latter should be moved with care.
\end{manual}


%%% \section{Bonus Examples}\label{appbonus}
%%% 
%%% Some additional \emph{untested} examples. \emph{Note: still need to be worked through.}
%%% 
%%% \begin{example}[\emph{hacs/samples/Bool.hx}]\leavevmode
%%% \inputhacs{../samples/Bool.hx}
%%% \end{example}
%%% 
%%% \begin{example}[\emph{hacs/samples/Deriv.hx}]\leavevmode
%%% \inputhacs{../samples/Deriv.hx}
%%% \end{example}


\section{Common Errors}\label{app:errors}

In this appendix we list some of the more common of what can be called the ``error messages'' of
\HAX. \emph{Note} that most of these only come out when \HAX is run with the \verb'-e' option.

\begin{error}[\HAX syntax]\leavevmode
\begin{code}
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
 Encountered " "." ". "" at line 35, column 6.
Was expecting one of:
    <MT_Repeat> ...
    "%Repeat" ...
    <MT_Attributes> ...
\end{code}
This error message from the \verb'hacs' command indicates a simple syntax errors in the \emph{.hx}
file.
\end{error}

\begin{error}[user syntax]\leavevmode
\begin{code}
Exception in thread "main" java.lang.RuntimeException:
  net.sf.crsx.CRSException: net.sf.crsx.parser.ParseException:
mycompiler.crs: Parse error in embedded myDecSome term at line 867, column 42:
 ⟦ $TA_Let2b ⟨Dec (#d)⟩{ ⟨DecSome (#ds)⟩} ⟧ at line 867, column 42
 Encountered " "\u27e9" "\u27e8Dec (#d)\u27e9 "" at line 867, column 53
…
\end{code}
  %%$
  This indicates a concrete syntax error in some parsed syntax---inside "⟦…⟧"---in the \emph{.hx}
  file. The offending fragment is given in double angles in the message. Check that it is correctly
  entered in the \HAX specification in a way that corresponds to a syntax production. Note that the
  line/column numbers refer to the generated \emph{build/…Rules.crs} file, which is not immediately
  helpful (this is a known bug). In error messages a sort is typically referenced as a lower case
  prefix followed by the sort name---here "myDecSome" indicates that the problem is with parsing the
  "DecSome" sort of the "My" parser.
\end{error}

\begin{error}[precedence error]\leavevmode
\begin{code}
Java Compiler Compiler Version 6.0_1 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file OrParser.jj . . .
Error: Line 170, Column 1: Left recursion detected: "N_Exp1... --> N_Exp2... --> N_Exp1..."
Detected 1 errors and 0 warnings.
\end{code}
  %%$
  This suggests that a production breaks the precedence rule that all subterm precedence markers
  must be at least as high as the entire production's precedence marker, in this case between the
  "Exp@1" and "Exp@2" prededence markers, so presumably one of the rules for "Exp" with "@2" allow
  as a first subterm an "Exp" with "@1".a
\end{error}


\begin{error}[JavaCC noise]\leavevmode
\begin{code}
Java Compiler Compiler Version ??.??_?? (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file FirstHx.jj . . .
Warning: Choice conflict involving two expansions at
         line 3030, column 34 and line 3033, column 8 respectively.
         A common prefix is: "{" <T_HX_VAR>
         Consider using a lookahead of 3 or more for earlier expansion.
Warning: Line 4680, Column 18: Non-ASCII characters used in regular expression.
Please make sure you use the correct Reader when you create the parser,
 one that can handle your character set.
File "TokenMgrError.java" does not exist.  Will create one.
File "ParseException.java" does not exist.  Will create one.
File "Token.java" does not exist.  Will create one.
File "SimpleCharStream.java" does not exist.  Will create one.
Parser generated with 0 errors and 1 warnings.
Note: net/sf/crsx/samples/gentle/FirstParser.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
\end{code}
  These are ``normal'' messages from JavaCC. Yes, the choice conflict is annoying but is in fact safe.
\end{error}

\begin{error}[missing library]\leavevmode
\begin{code}
gcc -std=c99 -g    -c -o crsx_scan.o crsx_scan.c
crsx.c:11:30: fatal error: unicode/umachine.h: No such file or directory
\end{code}
  The \HAX tools only use one library in C: ICU. You should get the \emph{libicu-dev} package (or
  similar) for your system.
\end{error}

\begin{error}[meta-variable mistake]\leavevmode
\begin{code}
Error in rule Tiger-Ty99_9148-1: contractum uses undefined meta-variable (#es)
Errors prevent normalization.
make: *** [pr3.crs-installed] Error 1
\end{code}
  A rule uses the metavariable "#es" in the replacement without defining it in the corresponding
  pattern.
\end{error}

\begin{error}[]\leavevmode
\begin{code}
/home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
cookmain: crsx.c:528: bufferEnd: Assertion
   `(((childTerm)->descriptor == ((void *)0)) ? 0 :
        (childTerm)->descriptor->arity) == bufferTop(buffer)->index' failed.
/bin/sh: line 1: 14278 Aborted
  (core dumped) /home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
\end{code}
  This indicates an arity error: a raw term in the \emph{.hx} file does not have the right number of
  arguments.
\end{error}

\begin{error}\leavevmode
\begin{code}
// $Sortify
// $[Load, ".../build/edu/nyu/csci/cc/fall14/Pr2Solution.hx", "pr2solutionMeta_HxModule"]
Exception in thread "main" edu.nyu.csci.cc.fall14.TokenMgrError:
   Lexical error at line 184, column 31.  Encountered: "t" (116), after : "Call"
\end{code}
This indicates that you have an undefined symbol of sort error in the \emph{.hx} file: the symbol
starting with "Callt" is either undefined or used in a location where it does not match the required
sort.
\end{error}

\begin{error}\leavevmode
\begin{code}
// $Sortify
// $[Load, ".../build/edu/nyu/csci/cc/fall14/Pr2Solution.hx", "pr2solutionMeta_HxModule"]
Exception in thread "main" java.lang.RuntimeException: net.sf.crsx.CRSException:
   Encountered " ")" ") "" at line 255, column 112.
Was expecting one of:
    "," ...
\end{code}
This indicates that you have an incorrect number of arguments in the \emph{.hx} file: here
insufficient arguments (encountering a ")" instead of ","); a similar but opposite error is given
when excess arguments are present.
\end{error}

\begin{error}[]\leavevmode
\begin{code}
/home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
cookmain: crsx.c:528: bufferEnd: Assertion
   `(((childTerm)->descriptor == ((void *)0)) ? 0 :
        (childTerm)->descriptor->arity) == bufferTop(buffer)->index' failed.
/bin/sh: line 1: 14278 Aborted
  (core dumped) /home/krisrose/Desktop/teaching/.../hacs/cookmain PG pr3.hxt > pr3.pg
\end{code}
  This indicates an arity error: a raw term in the \emph{.hx} file does not have the right number of
  arguments.
\end{error}

\begin{error}[]\leavevmode
\begin{code}
« $Print-Check[
...
»
\end{code}
%%$
This error from your \emph{.run} script indicates that you requested a \verb'--scheme' "Check",
which is not in fact declared as a "scheme" in the \emph{.hx} file.
\end{error}


\section{Limitations}\label{app:limits}

\begin{itemize}

\item At most one "nested" choice per "token" declaration.

\item It is not possible to use binders and left recursion in the same production with the same
  precedence.

\item Only \emph{immediate} left recursion is currently supported, \ie, left recursion should be
  within a single production. Specifically,
\begin{hacs}
sort A | ⟦ ⟨A⟩ stuff ⟧ | ⟦ other-stuff ⟧ ;
\end{hacs}
  and 
\begin{hacs}
sort A | ⟦ ⟨A@1⟩ stuff ⟧@1 | ⟦ other-stuff ⟧@2 ;
\end{hacs}
  are alright but
\begin{hacs}
sort A | ⟦ ⟨B⟩ a-stuff ⟧ | ⟦ other-a-stuff ⟧ ;
sort B | ⟦ ⟨A⟩ b-stuff ⟧ | ⟦ other-b-stuff ⟧ ;
\end{hacs}
  and 
\begin{hacs}
sort A  | ⟦ ⟨A@1⟩ stuff ⟧@2 | ⟦ other ⟧ ;
\end{hacs}
  are not: both involve indirect recursion (the latter case because the inner left recursive
  precedence 1 is less than the outer precedence 2). 

\item Productions can share a prefix but only within productions for the same sort, and the prefix
  has to be literally identical unit by unit (except for left recursive precedence markers), \ie,
\begin{code}
sort S | ⟦ ⟨A⟩ then ⟨B⟩ then C ⟧
       | ⟦ ⟨A⟩ then ⟨B⟩ or else D ⟧ ;
\end{code}
  is fine but
\begin{code}
sort S | ⟦ ⟨A⟩ then ⟨B⟩ then C ⟧
       | ⟦ ⟨A⟩ ⟨ThenB⟩ or else D ⟧ ;
sort ThenB | ⟦ then ⟨B⟩ ⟧;
\end{code}
  is not.

\item It is not possible to left-factor a binder (so multiple binding constructs cannot have the
  same binder prefix).

\item Variables embedded in "error⟦…⟧" instructions must start with a lower case letter.

\item When using the "symbol" qualifier on a reference to a token then the token \emph{must} allow
  ending with a sequence of "_"$n$ for $n$ any natural numbers.

\item When using the same name for a symbol inside of "⟦…⟧" and the corresponding raw variable
  outside of the ⟦⟧, then the common symbol and variable name must be a plain word starting with a
  lower case letter.

\item Special terms like "error⟦…⟧" cannot be used as raw subterms.

\item Synthesized attribute patterns with pattern matching in the attributes may not always work.

\end{itemize}


\bibliography{crs}


\end{document}


%% $Log: hacs-gently.tex,v $
%% Revision 1.29  2014/02/11 15:49:11  krisrose
%% Hash.crs fixed to allow linear use of hash.
%%
%% Revision 1.28  2014/01/26 21:14:34  krisrose
%% Compiled NumericEqual primitive fixed for rounding errors.
%%
%% Revision 1.27  2014/01/21 18:40:46  krisrose
%% Regenerated rulecompiler.
%%
%% Revision 1.26  2014/01/16 14:07:17  krisrose
%% Update compiled $[Decimal] to also handle double.
%%
%% Revision 1.25  2013/12/05 04:10:03  krisrose
%% Manual fix.
%%
%% Revision 1.24  2013/12/03 21:41:55  krisrose
%% Update of hacs.zip.
%%
%% Revision 1.23  2013/12/02 12:21:21  krisrose
%% Option fixes.
%%
%% Revision 1.22  2013/11/25 06:35:09  krisrose
%% HACS cleanup.
%%
%% Revision 1.21  2013/11/21 21:10:55  krisrose
%% Duplicate table row removed.
%%
%% Revision 1.20  2013/11/21 06:19:39  krisrose
%% HACS documentation rough version done.
%%
%% Revision 1.19  2013/11/21 04:01:32  krisrose
%% Newline support in HACS.
%%
%% Revision 1.18  2013/11/20 05:08:17  krisrose
%% HACS functional.
%%
%% Revision 1.17  2013/11/19 21:00:12  krisrose
%% first.hx refactored for hacs-gently runs again.
%%
%% Revision 1.16  2013/11/18 02:28:07  krisrose
%% HACS fully functional.
%%
%% Revision 1.15  2013/11/17 03:20:20  krisrose
%% reify option respects simple-terms.
%% Main .dr not output when modules generated.
%%
%% Revision 1.14  2013/10/30 04:28:28  krisrose
%% HACS synthesized attributes almost ready!
%%
%% Revision 1.13  2013/10/21 18:02:12  krisrose
%% HACS attribute grammar fixed.
%%
%% Revision 1.12  2013/10/17 16:51:43  krisrose
%% Locify avoids $[PassLocationProperties] when possible.
%%
%% Revision 1.11  2013/10/12 21:35:40  krisrose
%% Location and HACS work.
%% Checkpoint before major attribute distribution fix.
%%
%% Revision 1.10  2013/10/06 15:38:28  krisrose
%% First attempt at VariableUse with propserties.
%% Preparing for HACS for unmetaness.
%%
%% Revision 1.9  2013/09/30 11:06:42  krisrose
%% All HACS examples work again.
%%
%% Revision 1.8  2013/09/29 05:49:58  krisrose
%% Attributes next...
%%
%% Revision 1.7  2013/09/27 10:05:12  krisrose
%% Document RegExps.
%%
%% Revision 1.6  2013/09/25 17:14:15  krisrose
%% Change default action to Drop.
%%
%% Revision 1.5  2013/09/20 08:32:04  krisrose
%% Formatting.
%%
%% Revision 1.4  2013/09/20 08:17:57  krisrose
%% Drop diagnostic output.
%%
%% Revision 1.3  2013/09/20 08:11:58  krisrose
%% jar refresh
%%
%% Revision 1.2  2013/09/20 08:03:50  krisrose
%% Ready for the NYU students...
%%
%% Revision 1.1  2013/09/18 14:42:24  krisrose
%% Moved 'dragon' to 'gentle'.
%%
%% New.

%%---------------------------------------------------------------------
% Tell Emacs that this is a LaTeX document and how it is formatted:
% Local Variables:
% mode:latex
% fill-column:100
% End:
