\documentclass{article}
%\documentclass{easychair}

%% Style.
\usepackage{charter}
\usepackage{euler}
\bibliographystyle{plainurl}
\usepackage{cite}

\input{setup}
\def\version{\thanks{\textbf{UNFINISHED DRAFT---Feedback Appreciated}}}

%% Topmatter.
\title{Experience with Higher Order Rewriting \\ from the \\ Compiler Teaching Trenches}
%\titlerunning{HOR from the Compiler Teaching Trenches}
\author{Kristoffer H. Rose\\
  Two Sigma\thanks{} Labs
  and New York University
}
%\authorrunning{Kris Rose}
%\institute{Two Sigma Investments, LLC\\Courant Institute, New York University}

\begin{document}\small
\maketitle

\begin{abstract}\noindent
  We have now twice used the ``HACS'' compiler generator tool in an NYU graduate Compiler
  Construction class. HACS is based on a higher-order rewriting formalism, thus we have effectively
  been teaching students higher order rewriting techniques as the way to implement compilers. In
  this paper we report on how HACS matches specific rewriting notions to the techniques used by
  compiler writers, and where the main difficulties have been encountered in teaching these.
\end{abstract}

%\compacttableofcontents


\section{Introduction}

NYU (New York University) offers a graduate so-called ``Capstone'' class in \emph{Compiler
  Construction}~\cite{Rose:nyu2014}. The course is based on approximately a third of the material in
the ``dragon book'' \cite{Aho+:2006}, and includes a compiler programming project.  In class we
focus on the formal techniques used to structure a compiler, as the dragon book goes to great length
to use formal notions to describe and assist in the development of the various compiler components:

\smallskip\noindent\textbf{Regular Expressions} are used to formalize the \emph{lexical analysis} of (the text of)
  programs. The implementation of regular expressions by translation to deterministic finite
  automata (DFAs) is detailed.

\smallskip\noindent\textbf{Context Free Grammars} are used to formalize the \emph{syntax analysis} of programming
  languages, including how a program is converted into a parse tree.  Algorithms for syntax analysis
  based on translating grammars into data structures that can control either top-down (LL) or
  bottom-up (LALR) parsing are explained.

\smallskip\noindent\textbf{Attribute Grammars} (in variations called \emph{Syntax-Directed Definitions} (SDDs) and
  \emph{--~Translations}) are used to formalize most aspects of \emph{program analysis} and
  \emph{program translation} (such as to intermediate code).  General considerations on how one
  might ``evaluate'' an attribute grammar are considered, with remarks that one should really stick
  with specific restricted categories of attribute grammars that can be evaluated with a simple
  left-to-right strategy ( ``L-attributed'' for top-down and ``S-attributed'' for bottom-up
  parsing).

\smallskip\noindent\textbf{Graph Algorithms} are finally presented in somewhat informal form as the technique of choice
  for several optimization problems.

%%% The remaining parts of the dragon book cover further advanced rewriting-like algoritmhms, like
%%% tiling, but those are for now out of scope for the class.

In the class we present the view that the dragon book essentially defines the current practice of
compiler writing, and we notice that while it includes systematic methods for lexical and syntax
analysis, backed by tools that automate the process, no such methods are in common use for the
remaining formal notions.  Our goal is to retain and teach the original material in the dragon book
yet offer tool support for the entire compiler program development experience to enforce the deep
use of the used formalisms paired with an operational understanding of the involved algorithms.

Our answer to the issue raised above is the tool \HAX, which stands for \emph{Higher-order Attribute
  Contraction Schemes}.  \HAX allows the students to specify a complete compiler as a single source
file, which combines the following formal notations:
%%
\begin{itemize}

\item Lexical analysis based on usual \emph{regular expressions} for tokens and white space.

\item Syntax analysis based on a usual \emph{context free grammar} with special higher-order
  features supporting creation of lexical scopes (and the related symbol tables) in addition to
  (usual) operator precedence mechanics.

\item Propagation rules for \emph{synthetic attributes}.

\item Rules for specifying \emph{recursive translation schemes} that serve the dual role of
  supporting \emph{rewriting} and propagation of \emph{inherited attributes}.

\end{itemize}
%%
\HAX is built on top of \CRSX, which was developed and is used for implementing production compilers
at IBM~\cite{Rose:hor2007,rose:hor2010,Rose:rta2011}; a fact not wasted on the students, many of
whom are professionals pursuing an M.Sc.\ degree.

In this paper we discuss some details of the \HAX file format, and comment on issues encountered in
teaching these.
%%
In Section~\ref{sec:lex} we explain this for lexical analysis, and in Section~\ref{sec:syntax} for
syntactic analysis.
%%
Section~\ref{sec:analysis} then discusses how semantic analysis is introduced, which covers
explaining most of the \HAX concepts.
%%% , followed by the (very
%%% similar) translation to an \emph{intermediate representation} and \emph{code generation} in
%%% Section~\ref{sec:cg}.
%%
Throughout, we give examples from \emph{first.hx} of the \HAX distribution, which is a \HAX version
of~\cite[Chapter 2, Figure 7]{Aho+:2006}.


\section{Lexical Analysis}
\label{sec:lex}

Lexical analysis is the process of splitting the input text into tokens.  \HAX uses a fairly
standard variation of \emph{regular expressions} for this.

\begin{example}
  Here are the simple regular expressions used to define the spacing conventions and terminal
  symbols (tokens) for the compiler of our small expression language.
  %% 
\begin{hacs}[xleftmargin=\parindent,texcl]
space [ \t\n] ;

token Int    | ⟨Digit⟩+ ;
token Float  | ⟨Int⟩ [.] ⟨Int⟩ ;
token Id     | ⟨Lower⟩+ ([_]? ⟨Int⟩)* ;  // for symbols

token fragment Digit  | [0-9] ;
token fragment Lower  | [a-z] ;
\end{hacs}
\end{example}

The notation here is rather standard, and the only curious aspect is the use of the special Unicode
angle brackets $⟨…⟩$ to mark \emph{inclusion} of other defined tokens and fragments.  In addition,
spaces are not significant in regular expressions, as is the case for the formal notation used in
the dragon book.
%%
The lexical declarations also introduce the general shape of \HAX units: a keyword is followed by
the defined name and then several alternatives, all introduced by \texttt{|} (a vertical bar, like
the way conditional cases are done in Haskell~\cite{Marlow:haskell2010}), finally followed by a
\texttt{;} (semi-colon).
%%
The "Id" token is equipped with the special tail ``"([_]? ⟨Int⟩)*",'' which permits the use of "Id"
as a symbol token later on; this \HAX-particular convention of course has to be explained.
%%
As is common with parser generators, otherwise unique tokens (such as keywords) can be declared
``inline'' in the syntax productions where they are used, as we shall see below.


\section{Syntax Analysis}
\label{sec:syntax}

Once we have tokens, we can use \HAX to program a complete syntax analysis with a grammar that
specifies how the input text is decomposed to a \emph{parse tree} according to a \emph{concrete
  syntax} and how the desired \emph{abstract syntax tree} (AST) is constructed from that.

With it's roots in term rewriting and algebraic specification, \HAX focuses on the abstract syntax
tree, with explicit declarations of the algebraic \emph{sorts} that correspond to each kind of
abstract syntax tree node.  This means that \HAX works with the paradigm that \emph{all} concrete
syntax issues are handled by ``desugaring'' or ``disambiguation.''  For small and medium sized
grammars this works well (we come back to the limitations below).

\begin{example}\label{ex:syntax-hacs}
  %%
  Our small example source language merely has blocks, statements, and a few forms of expression.
  Here is a \HAX grammar for this:
  %%
\begin{hacs}[xleftmargin=\parindent,texcl]
sort Name  | symbol ⟦ ⟨Id⟩ ⟧ ;

sort Exp   | ⟦ ⟨Exp@1⟩ + ⟨Exp@2⟩ ⟧@1
           | ⟦ ⟨Exp@2⟩ * ⟨Exp@3⟩ ⟧@2
           | ⟦ ⟨Int⟩ ⟧@3
           | ⟦ ⟨Float⟩ ⟧@3
           | ⟦ ⟨Name⟩ ⟧@3
           | sugar ⟦ ( ⟨Exp#1⟩ ) ⟧@3 → Exp#1 ;

sort Stat  | ⟦ ⟨[x:Name]⟩ := ⟨Exp⟩ ; ⟨Stat[x:Exp]⟩ ⟧
           | ⟦ { ⟨Stat⟩ } ⟨Stat⟩ ⟧
           | ⟦ ⟧ ;
\end{hacs}
  %%
  We explain the details of the notation below.
  %%
\end{example}

The grammar is explained as capturing several elements from the dragon book:

\smallskip\noindent\textbf{Literal syntax} is indicated by the double ``syntax brackets,'' $⟦…⟧$.  Text inside $⟦…⟧$
  consists of three things only: space, literal character ``words,'' and references to non-terminals
  and predefined tokens inside (nested) $⟨…⟩$.  The literal syntax is taught as a variant of
  quasi-quoting or macro notation, common in many programming languages.

\smallskip\noindent\textbf{Symbol management} is specified with the "symbol" annotation used in the "Name" sort
  declaration. This declares that the "Name" sort is special in that the system should rename "Name"
  AST nodes to ensure that they are unique up to renaming, just as would be done in usual symbol
  table management.

\smallskip\noindent\textbf{Syntactic sugar} is represented by the "sugar" part of the "Exp" sort declaration, which
  states that the parser should accept an "Exp" in parenthesis, identified as "#1", and replace it
  with just that same "Exp", indicated by the "→ Exp#1" part.  This is explained as part of the
  mechanics between concrete and abstract syntax.

\smallskip\noindent\textbf{Precedence rules} are represented by the "@"-annotations, which assign precedence to each
  operator. This is taught in the obvious way by explaining that the same "Exp" language would be
  recognized by the following concrete \HAX specification (where we have also made the sugar
  concrete):
  %%
\begin{hacs}[xleftmargin=\parindent]
sort Exp   | ⟦ ⟨Exp1⟩ ⟧ ;
sort Exp1  | ⟦ ⟨Exp1⟩ + ⟨Exp2⟩ ⟧ | ⟦ ⟨Exp2⟩ ⟧ ;
sort Exp2  | ⟦ ⟨Exp2⟩ * ⟨Exp3⟩ ⟧ | ⟦ ⟨Exp3⟩ ⟧ ;
sort Exp3  | ⟦ ⟨Int⟩ ⟧ | ⟦ ⟨Float⟩ ⟧ | ⟦ ⟨Name⟩ ⟧ | ⟦ ( ⟨Exp⟩ ) ⟧ ;
\end{hacs}
  %%
  One can then explain the abstract syntax tree as the conversion of the concrete syntax tree
  obtained with this explicitly layered grammar to the structure described by the \HAX grammar in
  Example~\ref{ex:syntax-hacs} with the sugar and "@"-annotations removed.

\smallskip\noindent\textbf{Lexical scoping} and the associated renaming rules are specified as part of the syntax in
  \HAX---this of course is part of what makes \HAX a \emph{higher order} formalism.

%%  This has to be explicitly explained to students with the basic rules:
%%  \begin{itemize}
%%
%%  \item When a non-terminal reference to a sort is given as "⟨[x:T]⟩" instead of "⟨T⟩" for some
%%    sort~"T", the parser will behave the same as for "⟨T⟩" except the actual symbol will be recorded
%%    as a \emph{binder}, referenced as "x", and not put in the generated AST.
%%
%%    The binder declaration is only allowed when the "T" sort declaration contains a single
%%    alternative referencing a token with a "symbol" annotation and the special "([_]? ⟨Int⟩)*"
%%    suffix (as discussed above).
%%
%%  \item When a binder "x" has been defined, then a reference to the "N" sort can be given as
%%    "⟨N[x:S]⟩" with "S" some other sort, which then means the same as "⟨N⟩" except it explicitly
%%    declares the \emph{scope} of "x", \ie, any occurences of the "x" binder inside "N" will be
%%    \emph{bound} occurrences of the corresponding symbol of sort "S".
%%
%%    The scope declaration is allowed when all of the following are true: "x" is defined in the
%%    production by a binder declaration as above, one of the alternatives for the "N" sort is a
%%    simple reference "⟦⟨S⟩⟧", and finally "S" is either the same as "T" or contains an alternative,
%%    which is a simple reference "⟦⟨T⟩⟧".
%%
%%  \end{itemize}
  In our example, the first alternative of the "Stat" sort declaration defines a static scope with
  the use of "[x:…]".  The "⟨[x:Name]⟩" reference is the same as "⟨Name⟩" where we instead of
  inserting the token into the syntax tree just load it as a bound variable that we shall refer to
  as "x".  The "⟨Stat[x:Exp]⟩" reference is then the same as "⟨Stat⟩" except we also require that
  all the occurrences of the variable refererred to as "x" are considered to be instances of that
  bound variable.  The declaration is allowed because the additional conditions are satisfied.

Of these, the lexical scoping, which is a kind of higher-order abstract syntax (HOAS), is of course
the hardest to explain.  It is explained as a mechanism to implement symbol tables with lexical and
global scoping as well as allowing much easier access to scoped information in the later passes of
the compiler.  This is justified by showing how the students can experiment cleanly with the HOAS
aspect without being distracted by all the other functions that the symbol table has, and the claims
that the up front complexity pays back many times over, when the body of the compiler is to be
written, is justified by proceeding right to such an example, described in the following section.


\section{Semantic Analysis}
\label{sec:analysis}

In terms of higher order rewriting, the above sections provide algebraic sorts, defined by the
syntax productions---in terms of the dragon book, and thus our students, a mechanism for generating
abstract syntax trees (ASTs).

The next stage is to implement \emph{semantic} analyses: rules for enriching the AST with additional
information that facilitates the subsequent transformation to an intermediate representation (IR).
We use type analysis as an example.  Because type analysis uses the symbol table in a non-trivial
way, the dragon book delays type analysis until later, however, in the class we use it as an early
example of an analysis specified by a syntax-directed definition (SDD).

\begin{figure}[h]
  \begin{equation*}
    \begin{array}{r@{\,}l|lr}
      \hline
      \hline
      \multicolumn{2}{c|}{\textsc{Production}}  & \textsc{Semantic Rules} &\\
      \hline\Bigstrut
      S &→ \textbf{name} := E_1; S_2
      & E_1.e = S.e; S_2.e = \op{Extend}(S.e, \textbf{name}.sym, E_1.t) &\thetag{S1}
      \\[\jot]
      &\mid \{~S_1~\}~S_2 & S_1.e = S.e; S_2.e = S.e &\thetag{S2}
      \\[\jot]
      &\mid ε & &\thetag{S3}
      \\[\jot]
      \hline\Bigstrut
      E &→ E_1 + E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E1}\\[\jot]
      &\mid E_1 \ast E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) &\thetag{E2}\\[\jot]
      &\mid \textbf{int} & E.t = \op{Int}&\thetag{E3}\\[\jot]
      &\mid \textbf{float} & E.t = \op{Float}&\thetag{E4}\\[\jot]
      &\mid \textbf{name} & E.t = \text{if}~\op{Defined}(E.e,\textbf{name}.sym)~\text{then}~\op{Lookup}(E.e,\textbf{name}.sym)&\thetag{E5}\\
      && \qquad\quad\text{else} \op{TypeError}
      \\[\jot]
      \hline
    \end{array}
  \end{equation*}
  \caption{SDD for type checking.}
  \label{fig:sdd}
\end{figure}

\begin{figure}[t]\small
\begin{hacs}[xleftmargin=1.5em,numbers=left,numberstyle=\scriptsize,texcl]
// TYPE ANALYSIS

sort Type  | Int | Float | TypeError
           | scheme Unif(Type,Type) ;

Unif(Int, Int) → Int;
Unif(#t1, Float) → Float;
Unif(Float, #t2) → Float;
default Unif(#1,#2) → TypeError;

attribute↑t(Type);  // synthesized expression type
sort Exp | ↑t;

⟦ (⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ (⟨Exp#1 ↑t(#t1)⟩ * ⟨Exp#2 ↑t(#t2)⟩) ⟧ ↑t(Unif(#t1,#t2));
⟦ ⟨INT#⟩ ⟧ ↑t(Int);
⟦ ⟨FLOAT#⟩ ⟧ ↑t(Float);

attribute↓e{Name:Type};  // inherited type environment

sort Exp | scheme ⟦ TA ⟨Exp⟩ ⟧ ↓e ;  // propagates e over Exp

⟦ TA id ⟧ ↓e{⟦id⟧ : #t} → ⟦ id ⟧ ↑t(#t);
⟦ TA id ⟧ ↓e{¬⟦id⟧} → error⟦Undefined identifier ⟨id⟩⟧;

⟦ TA ⟨INT#⟩ ⟧ → ⟦ ⟨INT#⟩ ⟧;
⟦ TA ⟨FLOAT#⟩ ⟧ → ⟦ ⟨FLOAT#⟩ ⟧;
⟦ TA (⟨Exp#1⟩ + ⟨Exp#2⟩) ⟧ → ⟦ (TA ⟨Exp#1⟩) + (TA ⟨Exp#2⟩) ⟧;
⟦ TA (⟨Exp#1⟩ * ⟨Exp#2⟩) ⟧ → ⟦ (TA ⟨Exp#1⟩) * (TA ⟨Exp#2⟩) ⟧;

sort Stat | scheme ⟦ TA { ⟨Stat⟩ } ⟧ ↓e ;  // propagates e over Stat

⟦ TA { id := ⟨Exp#1⟩; ⟨Stat#2[⟦id⟧]⟩ } ⟧ → ⟦ TA2 { id := TA ⟨Exp#1⟩; ⟨Stat#2[⟦id⟧]⟩ } ⟧;
{
  | scheme ⟦ TA2 { ⟨Stat⟩ } ⟧ ↓e;
  ⟦ TA2 { id := ⟨Exp#1 ↑t(#t1)⟩; ⟨Stat#2[⟦id⟧]⟩ } ⟧ →
    ⟦ id := ⟨Exp#1⟩; ⟨Stat ⟦TA {⟨Stat#2[⟦id⟧]⟩}⟧ ↓e{⟦id⟧:#t1}⟩ ⟧;
}

⟦ TA { { ⟨Stat#1⟩ } ⟨Stat#2⟩ } ⟧ → ⟦ { TA { ⟨Stat#1⟩ } } TA { ⟨Stat#2⟩ } ⟧;

⟦ TA { } ⟧ → ⟦ { } ⟧;
\end{hacs}
  \caption{\HAX code for type analysis.}
  \label{fig:analysis-hacs}
\end{figure}

\begin{example}\label{ex:sdd}
  %%
  An SDD for a simple type analysis can be implemented with two attributes (and using the usual
  convention that the SDD uses $E$ and $S$ where the \HAX grammar has "Exp" and "Stat"):
  %%
  \begin{itemize}

  \item The inherited \emph{environment} attribute $e$, which is a map from variables to types on
    both the statement and expression non-terminals $S$ and $E$.

  \item The synthesized \emph{type} attribute $t$ on expressions $E$, which contains the type of the
    expression.

  \end{itemize}
  %%
  With those, the SDD can be expressed as shown in Figure~\ref{fig:sdd}, where we use the helpers
  \op{Extend}, \op{Defined}, and \op{Lookup} to build an extended environment with an additional
  type declaration, check for existence, and look one up, respectively, and \op{Unif} to find the
  type of an arithmetic operation with operands of two specific types.
  %% 
\end{example}

The SDD in Example~\ref{ex:sdd} is explained as a slightly more declarative version than the SDDs in
the dragon book, where SDDs generally operate on the symbol table using side effects.  It is not
hard to justify to students why it is worthwhile to strive for \emph{immutable} symbol tables and
that this helps relaxing the evaluation order used to evaluate the attributes.

\begin{example}\label{ex:analysis-hacs}
  The full \HAX code of the type checker is shown in Figure~\ref{fig:analysis-hacs}.
\end{example}

The \HAX code for the SDD of the example reveals most of the remaining \HAX properties:

\smallskip\noindent\textbf{Algebraic sorts} are available: the example defines the sort "Type" which contains our
  possible types as well as a so-called \emph{scheme} which has associated signature as well as…

\smallskip\noindent\textbf{Rewrite rules} can be given for schemes, here for example "Unif" in lines 6 through 9.  A
  scheme can have a "default" rule used as a fall-back.  \emph{Meta-variables} in rewrite rules are
  denoted by a starting~"#".

\smallskip\noindent\textbf{Synthesized attributes} are declared with an "attribute↑" declaration followed by the
  attribute name and sort of the attribute value to propagate in "()"s. Line 11 is an example of
  this, declaring the attribute "t" of sort "Type", and line 12 then declares that "t" is available
  on AST nodes of "Exp" sort.

\smallskip\noindent\textbf{Synthesis propagation rules} are for synthesized attributes that depend exclusively on other
  synthesized attributes (``S-attributed'') follow a specific scheme, illustrated by lines 14
  through 17.  Note that with \HAX this can be the case for only \emph{some} of the productions, as
  is the case here: the SDD productions \thetag{E1--E4} declare the $t$ attribute in a way that only
  depends on subterm synthesized attributes.  The pattern of such S-attributed propagation rules is
  as follows, illustrated by \thetag{E1}.  It appears like this in the SDD:
  \begin{equation*}
    \begin{array}{r@{\,}l|lr}
      \hline\Bigstrut
      E &→ E_1 + E_2 & E_1.e=E.e; E_2.e=E.e; E.t = \op{Unif}(E_1.t, E_2.t) \qquad&\thetag{E1}\\[\jot]
      \hline
    \end{array}
  \end{equation*}
  %%
  Here is how we teach converting the local S-attributed propagation of $t$ to \HAX form.
  %%
  \begin{enumerate}

  \item The first thing to do is copy the syntax production but omit any "@"-markings and add a
    unique "#"$n$ disambiguation mark to each production.  Since \thetag{E1} is based on the
    alternative
\begin{hacs}
sort Exp   | ⟦ ⟨Exp@1⟩ + ⟨Exp@2⟩ ⟧@1
\end{hacs}
    we start with 
\begin{hacs}
⟦ ⟨Exp#1⟩ + ⟨Exp#2⟩ ⟧
\end{hacs}
    where we have used the subscripts from \thetag{E1} as "#"-disambiguation marks.

  \item Next add in \emph{synthesis patterns} for the attributes we are reading.  Each attribute
    reference like $E_1.t$ becomes a pattern like "⟨Exp#1 ↑t(#t1)⟩", where the meta-variables like
    "#t1" should each be unique.  For our example, this gives us
\begin{hacs}
⟦ ⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩ ⟧
\end{hacs}
    which sets up "#t1" and "#t2" as synonyms for $E_1.t$ and $E_2.t$, respectively.

  \item Finally, add in the actual synthesized attribute, using the same kind of pattern at the
    \emph{end} of the rule (and add a ";"), and we get
\begin{hacs}
⟦ ⟨Exp#1 ↑t(#t1)⟩ + ⟨Exp#2 ↑t(#t2)⟩ ⟧ ↑t(Unif(#t1,#t2)) ;
\end{hacs}

  \end{enumerate}
  %%
  Synthesis rules are not hard for the students to get right.  We make sure that the invariant is
  clear: the purpose of synthesis rules is to make it possible to match on a synthesized attribute
  for every subexpression of the appropriate sort.  We also make sure that at this point the
  students take note of an rules that were \emph{not} S-attributed and are thus still left out.

\smallskip\noindent\textbf{Inherited attributes} are specified with an "attribute↓" declaration. This is similar to
  synthesized attributes except we also support \emph{maps} such as symbol tables (but the keys do
  not have to be symbols). Value attributes are specified as for synthesized with an attribute name
  followed by the sort in "()"s, and map attributes are specified with the name followed by a "{",
    the key sort, a ":", the value sort, and a "}".  The $e$ attribute of the SDD is declared in
  \HAX in line 19 in this way as a map from "Name" keys to "Type" values, thus capturing the type of
  variables.

\smallskip\noindent\textbf{Recursive schemes} are the final tool available. For our SDD we have to use recursive schemes
  for any semantic rule that involves any inherited attributes.  The basic rules that we teach are
  these:
  %%
  \begin{enumerate}

  \item Every inherited attribute needs to be \emph{associated} to a recursive scheme, which will
    then be responsible for propagating that inherited attribute.

    In general a scheme is declared with a "scheme" declaration, like the one in line 21, which
    otherwise looks like a syntax production alternative, and indeed \emph{defines new syntax}.
    This has to be carefully explained to students: that we effectively extend the syntax of the
    user language---after line 21, the expression \texttt{TA 2} is a legal expression in the
    language, however, it is a fake expression that is used internally by the compiler.  (This also
    has the advantage that one can more easily test internal schemes of the compiler by providing
    input ``programs'' that contain the schemes.)

    Line 21 also contains the association of the inherited "e" attribute to the "TA" "Exp" scheme:
    the "↓e" suffix.  This implies that \emph{every application of the TA scheme will always
      propagate the inherited e attribute from its context}.

    Line 31 achieves the similar effect to the "TA" scheme for "Stat" nodes (the name can be the
    same as there is no syntactic overlap between "Stat" and "Exp", a fact the students can
    understand from the parser material already treated in depth at this point).  Only notice that
    to avoid ambiguity, the "TA" scheme for the "Stat" sort includes "{}"s as part of the syntax.

  \item \emph{Semantic rules with simple inheritance} then get translated to straight recursive traversal
    rules.  An example of this is \thetag{E1}, where the environment $e$ is propagated straight to
    the two components $E_1$ and $E_2$. Line 28 captures this; indeed all of \thetag{E1--E4} satisfy
    this requirement, \thetag{E3--E4} trivially so, as captured by lines 26--29.  Conversely for 

    We here have to explain carefully how we exploit the syntax mechanism, so for example in line 28
    the left of the "→" is a \HAX pattern representation defining the application of "TA" to the
    expression $E_1+E_2$, whereas the right of the "→" represents the "+"-"Exp" where the two
    subexpressions are applications of "TA" to the sub-"Exp" that correspond to $E_1$ and $E_2$,
    respectively (including the use of the syntactic sugar~"(…)").  The recursive scheme fully
    represents the propagation of the inherited $e$ attribye in \thetag{E1}: the application of "TA"
    on the left side of "→" captures an $e$ corresponding to $E.e$ in \thetag{E1}, and the two
    recursive applications of "TA" on the right side of "→" conversely represent passing ``the $e$''
    into the corresponding subexpression, corresponding to the two attribute definitions
    $E_1.e=E.;E_2.e=E.e$.

    Here we also explain that it is posible to ``expand'' patterns such as the ones used in Line 28,
    which could have been written with nested references and literal syntax like this:
\begin{hacs}
⟦ TA ⟨Exp ⟦⟨Exp#1⟩ + ⟨Exp#2⟩⟧⟩ ⟧
  → ⟦ ⟨Exp ⟦TA ⟨Exp#1⟩⟧⟩ + ⟨Exp ⟦TA ⟨Exp#2⟩⟧⟩ ⟧;
\end{hacs}
    avoiding any use of the "()" sugar.  It is sometimes a help for students to go through this a
    few times when they have issues in the code.

    Line 40 and 41 correspond in a similar simple way to the simple inheritance of \thetag{S2--S3}.

  \item \emph{Semantic rules with synthetic depending on one inherited} are handled next.  They are
    rules that synthesize some attributes depending only on other synthetic attributes (of subterms)
    and, crucially, depend on \emph{one} inherited attribute.  An example in the example SDD is
    \thetag{E5}, which defines the $t$ synthesized attribute in terms of the inherited $e$.  This is
    represented by the rules in lines 23--24, which show how we define the "TA" scheme (since we are
    using "e") for the "Name" case: we have to use a parsed "Name", "id", and then we have two
    subcases, corresponding to the two cases of the \op{Defined} predicate from the SDD:
    \begin{itemize}

    \item the first case in line 23 captures that "e" has mapping from "⟦id⟧" (that is "id" parsed
      as a "Name", the key sort) to some type, which is captured by "#t". The "TA" scheme then takes
      care of the synthesis of "t" \emph{and} the propagation of "e" (which is trivial in this
      case);

    \item the second case in Line 24 captures when "e" does not have a mapping, which should result
      in an error.

    \end{itemize}
    When this situation arises, which is common, then we get a \emph{scheme dependency}: synthesis
    of the "t" attribute \emph{depends on} having finished the "TA" scheme for the appropriate
    subterm.

  \item Rules that \emph{extend the map attribute} require a bit more notation.  In our example,
    this is just \thetag{S1}, which extends the $e$ attribute that is passed down to $S_2$ with one
    more symbol to type mapping.  Specifically, we have to explain how to encode what we have
    written as $\op{Extend}(N_1.a,k,v)$ in the SDD notation, assuming the $a$ inherited attribute is
    associated with a scheme, say the "A" scheme for the "S" sort.  This is written as the special
    reference 
    \begin{center}
      "⟨S ⟦A ⟨N#1⟩⟧ ↓a{k:v}⟩".
    \end{center}
    We dissect this with the students:
    \begin{itemize}
    \item The outermost "⟨S…⟩" just establishes that this is an "S"-unit in the context (which we
      assume to be parsed syntax).
    \item The actual unit is computed by the "A" scheme on $N_1$, which we refer to by convention as
      "⟨N#1⟩", so the whole invokation is "⟦A ⟨N#1⟩⟧". If we were not using a changed $e$ that would
      be all, but we are modifying the "e" attribute sent in, by adding a map, which is indicated by
      adding "↓a{k:v}" for whatever "k" and "v" are.
    \end{itemize}
    However, \thetag{S1} has one more special feature, which is the next point invesitigated.

  \item Rules that \emph{have left-to-right attribute dependencies} require that helper schemes are
    introduced.  This is similar to the way one reasons about how to execute operational semantic
    inference systems ``from left to right:'' the intermediate state where only part of the
    attributes (premises for inference system) are instantiated needs to be captured.

    For rule \thetag{S1} this is present in that the extension of the inherited $S_2.e$ depends on
    the synthesized $E_1.t$, which in \HAX depends on having computed "TA" on the "Exp"
    corresponding to $E_1$ because "t" depends on "TA".

    This justifies the final block, lines 33--38, which consists of a ``splitting'' rule, line 33,
    that does nothing but invoke "TA" on the expression \emph{as well} as a new helper scheme,
    "TA2", invoked on the entire statement \emph{with} the nested "TA" inside.  "TA2" is then
    declared in line 35 (in the same sort and also propagating "e"). Line 36 has the "TA2" pattern,
    which now can safely require a synthesized "t" attribute on the embedded expression, captured by
    "#t1" in the pattern corresponding to $E_1.t$ in \thetag{S1}. Line 37 has the result of "TA2"
    which is a statement where we recurse the "TA" scheme into the $S_2$ substatement with an
    extended environment, written as discussed above.

  \end{enumerate}

Throughout this some of the details of the \HAX syntax matters, for example that the last "sort"
declaration is remembered as ``the current sort,'' which is what permits rules and some declarations
to ``know'' what sort they are applied to.

We also have to communicate some of the general properties of rewriting: that there is no predefined
strategy, so rules have to be explicitly mutually exclusive (with "default" rule as the exception).

Finally, it has been very helpful to work in a strongly (many-)sorted algebraic system, as it is
often possible when students have a problem to break their rules and declarations down to check
consistency.


%%% \section{Intermediate and Code Generation}
%%% \label{sec:cg}
%%% 
%%% The translation of analysis SDDs, detailed so far, is by far the hardest technique to explain.
%%% Actual \emph{translations} are much simpler to encode in rewriting.  The dragon 
%%% 
%%% 
%%% 
%%% 
%%% 
%%% \begin{figure}[p]\small
%%% \begin{code}
%%% token T | T ('_' ⟨Int⟩)* ; // temporary
%%% 
%%% // Concrete syntax & abstract syntax sorts.
%%% 
%%% sort I_Progr  | ⟦⟨I_Instr⟩ ⟨I_Progr⟩⟧ | ⟦⟧ ;
%%% 
%%% sort I_Instr  | ⟦⟨Tmp⟩ = ⟨I_Arg⟩ + ⟨I_Arg⟩;¶⟧
%%%               | ⟦⟨Tmp⟩ = ⟨I_Arg⟩ * ⟨I_Arg⟩;¶⟧
%%%               | ⟦⟨Tmp⟩ = ⟨I_Arg⟩;¶⟧
%%%               | ⟦⟨Name⟩ = ⟨Tmp⟩;¶⟧
%%%               ;
%%% 
%%% sort I_Arg    | ⟦⟨Name⟩⟧
%%%               | ⟦⟨Float⟩⟧
%%%               | ⟦⟨Int⟩⟧
%%%               | ⟦⟨Tmp⟩⟧
%%%               ;
%%% 
%%% sort Tmp | symbol ⟦ ⟨T⟩ ⟧ ;
%%% 
%%% // Translation scheme.
%%% 
%%% attribute ↓TmpType{Tmp:Type} ;
%%% 
%%% sort I_Progr;
%%% 
%%% | scheme ⟦ ICG { ⟨Stat⟩ } ⟧ ↓TmpType ;
%%% 
%%% ⟦ ICG { id := ⟨Exp#2 ↑t(#t2)⟩; ⟨Stat#3[⟦id⟧]⟩ } ⟧
%%%    → ⟦ { ⟨I_Progr ⟦ICGExp T ⟨Exp#2⟩⟧ ↓TmpType{⟦T⟧:#t2}⟩ } id = T; ICG { ⟨Stat#3[⟦id⟧]⟩ }  ⟧ ;
%%% ⟦ ICG { { ⟨Stat#1⟩ } ⟨Stat#2⟩ } ⟧ → ⟦ { ICG { ⟨Stat#1⟩ } } ICG { ⟨Stat#2⟩ } ⟧ ;
%%% ⟦ ICG { } ⟧ → ⟦ ⟧;
%%% 
%%% | scheme ⟦ ICGExp ⟨Tmp⟩ ⟨Exp⟩ ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨Int#1⟩ ⟧ → ⟦ T = ⟨Int#1⟩; ⟧ ;
%%% ⟦ ICGExp T ⟨Float#1⟩ ⟧ → ⟦ T = ⟨Float#1⟩; ⟧ ;
%%% ⟦ ICGExp T id ⟧ → ⟦ T = id; ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨Exp#1⟩ + ⟨Exp#2⟩ ⟧
%%%   → ⟦ {ICGExp T_1 ⟨Exp#1⟩} {ICGExp T_2 ⟨Exp#2⟩} T = T_1 + T_2; ⟧ ;
%%% 
%%% ⟦ ICGExp T ⟨Exp#1⟩ * ⟨Exp#2⟩ ⟧
%%%   → ⟦ {ICGExp T_1 ⟨Exp#1⟩} {ICGExp T_2 ⟨Exp#2⟩} T = T_1 * T_2; ⟧ ;
%%% 
%%% // Helper to flatten code sequence.
%%% | scheme ⟦ {⟨I_Progr⟩} ⟨I_Progr⟩ ⟧;
%%% ⟦ {} ⟨I_Progr#3⟩ ⟧ → #3 ;
%%% ⟦ {⟨I_Instr#1⟩ ⟨I_Progr#2⟩} ⟨I_Progr#3⟩ ⟧ → ⟦ ⟨I_Instr#1⟩ {⟨I_Progr#2⟩} ⟨I_Progr#3⟩ ⟧;
%%% \end{code}
%%%   \caption{Intermediate Code Generation.}
%%%   \label{fig:icgen}
%%% \end{figure}
%%% 
%%% \begin{example}\label{ex:icgen}
%%%   %%
%%%   The part of \emph{first.hx} that handles the translation from abstract syntax to the intermediate
%%%   representation is shown in Fig.~\ref{fig:icgen}. The fragment contains the usual components: a
%%%   syntax specification, rewrite schemes, and rewrite rules for the "ICG" scheme.
%%% 
%%%   The code only uses two new features: "¶" markers in the syntax to indicate newlines, and rules
%%%   that introduce \emph{fresh} variables (of "Tmp" sort): when the replacement of a rule uses a
%%%   symbol, which was not in the pattern, then this corresponds to \emph{generating} a new globally
%%%   unique symbol. So each time the rule
%%% \begin{code}
%%% ⟦ ICG id := ⟨Exp#2 ↑HasType(#t2)⟩; ⟧
%%%   → ⟦ { ⟨I_Progr ⟦ICGExp T ⟨Exp#2⟩⟧ ↓TmpType{T:#t2}⟩ } id = T; ⟧ ;
%%% \end{code}
%%%   is used, "T" denotes a new so-called ``fresh'' symbol.  When printed, the various incarnations of
%%%   "T" will be named "T_1", "T_86", \etc
%%%   %%
%%% \end{example}
%%% 
%%% \begin{figure}[p]\small
%%% \begin{code}[xleftmargin=1.66em,numbers=left]
%%% sort A_Progr | ⟦ ⟨A_Instr⟩ ⟨A_Progr⟩ ⟧ | ⟦⟧ ;
%%% 
%%% sort A_Instr | ⟦ LDF ⟨Tmp⟩, ⟨A_Arg⟩ ¶⟧
%%%              | ⟦ STF ⟨Name⟩, ⟨Tmp⟩ ¶⟧
%%%              | ⟦ ADDF ⟨A_Arg⟩, ⟨A_Arg⟩, ⟨A_Arg⟩ ¶⟧
%%%              | ⟦ MULF ⟨A_Arg⟩, ⟨A_Arg⟩, ⟨A_Arg⟩ ¶⟧ ;
%%% 
%%% sort A_Arg | ⟦ #⟨Float⟩ ⟧ | ⟦ #⟨Int⟩ ⟧ | ⟦ ⟨Name⟩ ⟧ | ⟦ ⟨Tmp⟩ ⟧ ;
%%% 
%%% sort A_Progr | scheme ⟦ CG ⟨I_Progr⟩ ⟧ ;
%%% 
%%% ⟦ CG ⟧ → ⟦⟧ ;
%%% 
%%% ⟦ CG T = ⟨I_Arg#1⟩ + ⟨I_Arg#2⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ ADDF T, [⟨I_Arg#1⟩], [⟨I_Arg#2⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% ⟦ CG T = ⟨I_Arg#1⟩ * ⟨I_Arg#2⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ MULF T, [⟨I_Arg#1⟩], [⟨I_Arg#2⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%%   
%%% ⟦ CG T = ⟨I_Arg#1⟩ ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ LDF T, [⟨I_Arg#1⟩] CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% ⟦ CG name = T ; ⟨I_Progr#⟩ ⟧
%%%   → ⟦ STF name, T CG ⟨I_Progr#⟩ ⟧ ;
%%% 
%%% sort A_Arg | scheme ⟦ [⟨I_Arg⟩] ⟧ ;
%%% ⟦ [T] ⟧ → ⟦ T ⟧ ;
%%% ⟦ [name] ⟧ → ⟦ name ⟧ ;
%%% ⟦ [⟨Float#1⟩] ⟧ → ⟦ #⟨Float#1⟩ ⟧ ;
%%% ⟦ [⟨Int#1⟩] ⟧ → ⟦ #⟨Int#1⟩ ⟧ ;
%%% \end{code}
%%%   \caption{Code Generation.}
%%%   \label{fig:cgen}
%%% \end{figure}
%%% 
%%% \begin{example}\label{ex:cgen}
%%%   The code generation part of \emph{first.hx} is the final translation "CG" from the intermediate
%%%   representation to assembly code. This uses no new features, and is shown in Fig.~\ref{fig:cgen},
%%%   however, it is still worth a sanity check, walking through the "CG" scheme and checking that all
%%%   syntactic cases are covered.
%%% \end{example}
%%% 
%%% \begin{remark}[concrete \emph{vs.} raw syntax]
%%%   In the presentation we have chosen to use \emph{concrete syntax} even for semantic
%%%   operations. This has the advantage of allowing direct invocation of even complex structured
%%%   calculations from the command line but it does ``pollute'' the syntax of the defined language.
%%%   (Production versions of \HAX (not yet released) will have the option of generating parsers that
%%%   ignore concrete syntax of schemes when running the compiler.) It is sometimes practical to define
%%%   ``bridge schemes'' that make schemes available both in syntax and raw; we give an example of this
%%%   in the following section.
%%% \end{remark}


\section{Conclusion and Future Work}
\label{sec:conc}

We have reported some of the details of what it has been like to use the \HAX (Higher-order
Attribute Contraction Schemes) formalism as the implementation language for the compiler project of
our Compiler Construction class at NYU.  The full class goes on to explain \emph{code generation}
using very similar techniques but rewriting the AST to a code AST instead of deriving a ``code''
attribute.

In conclusion it appears to definitely be possible to teach traditional compiler construction skills
along with a very formal methodology for actually programming the compiler.  In a sense it turns the
class on it's head compared to similar classes: very often the formalism is the ``exam question
stuff'' and the project programming is all low level hacking---in our case we often found that we
had to weight the rather low level algorithmic stuff (automata, common subexpression elimination) in
the homework and exam, since the formal methods were covered in the ``programming'' project.

\smallskip\noindent\textbf{Related Work.}
Many have observed that compiler formal systems are essentially algebraic or rewrite systems, and
tools have been developed that offer formal compiler development, from VDM~\cite{BjornerJones:1982}
through ASF+SDF~\cite{BrandEtal:toplas2002}. However, every compiler teaching effort that we know of
that uses these methods has effectively turned into a class \emph{on} the tool rather than a class
that just happens to use it (one possible exception to this may be classes in Amsterdam using the
now very mature Rascal~\cite{KlintvandDerStormVinju:scam2009} system). For a great analysis of the
issues with algebraic formal methods for coding systems such as compilers, see van den Bos
\emph{etal.}~\cite{Bos+:eptcs2011}, however, a comparison is out of scope for this paper.

\bibliography{crs}

\end{document}

%%---------------------------------------------------------------------
% Tell Emacs that this is a LaTeX document and how it is formatted:
% Local Variables:
% mode:latex
% fill-column:100
% TeX-master: t
% End:
